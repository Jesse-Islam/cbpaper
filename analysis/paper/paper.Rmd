---
documentclass: jss
author:
  - name: Sahir Rai Bhatnagar*
    affiliation: McGill University
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{sahir.bhatnagar@mail.mcgill.ca}
    url: http://sahirbhatnagar.com/
  - name: Maxime Turgeon*
    affiliation: 'University of Manitoba \AND'
    address: >
      186 Dysart Road
      Winnipeg, MB, Canada R3T 2N2
    email: \email{max.turgeon@umanitoba.ca}
    url: https://maxturgeon.ca/
  - name: Jesse Islam 
    affiliation: McGill University
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{jesse.islam@mail.mcgill.ca}
  - name: James Hanley
    affiliation: McGill University
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{james.hanley@mcgill.ca}
    url: http://www.medicine.mcgill.ca/epidemiology/hanley/
  - name: Olli Saarela
    affiliation: University of Toronto
    address: >
      Dalla Lana School of Public Health, 
      155 College Street, 6th floor, 
      Toronto, Ontario 
      M5T 3M7, Canada
    email: \email{olli.saarela@utoronto.ca}
    url: http://individual.utoronto.ca/osaarela/
title:
  formatted: "\\pkg{casebase}: An Alternative Framework For Survival Analysis"
  # If you use tex in the formatted title, also supply version without
  plain:     "casebase: An Alternative Framework For Survival Analysis"
  # For running headers, if needed
  short:     "\\pkg{casebase}: An Alternative Framework For Survival Analysis"
abstract: >
  The abstract of the article. *SRB and MT contributed equally to this work.
keywords:
  # at least one keyword must be supplied
  formatted: [survival analysis, absolute risk, data visualization]
  plain:     [survival analysis, absolute risk, data visualization]
preamble: >
  \usepackage{amsmath}
  \usepackage{longtable}
  \usepackage{graphicx}
  \usepackage{tabularx}
  \usepackage{float}
  \usepackage{booktabs}
  \usepackage{makecell}
  \usepackage{tabu}
  \DeclareUnicodeCharacter{2500}{-}
output: 
  rticles::jss_article:
    number_sections: TRUE     #added argument option 
    citation_package: "natbib"  #All my citations use biblatex, not natbib. 
biblio-style: jss      #Listed to use in JSS Instructions for Authors, but not in template by default. 
bibliography: references.bib  #Also not included in template by default. 
fig_width: 12
fig_height: 9
fig_caption: true
editor_options: 
  chunk_output_type: console
---

<!-- # Code formatting -->

<!-- Don't use markdown, instead use the more precise latex commands: -->

<!-- * \proglang{Java} -->
<!-- * \pkg{plyr} -->
<!-- * \code{print("abc")} -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  cache = TRUE, 
  comment = "#>",
  fig.path = "../figures/"
)
options(kableExtra.latex.load_packages = FALSE)
library(casebase)
library(colorspace)
library(cowplot)
library(data.table)
library(glmnet)
library(kableExtra)
library(magrittr)
library(pracma)
library(flexsurv) # for fitting gompertz AFT
library(prodlim)
library(riskRegression)
library(splines)
library(survival)
library(tibble)
library(visreg)
library(lubridate)
library(tidyverse)
library(scales)
options(digits = 2, scipen = 999)
# set the seed for reproducible output
set.seed(1234)

eval_introduction <- TRUE
eval_theory <- TRUE
eval_implementation <- TRUE
eval_cs1 <- TRUE
eval_cs2 <- TRUE
eval_cs3 <- FALSE
eval_cs4 <- TRUE
eval_colophon <- TRUE

echo_plot_code <- TRUE

paper_gg_theme <- theme_bw(base_size = 10) + theme(legend.position = "bottom")
```

# Introduction

Survival analysis has been greatly influenced over the last 50 years by the partial likelihood approach of the Cox proportional hazard model [@cox1972regression]. This approach provides a flexible way of assessing the influence of covariates on the hazard function, without the need to specify a parametric survival model. This flexibility comes at the cost of decoupling the baseline hazard from the effect of the covariates. To recover the whole survival curve---or the cumulative incidence function (CIF)---we then need to separately estimate the baseline hazard [@breslow1972discussion]. This in turn often leads to stepwise estimates of the survival function.

From the perspective of clinicians and their patients, the most relevant quantity is often the 5- or 10-year risk of having a certain event given the patient's particular circumstances, and not the hazard ratio between a treatment and control group. Therefore, to make sound clinical decisions, it is important to accurately estimate the *full* hazard function, which can then be used to estimate the cumulative incidence function (CIF). Since the parametric hazard is a smooth function of time, the CIF and the survival function estimates also vary smoothly over time.

With the goal of fitting smooth-in-time hazard functions, Hanley & Miettinen [-@hanley2009fitting] proposed a general framework for estimating fully parametric hazard models via logistic regression. Their approach provides users that are familiar with generalized linear models with a natural way of fitting parametric survival models. Moreover, their framework is very flexible: general functions of time can be estimated (e.g. using splines or general additive models), and hence these models retain some of the flexibility of partial likelihood approaches.

In this article, we present an \proglang{R} package that combines the ideas of Hanley & Miettinen into a simple interface. The purpose of the \pkg{casebase} package is to provide practitioners with an easy-to-use software tool to predict the risk (or cumulative incidence) of an event, conditional on a particular patient's covariate profile. Our package retains the flexibility of case-base sampling and the familiar interface of the \code{glm} function. In addition, we provide extensive visualization tools.

In what follows, we first recall some theoretical details on case-base sampling and its use for estimating parametric hazard functions. We then give a short review of existing \proglang{R} packages that implement comparable features as \pkg{casebase}. Next, we provide some details about the implementation of case-base sampling in our package, and we give a brief survey of its main functions. This is followed by four case studies that illustrate the flexibility and capabilities of \pkg{casebase}. We show how the same framework can be used for competing risk analyses, penalized estimation, and for studies with time-dependent exposures. Finally, we end the article with a discussion of the results and of future directions.

# Theoretical details {#theory}

As discussed in Hanley & Miettinen [-@hanley2009fitting], the key idea behind case-base sampling is to discretize the study base into an infinite amount of *person moments*. These person moments are indexed by both an individual in the study and a time point, and therefore each person moment has a covariate profile, an exposure status and an outcome status attached to it. We note that there is only a finite number of person moments associated with the event of interest (what Hanley & Miettinen call the *case series*). Case-base sampling refers to the sampling from the base of a representative finite sample called the *base series*.

To describe the theoretical foundations of case-base sampling, we use the framework of counting processes. In what follows, we abuse notation slightly and omit any mention of $\sigma$-algebras; the interested reader can refer to Saarela & Arjas [-@saarela2015non] and Saarela [-@saarela2016case]. First, let $N_{i}(t) \in \{0, 1\}$ be counting processes corresponding to the event of interest for individual $i=1, \ldots,n$. For simplicity, we will consider Type I censoring due to the end of follow-up at time $\tau$ (the general case of non-informative censoring is treated in Saarela [-@saarela2016case]). We assume a continuous time model, which implies that the counting process jumps are less than or equal to one. We are interested in modeling the hazard functions $\lambda_{i}(t)$ of the processes $N_i(t)$, and which satisfy
$$\lambda_{i}(t) dt = E[dN_{i}(t)].$$ 

Next, we model the base series sampling mechanism using non-homogeneous Poisson processes $R_i(t) \in \{0, 1, 2, \ldots\}$, with the person-moments where $dR_i(t) = 1$ constituting the base series. The process $Q_{i}(t) = R_i(t) + N_{i}(t)$ then counts both the case and base series person-moments contributed by individual $i$. This process is typically defined by the user via its hazard function $\rho_i(t)$. The process $Q_{i}(t)$ is characterized by $E[dQ_{i}(t)] = \lambda_{i}(t)dt + \rho_i(t)dt$. 

If the hazard function $\lambda_{i}(t; \theta)$ is parametrized in terms of $\theta$, we could define an estimator $\hat{\theta}$ by maximization of the likelihood expression
$$L_0(\theta) = \prod_{i=1}^n \exp\left\{ -\int_0^\tau \lambda_i(t; \theta) dt \right\} \prod_{i=1}^{n} \prod_{t\in[0,\tau)} \lambda_{i}(t;\theta)^{dN_{i}(t)}\left(1 - \lambda_{i}(t;\theta)\right)^{1 - dN_{i}(t)},$$
where $\prod_{t\in[0,u)}$ represents a product integral from $0$ to $u$. However, the integral over time makes the computation and maximization of $L_0(\theta)$ challenging. 

Case-base sampling allows us to avoid this integral. By conditioning on a sampled person-moment, we get individual contributions of the form
$$P(dN_{i}(t) \mid dQ_{i}(t) = 1) \stackrel{\theta}{\propto} \frac{\lambda_{i}(t; \theta)^{dN_{i}(t)}}{\rho_i(t) + \lambda_{i}(t;\theta)}.$$
Therefore, we can define an estimating equation for $\theta$ as follows:
$$L(\theta) = \prod_{i=1}^{n} \prod_{t\in[0,\tau)} \left(\frac{\lambda_{i}(t; \theta)^{dN_{i}(t)}}{\rho_i(t) + \lambda_{i}(t;\theta)}\right)^{dQ_i(t)}.$$
When a logarithmic link function is used for modeling the hazard function, the above expression is of a logistic regression form with an offset term $\log(1/\rho_i(t))$. Note that the sampling units selected in the case-base sampling mechanism are person-moments, rather than individuals, and the parameters to be estimated are hazards or hazard ratios rather than odds or odds ratios. Generally, an individual can contribute more than one person-moment, and thus the terms in the product integral are not independent. Nonetheless, Saarela [-@saarela2016case] showed that the logarithm of this estimating function has mean zero at the true value $\theta=\theta_0$, and that the resulting estimator $\hat{\theta}$ is asymptotically normally distributed.

In Hanley & Miettinen [-@hanley2009fitting], the authors suggest sampling the base series *uniformly* from the study base. In terms of Poisson processes, their sampling strategy corresponds essentially to a time-homogeneous Poisson process with hazard equal to $\rho_i(t) = b/B$, where $b$ is the number of sampled observations in the base series, and $B$ is the total population-time for the study base (e.g. the sum of all individual follow-up times). More complex examples are also possible; see for example Saarela & Arjas [-@saarela2015non], where the intensity functions for the sampling mechanism are proportional to the cardiovascular disease event rate given by the Framingham score. With this sampling mechanism, Saarela & Arjas are able to increase the efficiency of their estimators, when compared to uniform sampling. 

Let $g(t; X)$ be the linear predictor such that $\log(\lambda(t;X)) = g(t; X)$. Different functions of $t$ lead to different parametric hazard models. The simplest of these models is the one-parameter exponential distribution which is obtained by taking the hazard function to be constant over the range of $t$:

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 X. \label{eq:exp}
\end{equation}


In this model, the instantaneous failure rate is independent of $t$.\footnote{The conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been in the study. This is also known as the \textit{memoryless property} \citep{kalbfleisch2011statistical}.}

The Gompertz hazard model is given by including a linear term for time:

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 t + \beta_2 X. \label{eq:gomp}
\end{equation}

Use of $\log(t)$ yields the Weibull hazard which allows for a power dependence of the hazard on time [@kalbfleisch2011statistical]:

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 \log(t) + \beta_2 X. \label{eq:weibull}
\end{equation}


Case-base sampling can also be used in the context of competing-risk analyses. Assuming there are $J$ competing events, we can show that each person-moment's contribution of the likelihood is of the form

$$\frac{\lambda_j(t)^{dN_j(t)}}{\rho(t) + \sum_{j=1}^J\lambda_j(t)},$$

where $N_j(t)$ is the counting process associated with the event of type $j$ and $\lambda_j(t)$ is the corresponding hazard function. As may be expected, this functional form is similar to the terms appearing in the likelihood function for multinomial regression.\footnote{Specifically, it corresponds to the following parametrization: \begin{align*} \log\left(\frac{P(Y=j \mid X)}{P(Y = J \mid X)}\right) = X^T\beta_j, \qquad j = 1,\ldots, J-1.\end{align*}}

# Existing packages

Survival analysis is an important branch of applied statistics and epidemiology. Accordingly, there is already a vast ecosystem of \code{R} packages implementing different methodologies. In this section, we describe how the functionalities of \pkg{casebase} compare to these packages.

At the time of writing, a cursory examination of CRAN's task view on survival analysis reveals that there are over 250 packages related to survival analysis [-@survTaskView]. For the purposes of this article, we restricted our review to packages that implement at least one of the following features: parametric modeling, non-proportional hazard models, competing risk analysis, penalized estimation, and CIF estimation. By searching for appropriate keywords in the \code{DESCRIPTION} file of these packages, we found 60 relevant packages. These 60 packages were then manually examined to determine which ones are comparable to \pkg{casebase}. In particular, we excluded packages that were focused on a different set of problems, such as frailty and multi-state models. The remaining 14 packages appear in Table \ref{tab:surv-pkgs}, along with some of the functionalities they offer. 

Parametric survival models are implemented in a handful of packages: \pkg{CFC} [-@mahani2015bayesian], \pkg{flexsurv} [-@flexsurv], \pkg{SmoothHazard} [-@smoothHazard], \pkg{rsptm2} [-@clements_liu], \pkg{mets} [-@scheike2014estimating], and \pkg{survival} [-@survival-package]. The types of models they allow vary for each package. For example, \pkg{SmoothHazard} is limited to Weibull distributions [-@smoothHazard], whereas both \pkg{flexsurv} and \pkg{survival} allow users to supply any distribution of their choice. Also, \pkg{flexsurv}, \pkg{smoothhazard}, \pkg{mets} and \pkg{rstpm2} also have the ability to model the effect of time using splines, which allows flexible modeling of the hazard function. Moreover, \pkg{flexsurv} has the ability to estimate both scale and shape parameters for a variety of parametric families. As discussed above, \pkg{casebase} can model any parametric family whose log-hazard can be expressed as a linear combination of covariates (including time). Therefore, our package allows the user to model the effect of time using splines. Also, by including interaction terms between covariates and time, it also allows users to fit time-varying coefficient models. However, we do not explicitly model any shape parameter, unlike \pkg{flexsurv}.

Several packages implement penalized estimation for the Cox model: \pkg{glmnet} [-@regpathcox], \pkg{glmpath} [-@park_hastie], \pkg{penalized} [-@l1penal], \pkg{RiskRegression} [-@gerds_blanche]. Moreover, some packages also include penalized estimation in the context of Cox models with time-varying coefficients: elastic-net penalization with \pkg{CoxRidge} [-@perperoglou] and \pkg{rstpm2} [-@clements_liu], while \pkg{survival} [-@survival-package] has an implementation of ridge-penalized estimation. On the other hand, our package \pkg{casebase} provides penalized estimation in the context of parametric hazards. To our knowledge, \pkg{casebase} and \pkg{rsptm2} are the only packages to offer this functionality.

Next, several \proglang{R} packages implement methodologies for competing risk analysis; for a different perspective on this topic, see Mahani & Sharabiani [-@mahani2015bayesian]. The package \pkg{cmprsk} provides methods for cause-specific subdistributions, such as in the Fine-Gray model [-@fine1999proportional]. On the other hand, the package \pkg{CFC} estimates cause-specific CIFs from unadjusted, non-parametric survival functions. Our package \pkg{casebase} also provides functionalities for competing risk analysis by estimating parametrically the cause-specific hazards. From these quantities, we can then estimate the cause-specific CIFs. 

Finally, several packages include functions to estimate the CIF. The corresponding methods generally fall into two categories: transformation of the estimated hazard function, and semi-parametric estimation of the baseline hazard. The first category broadly corresponds to parametric survival models, where the full hazard is explicitly modeled. Using this estimate, the survival function and the CIF can be obtained using their functional relationships (see Equations \ref{eqn:surv} and \ref{eqn:CI} below). Packages providing this functionality include \pkg{CFC}, \pkg{Flexsurv}, \pkg{mets}, and \pkg{survival}. Our package \pkg{casebase} also follows this approach for both single-event and competing-risk analyses. The second category outlined above broadly corresponds to semi-parametric models. These models do not model the full hazard function, and therefore the baseline hazard needs to be estimated separately in order to estimate the survival function. This is achieved using semi-parametric estimators (e.g. Breslow's estimator) or parametric estimators (e.g. spline functions). Packages that implement this approach include \pkg{RiskRegression}, \pkg{rstpm2}, and \pkg{survival}. As mentioned in the introduction, a key distinguishing factor between these two approaches is that the first category leads to smooth estimates of the cumulative incidence function, whereas the second category often produces estimates in the form of step-wise functions. Providing smooth estimates of the CIF was one of the main motivations for introducing case-base sampling in survival analysis.


\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
\hline
& \textbf{Competing} & \textbf{Allows} &  \textbf{Penalized}   &    &  & \textbf{Semi} & \textbf{Interval/Left} & \textbf{Absolute}  \\
\textbf{Package}        & \textbf{Risks}  & \textbf{Non PH} & \textbf{Likelihood} & \textbf{Splines} & \textbf{Parametric} & \textbf{Parametric} & \textbf{Censoring} & \textbf{Risk}           \\
\hline
\textbf{casebase}        & x                        & x                         & x                     & x                & x                   &                          &                                  & x                                \\
\textbf{CFC}             & x                        & x                         &                       &                  & x                   &                          &                                  & x                                \\
\textbf{cmprsk}             & x                        &                           &                       &                  &                     &  x                        &                                  & x                                \\
\textbf{coxRidge}        &                          & x                         & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{crrp}            & x                        &                           & x                     &                  &                     &                          &                                  &                                  \\
\textbf{fastcox}         &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{Flexrsurv}       &                          & x                         &                       & x                & x                   &                          &                                  & x               \\
\textbf{Flexsurv}        & x                        & x                         &                       & x                & x                   &                          &                                  & x               \\
\textbf{glmnet}          &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{glmpath}        &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{mets}            & x                        &                           &                       & x                &                     & x                        &                                  & x                                \\
\textbf{penalized}       &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{RiskRegression}  &                          &                           & x                     &                  &                     & x                        &                                  & x                                \\
\textbf{Rstpm2}          &                          & x                         & x                     & x                & x                   & x                        & x                                & x                         \\
\textbf{SmoothHazard}    &                          & x                         &                       & x                & x                   &                          & x                            &                                      \\
\textbf{Survival}        & x                        & x                         &                       &                  & x                   & x                        & x                                & x                               \\
\hline
\end{tabular}%
}
\caption{Comparison of various \proglang{R} packages for survival analysis based on several defining features.}
\label{tab:surv-pkgs}
\end{table}

<!--
\begin{table}
  \begin{tabular}{p{1.8in}lll}
\hline
    &  Parameters &  Density \proglang{R} function & \code{dist}\\
    & {\footnotesize{(location in \code{\color{red}{red}})}} & & \\
\hline
    Exponential & \code{\color{red}{rate}}             & \code{dexp}   & \code{"exp"} \\
    Weibull (accelerated failure time)     & \code{shape, {\color{red}{scale}}}     & \code{dweibull} & \code{"weibull"} \\
    Weibull (proportional hazards)     & \code{shape, {\color{red}{scale}}}     & \code{dweibullPH} & \code{"weibullPH"} \\
    Gamma       & \code{shape, \color{red}{rate}}      & \code{dgamma} & \code{"gamma"}\\
    Log-normal  & \code{{\color{red}{meanlog}}, sdlog}   & \code{dlnorm} & \code{"lnorm"}\\
    Gompertz    & \code{shape, {\color{red}{rate}}}      & \code{dgompertz} & \code{"gompertz"} \\
    Log-logistic & \code{shape, {\color{red}{scale}}}   & \code{dllogis} & \code{"llogis"}\\
    Generalized gamma (Prentice 1975)   & \code{{\color{red}{mu}}, sigma, Q} & \code{dgengamma} & \code{"gengamma"} \\
    Generalized gamma (Stacy 1962)& \code{shape, {\color{red}{scale}}, k} & \code{dgengamma.orig} & \code{"gengamma.orig"} \\
    Generalized F     (stable)    & \code{{\color{red}{mu}}, sigma, Q, P} & \code{dgenf} & \code{"genf"} \\
    Generalized F     (original)  & \code{{\color{red}{mu}}, sigma, s1, s2} & \code{dgenf.orig} & \code{"genf.orig"} \\
\hline
  \end{tabular}
  \caption{Built-in parametric survival distributions in \pkg{flexsurv}.}
  \label{tab:dists}
\end{table}
-->

# Implementation details

The functions in the \pkg{casebase} package can be divided into two categories: 1) exploratory data analysis, in the form of population-time plots; and 2) parametric modeling of the hazard function. We strove for compatibility with both \code{data.frame}s and \code{data.table}s; this can be seen in the coding choices we made and the unit tests we wrote.

## Population-time plots

The case-base sampling approach described in Section \ref{theory} can be visualized in the form of a population time plot. These plots are extremely informative graphical displays of survival data and should be one of the first steps in an exploratory data analysis. The \code{popTime} function and \code{plot} method facilitate this task:

1. The \code{casebase::popTime} function takes as input the original dataset along with the column names corresponding to the timescale, the event status and an exposure group of interest (optional). This will create an object of class \code{popTime}.  
2. The corresponding \code{plot} method for the object created in Step 1 can be called to create the population time plot with several options for customizing the aesthetics.  

By splitting these tasks, we give flexibility to the user. While the method call in Step 2 allows further customization by using the \pkg{ggplot2} [@ggplot2] family of functions, users can use for the graphics system of their choice to create population-time plots from the object created in Step 1.

To illustrate these functions, we will use the European Randomized Study of Prostate Cancer Screening (ERSPC) data [@schroder2009screening] which was extracted using the approach described in Liu *et al.* [-@liu2014recovering] This dataset is available through the \pkg{casebase} package. It contains the individual observations for 159,893 men  from seven European countries, who were between the ages of 55 and 69 years when recruited for the trial. 

We first create the necessary dataset for producing the population time plot using the \code{popTime} function. In this example, we stratify the plot by treatment group. The resulting object inherits from class \code{popTime} and stores the exposure variable as an attribute:

```{r erspc-data, eval = eval_cs1, echo = c(-1:-3)}
data("ERSPC")
ERSPC$ScrArm <- factor(ERSPC$ScrArm, 
                       levels = c(0,1), 
                       labels = c("Control group", "Screening group"))

pt_object <- casebase::popTime(ERSPC, 
                               time = "Follow.Up.Time",
                               event = "DeadOfPrCa",
                               exposure = "ScrArm")
inherits(pt_object, "popTime")
attr(pt_object, "exposure")
```

We then pass this object to the corresponding \code{plot} method:

```{r plot-stratified-erspc-data-code, eval = FALSE, echo = TRUE}
plot(pt_object, add.base.series = TRUE)
```

```{r plot-stratified-erspc-data, eval = eval_cs1, echo = FALSE, fig.width=7, fig.height=5, fig.asp=0.75, fig.cap="Population time plot for ERSPC data."}
label_number_si <- function (accuracy = 1, unit = NULL, sep = NULL, ...) {
    sep <- if (is.null(unit)) 
        ""
    else " "
    scales:::force_all(accuracy, ...)
    function(x) {
        breaks <- c(0, 10^c(K = 3, M = 6, B = 9, T = 12))
        n_suffix <- cut(abs(x), breaks = c(unname(breaks), Inf), 
            labels = c(names(breaks)), right = FALSE)
        n_suffix[is.na(n_suffix)] <- ""
        suffix <- paste0(sep, n_suffix, unit)
        scale <- 1/breaks[n_suffix]
        scale[which(scale %in% c(Inf, NA))] <- 1
        number(x, accuracy = accuracy, scale = unname(scale), 
            suffix = suffix, ...)
    }
}
plot(pt_object, 
     add.base.series = TRUE,
     ratio = 0.7,
     facet.params = list(ncol = 2),
     ribbon.params = list(fill = "gray50"),
     case.params = list(size = 0.8),
     base.params = list(size = 0.8)) + 
  paper_gg_theme +  
  scale_y_continuous(labels = label_number_si())
```


Figure \ref{fig:plot-stratified-erspc-data} is built sequentially by first adding a layer for the area representing the population time in gray, with subjects having the least amount of observation time plotted at the top of the y-axis. We immediately notice a distinctive *step-wise shape* in the population time area. This is due to the randomization of the Finnish cohorts which were carried out on January 1 of each of year from 1996 to 1999. Coupled with the uniform December 31 2006 censoring date, this led to large numbers of men with exactly 11, 10, 9 or 8 years of follow-up. Tracked backwards in time (i.e. from right to left), the population-time plot shows the recruitment pattern from its beginning in 1991, and the January 1 entries in successive years. Tracked forwards in time (i.e. from left to right), the plot for the first three years shows attrition due entirely to death (mainly from other causes). Since the Swedish and Belgian centres were the last to complete recruitment--in December 2003--the minimum potential follow-up is three years. Tracked further forwards in time (i.e. after year 3) the attrition is a combination of deaths and staggered entries. As we can see, population-time plots summarise a wealth of information about the study into a simple graph.

Next, layers for the case series and base series are added. The case series is sampled at random vertically on the plot to avoid having all points along the upper edge of the gray area. By randomly distributing the cases, we can get a better sense of the incidence density. In Figure \ref{fig:plot-stratified-erspc-data}, we see that more events are observed at later follow-up times. Therefore, a constant hazard model would not be appropriate in this instance as it would overestimate the cumulative incidence earlier on in time, and underestimate it later on. Finally, the base series is sampled horizontally with sampling weight proportional to their follow-up time. The reader should refer to the package vignettes for more examples and a detailed description of how to modify the aesthetics of a population-time plot.

## Parametric modeling

The parametric modeling step was separated into three parts: 

  1. case-base sampling; 
  2. estimation of the smooth hazard function; 
  3. estimation of the CIF. 
  
By separating the sampling and estimation functions, we allow the possibility of users implementing more complex sampling scheme (as described in Saarela [-@saarela2016case]), or more complex study designs (e.g. time-varying exposure).  

The sampling scheme selected for \code{sampleCaseBase} was described in Hanley & Miettinen [-@hanley2009fitting]: we first sample along the "person" axis, proportional to each individual's total follow-up time, and then we sample a moment uniformly over their follow-up time. This sampling scheme is equivalent to the following picture: imagine representing the total follow-up time of all individuals in the study along a single dimension, where the follow-up time of the next individual would start exactly when the follow-up time of the previous individual ends. Then the base series could be sampled uniformly from this one-dimensional representation of the overall follow-up time. In any case, the output is a dataset of the same class as the input, where each row corresponds to a person-moment. The covariate profile for each such person-moment is retained, and an offset term is added to the dataset. This output could then be used to fit a smooth hazard function, or for visualization of the base series.

Next, the fitting function \code{fitSmoothHazard} starts by looking at the class of the dataset: if it was generated from \code{sampleCaseBase}, it automatically inherited the class \code{cbData}. If the dataset supplied to \code{fitSmoothHazard} does not inherit from \code{cbData}, then the fitting function starts by calling \code{sampleCaseBase} to generate the base series. In other words, users can bypass \code{sampleCaseBase} altogether and only worry about the fitting function \code{fitSmoothHazard}. 

The fitting function retains the familiar formula interface of \code{glm}. The left-hand side of the formula should be the name of the column corresponding to the event type. The right-hand side can be any combination of the covariates, along with an explicit functional form for the time variable. Note that non-proportional hazard models can be achieved at this stage by adding an interaction term involving time (cf. Case Study 4 below). The offset term does not need to be specified by the user, as it is automatically added to the formula before calling \code{glm}.

To fit the hazard function, we provide several approaches that are available via the \code{family} parameter. These approaches are:

  - \code{glm}: This is the familiar logistic regression.
  - \code{glmnet}: This option allows for variable selection using Lasso or elastic-net (cf. Case Study 3). This functionality is provided through the \pkg{glmnet} package [@friedman2010jss].
  - \code{gam}: This option provides support for *Generalized Additive Models* via the \pkg{mgcv} package [@hastie1987generalized].
  - \code{gbm}: This option provides support for *Gradient Boosted Trees* via the \pkg{gbm} package. This feature is still experimental.
  
In the case of multiple events, the hazard is fitted via multinomial regression as performed by the \pkg{VGAM} package. We selected this package for its ability to fit multinomial regression models with an offset.

Once a model-fit object has been returned by \code{fitSmoothHazard}, all the familiar summary and diagnostic functions are available: \code{print}, \code{summary}, \code{predict}, \code{plot}, etc. Our package provides one more functionality: it computes risk functions from the model fit. For the case of a single event, it uses the familiar identity
\begin{equation}\label{eqn:surv}
S(t) = \exp\left(-\int_0^t \lambda(u;X) du\right).
\end{equation}
The integral is computed using either the \code{stats::integrate} function or Monte-Carlo integration. The risk function (or cumulative incidence function) is then defined as
\begin{equation}\label{eqn:CI}
CI(t) = 1 - S(t).
\end{equation}

For the case of a competing-event analysis, the event-specific risk is computed using the following procedure: first, we compute the overall survival function (i.e. for all event types):

$$ S(t) = \exp\left(-\int_0^t \Lambda(u;X) du\right),\qquad \Lambda(t;X) = \sum_{j=1}^J \lambda_j(t;X).$$
From this, we can derive the event-specific subdensities:

$$ f_j(t) = \lambda_j(t)S(t).$$

Finally, by integrating these subdensities, we obtain the event-specific cumulative incidence functions:

$$ CI_j(t) = \int_0^t f_j(u)du.$$
The integrals are computed using either numerical integration (via the trapezoidal rule) or Monte Carlo integration. This option is controlled by the argument \code{method} of the \code{absoluteRisk} function.

<!--We created \code{absoluteRisk} as an \code{S3} generic, with methods that match the different types of outputs of \code{fitSmoothHazard}. The method dispatch system of \proglang{R} then takes care of matching the correct output to the correct methodology for calculating the cumulative incidence function, without requiring any intervention from the user.-->

In the following sections, we illustrate these functionalities in the context of four case studies.

# Case study 1---European Randomized Study of Prostate Cancer Screening

```{r, eval = eval_cs1, echo=FALSE}
fmla <- list(exponential = as.formula(DeadOfPrCa ~ ScrArm),
             gompertz = as.formula(DeadOfPrCa ~ Follow.Up.Time + ScrArm),
             weibull = as.formula(DeadOfPrCa ~ log(Follow.Up.Time) + ScrArm),
             splines = as.formula(DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm))

fits <- lapply(fmla, function(i) {
  casebase::fitSmoothHazard(i, data = ERSPC, ratio = 100)
})
```

For our first case study, we return to the ERSPC study and investigate the differences in survival between the control and screening arms. We fit four models that differ in which functional form of time is used: 1) excluded from the linear predictor, 2) linear function, 3) log function, and 4) a smooth function using cubic B-splines. The models are fit using \code{fitSmoothHazard}. Since the output object from \code{fitSmoothHazard} inherits from the \code{glm} class, we can directly use the function \code{summary}:

```{r, eval = FALSE, echo = TRUE}
fmla <- list(exponential = as.formula(DeadOfPrCa ~ ScrArm),
             gompertz = as.formula(DeadOfPrCa ~ Follow.Up.Time + ScrArm),
             weibull = as.formula(DeadOfPrCa ~ log(Follow.Up.Time) + ScrArm),
             splines = as.formula(DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm))

fits <- lapply(fmla, function(i) {
  casebase::fitSmoothHazard(i, data = ERSPC, ratio = 100)
})

summary(fits[["gompertz"]])
```

```{r, eval = eval_cs1, echo=FALSE}
summary(fits[["gompertz"]])
```

Next, the \code{absoluteRisk} function takes as input the \code{fitSmoothHazard} object and returns a matrix where each column corresponds to the covariate profiles specified in the \code{newdata} argument, and each row corresponds to a specified time point:

```{r, eval=FALSE, echo=echo_plot_code}
new_data <- data.frame(ScrArm = c("Control group", "Screening group"))

risks <- lapply(fits, function(i) {
  casebase::absoluteRisk(object = i, time = seq(0,15,0.1), newdata = new_data)
})

head(risks[["gompertz"]])
```

In Figure \ref{fig:erspc-cox-cif}, we overlay the estimated CIFs from \pkg{casebase} on the Cox model CIF. The CIF estimates for the exponential model in panel (1) overestimate the cumulative incidence earlier on in time, and underestimate it later on. Based on our earlier discussion of the population-time plot, this poor fit for the exponential hazard was expected. 

In Figure \ref{fig:erspc-cox-cif} (panels 2--4), we notice a better fit with increasing complexity of our model for time. As noted above, the usual asymptotic results hold for likelihood ratio tests built using case-base sampling models. Therefore, we can easily test the null hypothesis that the exponential model is just as good as the larger (in terms of number of parameters) splines model:

```{r erscp-compare, eval = eval_cs1, echo = FALSE}
anova(fits$exponential, fits$splines, test = "LRT")
```

As expected, we see that splines model provides a better fit. Similarly, we can compare the AIC for all four case-base models:

```{r erscp-aic, eval = eval_cs1, echo=FALSE}
c("Exp." = AIC(fits$exponential),
  "Gomp." = AIC(fits$gompertz),
  "Weib." = AIC(fits$exponential),
  "Splines" = AIC(fits$splines))
```

Once again, we have evidence that the splines model provides the best fit.

<!--A common choice for this type of analysis is to use a Cox regression model and estimate the hazard ratio for the screening group (relative to the control group). In \proglang{R}, it can be done as follows:-->

```{r erspc-cox, eval = eval_cs1, echo=FALSE}
surv_obj <- with(ERSPC, Surv(Follow.Up.Time, DeadOfPrCa))
cox_model <- survival::coxph(surv_obj ~ ScrArm, data = ERSPC)
```

```{r erspc-cox-cif, eval = eval_cs1, echo = FALSE, fig.width=8, fig.height=6, fig.cap="CIFs for control and screening groups in the ERSPC data. In each of the panels, we plot the CIF from the Cox model using \\code{survival::survfit} (solid line) and the CIF from the case-base sampling scheme (dashed line) with different functional forms of time. (1) The time variable is excluded (exponential). (2) Linear function of time (Gompertz). (3) The natural logarithm (Weibull). (4) Cubic B-spline expansion of time."}
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
myCols <- cbPalette[c(4,7)]
new_data <- data.frame(
  ScrArm = c("Control group", "Screening group")#,
  # ignore = 99
)

risks <- lapply(fits, function(i) {
  casebase::absoluteRisk(object = i, time = seq(0,15,0.1), newdata = new_data)
})

aft_fits <- lapply(list("exp","gompertz","weibull"), function(i)
  flexsurv::flexsurvreg(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm,
                           data = ERSPC, dist = i)
  )

names(aft_fits) <- list("exponential","gompertz","weibull")

# dev.off()
layout(matrix(1:4, ncol = 2, byrow = FALSE), respect = FALSE)
par(mar = c(0, 4.1, 4.1, 0))
plot(survfit(cox_model, newdata = new_data),
     ylab = "",
     xlab = "",
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = myCols, 
     # main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
     #                Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
     #                exp(coef(cox_model)), 
     #                exp(confint(cox_model))[1], 
     #                exp(confint(cox_model))[2]), 
     lwd = 2,
     yscale = 100,
     xaxt = 'n')
legend("topleft", 
       legend = c("Control (Cox)","Control (Casebase)",
                  "Screening (Cox)", "Screening (Casebase)"), 
       col = rep(myCols, each = 2),
       lty = c(1, 2, 1, 2), 
       lwd = 2,
       cex = 1.1,
       bg = "gray90")
lines(risks$exponential[,1], risks$exponential[,2], col = myCols[1], lty = 2, lwd = 2)
lines(risks$exponential[,1], risks$exponential[,3], col = myCols[2], lty = 2, lwd = 2)
text(x = 15*0.97, y = 0.05/100, labels = "(1)", cex = 1.5)

par(mar = c(4.1, 4.1, 0, 0))
plot(survfit(cox_model, newdata = new_data),
     ylab = "", xlab = "",
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = myCols, 
     lwd = 2,
     yscale = 100)
lines(risks$weibull[,1], risks$weibull[,2], col = myCols[1], lty = 2, lwd = 2)
lines(risks$weibull[,1], risks$weibull[,3], col = myCols[2], lty = 2, lwd = 2)  
text(x = 15*0.97, y = 0.05/100, labels = "(3)", cex = 1.5)
mtext("Cumulative incidence (%)", side = 2, line = -1.5, outer = TRUE)

par(mar = c(0, 0, 4.1, 3.9))
plot(survfit(cox_model, newdata = new_data),
     ylab = "",xlab = "",
     fun = "event",
     xaxt = 'n', yaxt = 'n',
     xlim = c(0,15), conf.int = FALSE, col = myCols, 
     lwd = 2,
     yscale = 100)
lines(risks$gompertz[,1], risks$gompertz[,2], col = myCols[1], lty = 2, lwd = 2)
lines(risks$gompertz[,1], risks$gompertz[,3], col = myCols[2], lty = 2, lwd = 2)
text(x = 15*0.97, y = 0.05/100, labels = "(2)", cex = 1.5)

par(mar = c(4.1, 0, 0, 3.9))
plot(survfit(cox_model, newdata = new_data),
     ylab = "",xlab = "",
     fun = "event",
     yaxt = 'n',
     xlim = c(0,15), conf.int = FALSE, col = myCols, 
     lwd = 2,
     yscale = 100)
lines(risks$splines[,1], risks$splines[,2], col = myCols[1], lty = 2, lwd = 2)
lines(risks$splines[,1], risks$splines[,3], col = myCols[2], lty = 2, lwd = 2)
text(x = 15*0.97, y = 0.05/100, labels = "(4)", cex = 1.5)
mtext("Years since randomization", side = 1, line = -1.5, outer = TRUE)
```

```{r, results='hide', eval = FALSE, echo = FALSE}
# par(mfrow = c(2, 2))
# lapply(fits, function(i) {
#   casebase::hazardPlot(i, newdata = new_data[2,], ci = FALSE)
#   casebase::hazardPlot(i, newdata = new_data[1,], ci = FALSE, add = TRUE, lty = 2)
# })
```

```{r erspc-casebase-exponential, eval = FALSE, echo = FALSE}
# # Equivalence between casebase and AFT
# # exponential -------------------------------------------------------------
# 
# casebase_exponential <- casebase::fitSmoothHazard(DeadOfPrCa ~ ScrArm, 
#                                                   data = ERSPC, 
#                                                   ratio = 100)
# aft_exponential <- survreg(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm, 
#                            data = ERSPC, dist = "exp")
# 
# -coef(aft_exponential)
# coef(casebase_exponential)
# summary(casebase_exponential)
# exp(coef(casebase_exponential)[2])
# exp(confint(casebase_exponential)[2, ])
# 
# # weibull -----------------------------------------------------------------
# 
# casebase_weibull <- casebase::fitSmoothHazard(DeadOfPrCa ~ log(Follow.Up.Time) + ScrArm, 
#                                            data = ERSPC, 
#                                            ratio = 100)
# aft_weibull <- survreg(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm, 
#                        data = ERSPC, dist = "weibull")
# 
# -coef(aft_weibull)/aft_weibull$scale
# coef(casebase_weibull)
```

```{r erspc-casebase-cif, eval = FALSE, echo = FALSE}
# smooth_risk_exp <- casebase::absoluteRisk(
#   object = casebase_exponential,
#   time = seq(0, 15, 0.1),
#   newdata = new_data
# )
# 
# plot(survfit(cox_model, newdata = new_data),
#   xlab = "Years since Randomization",
#   ylab = "Cumulative Incidence (%)",
#   fun = "event",
#   xlim = c(0, 15), conf.int = FALSE, col = c("red", "blue"),
#   main = sprintf(
#     "Estimated Cumulative Incidence (risk) of Death from Prostate 
#                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
#     exp(coef(cox_model)),
#     exp(confint(cox_model))[1],
#     exp(confint(cox_model))[2]
#   ),
#   ylim = c(0, 2), yscale = 100
# )
# lines(smooth_risk_exp[, 1], smooth_risk_exp[, 2], col = "red", lty = 2)
# lines(smooth_risk_exp[, 1], smooth_risk_exp[, 3], col = "blue", lty = 2)
# 
# legend("topleft",
#   legend = c(
#     "Control group (Cox)", "Control group (Casebase)",
#     "Screening group (Cox)", "Screening group (Casebase)"
#   ),
#   col = c("red", "red", "blue", "blue"),
#   lty = c(1, 2, 1, 2),
#   bg = "gray90"
# )
```

<!-- The \code{fitSmoothHazard} function provides an estimate of the hazard function $h(x, t)$ is the hazard function, where $t$ denotes the numerical value of a point in prognostic/prospective time and $x$ is the realization of the vector $X$ of variates based on the patient's profile and intervention (if any). -->
<!-- The \code{absoluteRisk} function provides an estimate of the cumulative incidence curves for a specific risk profile using the following equation: -->
<!-- $$ CI(x, t) = 1 - \exp\left( - \int_0^t h(x, u) \textrm{d}u \right) $$ -->

```{r erspc-casebase-weibull-cif, eval = FALSE, echo = FALSE}
# smooth_risk_time <- casebase::absoluteRisk(
#   object = casebase_time,
#   time = seq(0, 15, 0.1),
#   newdata = new_data
# )
# 
# plot(survfit(cox_model, newdata = new_data),
#   xlab = "Years since Randomization",
#   ylab = "Cumulative Incidence (%)",
#   fun = "event",
#   xlim = c(0, 15), conf.int = FALSE, col = c("red", "blue"),
#   main = sprintf(
#     "Estimated Cumulative Incidence (risk) of Death from Prostate 
#                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
#     exp(coef(cox_model)),
#     exp(confint(cox_model))[1],
#     exp(confint(cox_model))[2]
#   ),
#   ylim = c(0, 2), yscale = 100
# )
# lines(smooth_risk_time[, 1], smooth_risk_time[, 2], col = "red", lty = 2)
# lines(smooth_risk_time[, 1], smooth_risk_time[, 3], col = "blue", lty = 2)
# 
# legend("topleft",
#   legend = c(
#     "Control group (Cox)", "Control group (Casebase)",
#     "Screening group (Cox)", "Screening group (Casebase)"
#   ),
#   col = c("red", "red", "blue", "blue"),
#   lty = c(1, 2, 1, 2),
#   bg = "gray90"
# )
```

```{r erspc-95ci22, echo = FALSE, eval = eval_cs1}
CI_cox <- c(exp(coef(cox_model)), exp(confint(cox_model)))
# CI_exp <- c(exp(coef(fits$exponential))["ScrArmScreening group"], exp(confint(fits$exponential))["ScrArmScreening group",])
# CI_gompertz <- c(exp(coef(fits$gompertz))["ScrArmScreening group"], exp(confint(fits$gompertz))["ScrArmScreening group",])
# CI_weibull <- c(exp(coef(fits$weibull))["ScrArmScreening group"], exp(confint(fits$weibull))["ScrArmScreening group",])
# CI_splines <- c(exp(coef(fits$splines))["ScrArmScreening group"], exp(confint(fits$splines))["ScrArmScreening group",])

# function to exponentiate HR and CI and return a HR (lower, Upper) character
exp_est_ci <- function(model = c("exponential","gompertz","weibull","splines"),
                       fit, 
                       covariate = "ScrArmScreening group", 
                       type = c("casebase", "survreg")) {
  
  type <- match.arg(type)
  covariate <- match.arg(covariate)
  
  if (type == "casebase"){
    
    cis <- confint(fit[[model]])
    
    sprintf("%0.2f (%0.2f, %0.2f)",
            exp(coef(fit[[model]]))[[covariate]],
            exp(cis)[covariate,1],
            exp(cis)[covariate,2])
  } else {
    
    if (model == "weibull") {

      # The maximized log-likelihoods are the same between survreg and flexsurvreg, 
      # however the parameterisation is different: the first coefficient (Intercept) 
      # reported by survreg is log(mu), and survreg's "scale" is
      # dweibull's (thus flexsurvreg)'s 1 / shape
      res <- -fit[[model]]$res[covariate,] / (1 / fit[[model]]$res["shape",])
      
      sprintf("%0.2f (%0.2f, %0.2f)",
              exp(res[["est"]]),
              exp(res[["U95%"]]),
              exp(res[["L95%"]]))
      
    } else if (model == "splines") {
      
      c("--")
      
    } else {
      
      res <- fit[[model]]$res[covariate,]
      
      sprintf("%0.2f (%0.2f, %0.2f)",
              exp(res[["est"]]),
              exp(res[["L95%"]]),
              exp(res[["U95%"]]))
    }
  }
}

casebase_HR_CI <- lapply(list("exponential","gompertz","weibull","splines"), 
                         exp_est_ci, 
                         fit = fits,
                         covariate = "ScrArmScreening group", 
                         type = "casebase")

survreg_HR_CI <- lapply(list("exponential","gompertz","weibull","splines"), 
                         exp_est_ci, 
                         fit = aft_fits,
                         covariate = "ScrArmScreening group", 
                         type = "survreg")

ptable <- cbind(list("Exponential","Gompertz","Weibull","Splines"),
            do.call(rbind, casebase_HR_CI), do.call(rbind, survreg_HR_CI))

# table_ci <- rbind(CI_cox, CI_exp, CI_gompertz, CI_weibull, CI_splines)
# colnames(table_ci) <- c("HR", "LB", "UB")
# 
# data.frame(Model = c("Cox", "Exponential", "Gompertz", "Weibull", "Splines")) %>% 
#   bind_cols(as.data.frame(table_ci)) %>% 
#   mutate(CI = paste0("(", round(LB, 2), ", ",
#                      round(UB, 2), ")"),
#          Width = UB - LB) %>% 
#   dplyr::select(-LB, -UB) %>% 
#   knitr::kable(format = "latex", booktabs = TRUE)  %>%
#   kable_styling()
```

```{r erspc-casebase-splines-cif, eval = FALSE, echo = FALSE}
# smooth_risk_splines <- absoluteRisk(
#   object = casebase_splines,
#   time = seq(0, 15, 0.1),
#   newdata = new_data
# )
# 
# plot(survfit(cox_model, newdata = new_data),
#   xlab = "Years since Randomization",
#   ylab = "Cumulative Incidence (%)",
#   fun = "event",
#   xlim = c(0, 15), conf.int = FALSE, col = c("red", "blue"),
#   main = sprintf(
#     "Estimated Cumulative Incidence (risk) of Death from Prostate 
#                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
#     exp(coef(cox_model)),
#     exp(confint(cox_model))[1],
#     exp(confint(cox_model))[2]
#   ),
#   ylim = c(0, 2), yscale = 100
# )
# lines(smooth_risk_splines[, 1], smooth_risk_splines[, 2], col = "red", lty = 2)
# lines(smooth_risk_splines[, 1], smooth_risk_splines[, 3], col = "blue", lty = 2)
# 
# legend("topleft",
#   legend = c(
#     "Control group (Cox)", "Control group (Casebase)",
#     "Screening group (Cox)", "Screening group (Casebase)"
#   ),
#   col = c("red", "red", "blue", "blue"),
#   lty = c(1, 2, 1, 2),
#   bg = "gray90"
# )
```

In the following table, we present a side-by-side comparison of the hazard ratios and confidence intervals estimated from \code{fitSmoothHazard} and the corresponding parametric model using \code{survival::survreg}, as well as the Cox model estimate. The hazard ratio estimates and confidence intervals are similar across all four models. This reinforces the idea that, under proportional hazards, we do not need to model the full hazard to obtain reliable estimates of the hazard ratio. Nevertheless, Figure \ref{fig:erspc-cox-cif} shows that different parametric models can still give rise to qualitatively different estimates for the CIF. 

```{r print-erspc-estimates, eval = eval_cs1}
## ---- print-sim-table ----

kable(ptable, "latex", booktabs = TRUE, align = c("l","c","c"),
      caption = c("Comparison of estimated hazard ratios and 95\\% confidence intervals for ERSPC data."),
      col.names = c("Model","casebase::fitSmoothHazard","survival::survreg")) %>%
  kable_styling() %>%
  # add_header_above(c(" "," ","No overlap" = 2, "All causal SNPs\nin kinship" = 2, "No overlap" = 2, "All causal SNPs\nin kinship" = 2)) %>%
  # add_header_above(c(" "," ","Null model" = 4, "1% Causal SNPs" = 4)) %>%
  column_spec(1, bold = TRUE) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign = "middle") %>%
  footnote(general = sprintf("Cox model estimate: HR (95%% CI) = %0.2f (%0.2f, %0.2f)", CI_cox[1], CI_cox[2], CI_cox[3]),
           general_title = "") #%>%
  # footnote(general = "Model Size is the number of selected variables using the high-dimensional BIC for \texttt{ggmix} and 10-fold cross validation for \texttt{lasso} and \texttt{twostep}.")
  #          # number = c("", "Footnote 2; "),
  #          # alphabet = c("Footnote A; ", "Footnote B; "),
  #          # symbol = c("Footnote Symbol 1; ", "Footnote Symbol 2"),
  #          general_title = "Note: ", number_title = "Type I: ",
  #          alphabet_title = "Type II: ", symbol_title = "Type III: ",
  #          footnote_as_chunk = TRUE, title_format = c("italic", "underline")
  # )
```

```{r erspc-95ci, echo = FALSE, eval = FALSE}
CI_cox <- c(exp(coef(cox_model)), exp(confint(cox_model)))
CI_exp <- c(exp(coef(casebase_exponential))["ScrArmScreening group"], exp(confint(casebase_exponential))["ScrArmScreening group", ])
CI_time <- c(exp(coef(casebase_time))["ScrArmScreening group"], exp(confint(casebase_time))["ScrArmScreening group", ])
CI_splines <- c(exp(coef(casebase_splines))["ScrArmScreening group"], exp(confint(casebase_splines))["ScrArmScreening group", ])

table_ci <- rbind(CI_cox, CI_exp, CI_time, CI_splines)
colnames(table_ci) <- c("HR", "LB", "UB")

data.frame(Model = c("Cox", "Exponential", "Weibull", "Splines")) %>%
  bind_cols(as.data.frame(table_ci)) %>%
  mutate(
    CI = paste0(
      "(", round(LB, 2), ", ",
      round(UB, 2), ")"
    ),
    Width = UB - LB
  ) %>%
  dplyr::select(-LB, -UB) %>%
  knitr::kable(format = "latex", booktabs = TRUE) %>%
  kable_styling()
```

With this first case study, we explored how \pkg{casebase} allows us to fit different parametric survival models, and how we can compare the fit of each model with tools from GLMs.

# Case study 2---Bone-marrow transplant

In the next case study, we show how case-base sampling can  be used in the context of a competing risk analysis. For illustrative purposes, we will use the same data that was used in Scrucca *et al* [-@scrucca2010regression]. The data was downloaded from the first author's website, and it is now available as part of the \pkg{casebase} package.

```{r bmtcrr-data, eval = eval_cs2, message = FALSE}
data(bmtcrr)
```

The data contains information on 177 patients who received a stem-cell transplant for acute leukemia. The event of interest is relapse, but other competing causes (e.g. transplant-related death) were also recorded. Several covariates were captured at baseline: sex, disease type (acute lymphoblastic or myeloblastic leukemia, abbreviated as ALL and AML, respectively), disease phase at transplant (Relapse, CR1, CR2, CR3), source of stem cells (bone marrow and peripheral blood, coded as BM+PB, or only peripheral blood, coded as PB), and age. <!--A summary of these baseline characteristics appear in Table \ref{tab:table1bmtcrr}. We note that the statistical summaries were generated differently for different variable types: for continuous variables, we gave the range, followed by the mean and standard deviation; for categorical variables, we gave the counts for each category.-->

```{r echo = FALSE, eval = FALSE}
tibble::tribble(
  ~Variable, ~Description, ~`Statistical summary`,
  "\\code{Sex}", "Sex", "M=Male (100)",
  "", "", "F=Female (77)",
  "\\code{D}", "Disease", "ALL (73)",
  "", "", "AML (104)",
  "\\code{Phase}", "Phase", "CR1 (47)",
  "", "", "CR2 (45)",
  "", "", "CR3 (12)",
  "", "", "Relapse (73)",
  "\\code{Source}", "Type of transplant", "BM+PB (21)",
  "", "", "PB (156)",
  "\\code{Age}", "Age of patient (years)", "4--62",
  "", "", "30.47 (13.04)",
  "\\code{ftime}", "Failure time (months)", "0.13--131.77",
  "", "", "20.28 (30.78)",
  "\\code{Status}", "Status indicator", "0=censored (46)",
  "", "", "1=relapse (56)",
  "", "", "2=competing event (75)"
) %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    caption = "Baseline characteristics of patients in the stem-cell transplant study.",
    label = "table1bmtcrr",
    escape = FALSE
  ) %>%
  kable_styling()
```

First, we can look at a population-time plot to visualize the incidence density of both relapse and the competing events. In Figure \ref{fig:compPop}, failure times are highlighted on the plot using red dots for the event of interest (panel A) and blue dots for competing events (panel B). In both panels, we see evidence of a non-constant hazard function: the density of points is larger at the beginning of follow-up than at the end.

```{r compPop, fig.cap="Population-time plot for the stem-cell transplant study with both relapse and competing events.", echo=FALSE, eval = eval_cs2}

# fill_cols <- colorspace::sequential_hcl(n = 3, palette = "Viridis")
# 
# (fill_colors <- c("Case series" = fill_cols[1],
#                  "Competing event" = fill_cols[3],
#                  "Base series" = fill_cols[2]))
# 
# color_cols <- colorspace::darken(col = fill_cols, amount = 0.3)
# 
# (color_colors <- c("Case series" = color_cols[1],
#                    "Competing event" = color_cols[3],
#                    "Base series" = color_cols[2]))
# 
# pt_object <- casebase::popTime(bmtcrr, event = "Status", time = "ftime")
# 
# pt_object_comp <- bmtcrr %>%
#   mutate(Status = case_when(
#     Status == 1 ~ 2L,
#     Status == 2 ~ 1L,
#     TRUE ~ Status
#   )) %>%
#   casebase::popTime(event = "Status", time = "ftime")
# 
# plot_grid(plot(pt_object) + ggtitle("Relapse"),
#           plot(pt_object_comp, point.colour = "blue") + ggtitle("Competing events"),
#           labels = c("A", "B"))

popTimeData <- popTime(data = bmtcrr, time = "ftime")


plot(popTimeData, 
     add.competing.event = TRUE,
     comprisk = TRUE,
     # add.base.series = TRUE,
     # ratio = 0.7,
     # facet.params = list(ncol = 2),
     ribbon.params = list(fill = "gray50"),
     case.params = list(size = 0.8),
     competing.params = list(size = 0.8)) + 
  paper_gg_theme +  
  scale_y_continuous(labels = label_number_si())

# plot(popTimeData,
#      add.case.series = TRUE,
#      # add.base.series = TRUE,
#      add.competing.event = TRUE,
#      comprisk = TRUE)

```

Our main objective is to compute the absolute risk of relapse for a given set of covariates. We start by fitting a smooth hazard to the data using a linear term for time:

```{r bmtcrr-casebase-weibull, warning = FALSE, eval = eval_cs2}
model_cb <- fitSmoothHazard(
  Status ~ ftime + Sex + D + Phase + Source + Age,
  data = bmtcrr,
  ratio = 100,
  time = "ftime"
)
```

We will compare our hazard ratio estimates to that obtained from a Cox regression. To do so, we need to treat the competing event as censoring.
 
```{r bmtcrr-cox, echo = TRUE, eval = eval_cs2}
library(survival)
# Treat competing event as censoring
model_cox <- coxph(Surv(ftime, Status == 1) ~ Sex + D + Phase + Source + Age,
  data = bmtcrr
)
```

```{r bmtcrr-cis, echo=FALSE, eval = eval_cs2}
# Table of coefficients----
library(glue)
z_value <- qnorm(0.975)
foo <- summary(model_cb)@coef3
table_cb <- foo[
  !grepl("Intercept", rownames(foo)) &
    !grepl("ftime", rownames(foo)) &
    grepl(":1", rownames(foo)),
  1:2
]
table_cb <- cbind(
  table_cb[, 1],
  table_cb[, 1] - z_value * table_cb[, 2],
  table_cb[, 1] + z_value * table_cb[, 2]
)
table_cb <- round(exp(table_cb), 2)

table_cox <- round(summary(model_cox)$conf.int[, -2], 2)
rownames(table_cb) <- rownames(table_cox)
colnames(table_cb) <- colnames(table_cox)

table_cb <- as.data.frame(table_cb) %>%
  tibble::rownames_to_column("Covariates") %>%
  mutate(CI = glue::glue_data(., "({`lower .95`}, {`upper .95`})")) %>%
  rename(HR = `exp(coef)`) %>%
  select(Covariates, HR, CI)

table_cox <- as.data.frame(table_cox) %>%
  tibble::rownames_to_column("Covariates") %>%
  dplyr::mutate(CI_Cox = glue::glue_data(., "({`lower .95`}, {`upper .95`})")) %>%
  dplyr::rename(HR_Cox = `exp(coef)`) %>%
  dplyr::select(Covariates, HR_Cox, CI_Cox)

table_cb %>%
  dplyr::inner_join(table_cox, by = "Covariates") %>%
  dplyr::mutate(Covariates = dplyr::case_when(
    Covariates == "SexM" ~ "Sex",
    Covariates == "DAML" ~ "Disease",
    Covariates == "PhaseCR2" ~ "Phase (CR2 vs. CR1)",
    Covariates == "PhaseCR3" ~ "Phase (CR3 vs. CR1)",
    Covariates == "PhaseRelapse" ~ "Phase (Relapse vs. CR1)",
    Covariates == "SourcePB" ~ "Source",
    TRUE ~ Covariates
  )) %>%
  knitr::kable(
    format = "latex", booktabs = TRUE,
    col.names = c("Covariates", "HR", "95% CI", "HR", "95% CI"),
    caption = "Estimates and confidence intervals for the hazard ratios for each coefficient. Both estimates from case-base sampling and Cox regression are presented."
  ) %>%
  kable_styling() %>%
  add_header_above(c(" " = 1, "Case-Base" = 2, "Cox" = 2))
```

From the fit object, we can extract both the hazard ratios and their corresponding confidence intervals. These quantities appear in Table \ref{tab:bmtcrr-cis}. As we can see, the only significant hazard ratio identified by case-base sampling is the one associated with the phase of the disease at transplant. More precisely, being in relapse at transplant is associated with a hazard ratio of 3.89 when compared to CR1.

Given the estimate of the hazard function obtained using case-base sampling, we can compute the absolute risk curve for a fixed covariate profile. We perform this computation for a 35 year old woman who received a stem-cell transplant from peripheral blood at relapse. We compared the absolute risk curve for such a woman with ALL with that for a similar woman with AML. We will estimate the curve from 0 to 60 months.

```{r cb_risk, warning = FALSE, cache = FALSE, eval = eval_cs2}
# Pick 100 equidistant points between 0 and 60 months
time_points <- seq(0, 60, length.out = 50)

# Data.frame containing risk profile
newdata <- data.frame(
  "Sex" = factor(c("F", "F"),
    levels = levels(bmtcrr[, "Sex"])
  ),
  "D" = c("ALL", "AML"), # Both diseases
  "Phase" = factor(c("Relapse", "Relapse"),
    levels = levels(bmtcrr[, "Phase"])
  ),
  "Age" = c(35, 35),
  "Source" = factor(c("PB", "PB"),
    levels = levels(bmtcrr[, "Source"])
  )
)

# Estimate absolute risk curve
risk_cb <- absoluteRisk(
  object = model_cb, time = time_points,
  method = "numerical", newdata = newdata
)
```

We will compare our estimates to that obtained from a corresponding Fine-Gray model [-@fine1999proportional]. The Fine-Gray model is a semiparametric model for the cause-specific *subdistribution*, i.e. the function $f_k(t)$ such that
$$CI_k(t) =1 - \exp\left( - \int_0^t f_k(u) \textrm{d}u \right),$$
where $CI_k(t)$ is the cause-specific cumulative incidence. The Fine-Gray model allows to directly assess the effect of a covariate on the subdistribution, as opposed to the hazard. For the computation, we will use the \pkg{timereg} package [@timereg]:

```{r fg_risk, eval = eval_cs2, echo = TRUE}
library(timereg)
model_fg <- comp.risk(Event(ftime, Status) ~ const(Sex) + const(D) +
                        const(Phase) + const(Source) + const(Age),
                      data = bmtcrr, cause = 1, model = "fg")

# Estimate absolute risk curve
risk_fg <- predict(model_fg, newdata, times = time_points)
```

```{r bmtcrr-risk, echo = FALSE, fig.cap="\\label{fig:compAbsrisk} Absolute risk curve for a fixed covariate profile and the two disease groups. The estimate obtained from case-base sampling is compared to the Kaplan-Meier estimate.", eval = eval_cs2}
risk_all <- dplyr::bind_rows(
  data.frame(
    Time = time_points,
    Method = "Case-base",
    Risk = risk_cb[, 2],
    Disease = "ALL",
    stringsAsFactors = FALSE
  ),
  data.frame(
    Time = time_points,
    Method = "Case-base",
    Risk = risk_cb[, 3],
    Disease = "AML",
    stringsAsFactors = FALSE
  ),
  data.frame(
    Time = time_points,
    Method = "Fine-Gray",
    Risk = risk_fg$P1[1, ],
    Disease = "ALL",
    stringsAsFactors = FALSE
  ),
  data.frame(
    Time = time_points,
    Method = "Fine-Gray",
    Risk = risk_fg$P1[2, ],
    Disease = "AML",
    stringsAsFactors = FALSE
  )
)

ggplot(risk_all, aes(x = Time, y = Risk, colour = Method)) +
  # geom_line for smooth curve
  geom_line(data = dplyr::filter(risk_all, Method == "Case-base")) +
  # geom_step for step function
  geom_step(data = dplyr::filter(risk_all, Method != "Case-base")) +
  facet_grid(Disease ~ .) +
  ylim(c(0, 1)) +
  paper_gg_theme +  
  xlab("Time (in Months)") +
  ylab("Relapse risk")
```

Figure \ref{fig:compAbsrisk} shows the absolute risk curves for both case-base sampling and the Fine-Gray model. As we can see, the two approaches agree quite well for AML; however, there seems to be a difference of about 5% between the two curves for ALL. This difference does not appear to be significant: the curve from case-base sampling is contained within a 95% confidence band around the Fine-Gray absolute risk curve (figure not shown).

# Case study 3---SUPPORT Data

In the first two case studies, we described the basic functionalities of the \pkg{casebase} package: creating population-time plots, fitting parametric models for hazard functions, and estimating the corresponding cumulative incidence curves. <!--In the next two case studies, we explore more advanced features.--> For the third case study, we show how \pkg{casebase} can also be used for variable selection through regularized estimation of the hazard function. To do this, we simply replace logistic regression with a penalized counterpart. In \pkg{casebase}, we achieve this by using the \pkg{glmnet} package [@friedman2010jss]. 

To illustrate this functionality, we use the dataset from the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) [@knaus1995support]. The SUPPORT dataset tracks death in five American hospitals within individuals who are considered seriously ill. The original data is available online from the Department of Biostatistics at Vanderbilt University [@harrell_2020]. The cleaned and imputed data consists of 9104 observations and 30 variables, and it is available as part of the \pkg{casebase} package. For more information about this dataset, the reader is encouraged to look at the documentation in our package. A description of the variables can be found in Table \ref{tab:support1}, and a breakdown of each categorical variable appears in Table \ref{tab:support2}. 

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Name} & \textbf{Labels}                          & \textbf{Levels} & \textbf{Storage} & \textbf{NAs} \\
\hline
age           & Age                                      &                 & double           & 0            \\
death         & Death at any time up to NDI date:31DEC94 &                 & double           & 0            \\
sex           &                                          & 2               & integer          & 0            \\
slos          & Days from Study Entry to Discharge       &                 & double           & 0            \\
d.time        & Days of Follow-Up                        &                 & double           & 0            \\
dzgroup       &                                          & 8               & integer          & 0            \\
dzclass       &                                          & 4               & integer          & 0            \\
num.co        & number of comorbidities                  &                 & double           & 0            \\
edu           & Years of Education                       &                 & double           & 202          \\
income        &                                          & 4               & integer          & 349          \\
scoma         & SUPPORT Coma Score based on Glasgow D3   &                 & double           & 0            \\
avtisst       & Average TISS, Days 3-25                  &                 & double           & 6            \\
race          &                                          & 5               & integer          & 5            \\
sps 	        & support physiology score day 3           &	               & double           &	1            \\
aps           & APS III no coma, imp bun,uout for ph1,D3 &	               & double	          & 1            \\
hday	        & Day in Hospital at Study Admit	         &                 & double           & 0            \\	
diabetes      &	Diabetes (Com 27-28, Dx 73)	             &                 & double	          & 0            \\
dementia      & Dementia (Comorbidity 6)                 &	               & double	          & 0            \\
ca		        & Cancer State                             & 3               & integer	        & 0            \\
meanbp        & Mean Arterial Blood Pressure Day 3       &                 & double           & 0            \\
wblc          & White Blood Cell Count Day 3             &                 & double           & 24           \\
hrt           & Heart Rate Day 3                         &                 & double           & 0            \\
resp          & Respiration Rate Day 3                   &                 & double           & 0            \\
temp          & Temperature (Celsius) Day 3              &                 & double           & 0            \\
pafi          & PaO2/(.01*FiO2) Day 3                    &                 & double           & 253          \\
alb           & Serum Albumin Day 3                      &                 & double           & 378          \\
bili          & Bilirubin Day 3                          &                 & double           & 297          \\
crea          & Serum creatinine Day 3                   &                 & double           & 3            \\
sod           & Serum sodium Day 3                       &                 & double           & 0            \\
ph            & Serum pH (arterial) Day 3                &                 & double           & 250          \\
glucose       & Glucose Day 3                            &                 & double           & 470          \\
bun           & BUN Day 3                                &                 & double           & 455          \\
urine         & Urine Output Day 3                       &                 & double           & 517          \\
adlp          & Activiites of Daily Living (ADL) Patient Day 3 &           & double           & 634          \\
adlsc         & Imputed ADL Calibrated to Surrogate      &                 & double           & 0           
\end{tabular}
\caption{Description of each variable in the SUPPORT dataset.}
\label{tab:support1}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{cc}
\hline
\textbf{Variable} & \textbf{Levels}                      \\
\hline
sex      & female                                        \\
         & male                                          \\
dzgroup  & ARF/MOSF w/Sepsis                             \\
         & COPD                                          \\
         & CHF                                           \\
         & Cirrhosis                                     \\
         & Coma                                          \\
         & Colon Cancer                                  \\
         & Lung Cancer                                   \\
         & MOSF w/Malig                                  \\
dzclass  & ARF/MOSF                                      \\
         & COPD/CHF/Cirrhosis                            \\
         & Coma                                          \\
         & Cancer                                        \\
         & \textgreater{}\$50k                           \\
race     & white                                         \\
         & black                                         \\
         & asian                                         \\
         & other                                         \\
         & hispanic                                      \\
ca       & Yes                                           \\
         & No                                            \\
         & Metastatic                                    \\
\end{tabular}
\caption{Description of each level within each categorical variable in the SUPPORT dataset.}
\label{tab:support2}
\end{table}

<!--Two variables are SUPPORT day 3 physiology score (sps) and APACHE III day 3 physiology score (aps). These scores were developed for a period of 180 days, which makes our comparison over 5.56 years exploratory. Therefore, we will assess these two models before using either as our baseline, by comparing them to a full model using all variables. Also, our single imputation introduces bias in the data. As the goal here is purely demonstrative, this bias will be consistent with any of the following models. As such, their comparison is still valid, though any interpretation requires an awareness of said bias. The SUPPORT dataset will be used to demonstrate regularization within casebase, while comparing their absolute risk predictions for a new individual.--> 

In the original study, the authors developed two scores for predicting the outcome. These scores are available as part of the data: SUPPORT day 3 physiology score (\code{sps}) and APACHE III day 3 physiology score (\code{aps}). Using these scores, we will explore three Cox regression scenarios as a baseline comparison for \pkg{casebase}: \code{sps} only, \code{aps} only, and all covariates excluding \code{sps} and \code{aps}. We will compare the three models using a time-dependent version of the classical Brier score that is adjusted for censoring [@graf1999ass]. The Brier score can be used to assess both discrimination and calibration in a single model. Moreover, it provides a metric that can be used to compare parametric and semi-parametric survival models. To account for overfitting, we will use 95% of the observations to train our models, while the remaining 5% will be used to compute absolute risk curves and the Brier scores. 

```{r supportData, eval = eval_cs3}
data(support)
# Change time to years
support$d.time <- support$d.time/365.25

# Split into test and train
train_index <- sample(nrow(support), 0.95*nrow(support))
test_index <- setdiff(1:nrow(support), train_index)

train <- support[train_index,]
test <- support[test_index,]
```

In Figure\ref{fig:brier1} (A), we can see that all Cox models have similar average survival curves. On the other hand, Figure \ref{fig:brier1} (B) shows that the full model has the lowest Brier score overall. For this reason, we will use this model in our comparison between regularized Cox regression, Kaplan-Meier estimation, and the approach from \pkg{casebase}.

<!--Before generating an absolute risk curve using regularized logistic regression, we will explore three options as a baseline comparison. The original study compared these scores over a span of 180 days, which may not be comparable over the full 5.56 years. Therefore, we will examine three Cox regression scenarios: sps only, aps only and all covariates excluding sps and aps. Next, we will compare each modeling using the Brier score [@graf1999ass]. The Brier score can demonstrate both discrimination and calibration in a single model [@graf1999ass] and provide a comparable metric between semi-parametric and parametric methods in the survival context. For all the following analyses in this case study, we will use 80% of the observations to train our models, while 20% will be used to generate absolute risk curves, all of which will be averaged to demonstrate each model's predictive ability. Over 5.56 years, \ref{fig:baselines} demonstrates that all the cox models have similar, if not identical average predictions. Figure \ref{fig:brier1} examines the brier scores as they change over time. As the full model has the lowest brier score on overall, it will be used in the following absolute risk comparison to regularized cox, regularized logistic regression and the unadjusted Kaplan-Meier curve.-->

```{r baselinePlotsPrep, eval=eval_cs3, echo = FALSE}
# Define colour palette for this section
q4 <- qualitative_hcl(4, palette = "Dark 3")

# 1. Cox with only sps
coxSPS <- survival::coxph(Surv(time = d.time, event = death) ~ sps, 
                          data = train, x = TRUE)
abcoxSPS <- survival::survfit(coxSPS, newdata = test)
# 2. Cox with only aps
coxAPS <- survival::coxph(Surv(time = d.time, event = death) ~ aps, 
                          data = train, x = TRUE)
abcoxAPS <- survival::survfit(coxAPS, newdata = test)
# 3. Cox with everything but sps and aps
coxFull <- survival::coxph(Surv(time = d.time, event = death) ~ . - aps - sps, 
                           data = train, x = TRUE)
abcoxFull <- survival::survfit(coxFull, newdata = test)
# 4. Kaplan-Meier
KM <- survival::coxph(Surv(time = d.time, event = death) ~ 1, data = test)
abKM <- survival::survfit(KM)
abKMcompare <- data.frame(
  Time = abKM$time,
  KM = 1 - abKM$surv
)

# Combine all CI estimates from Cox models
baseCoxComparisonData <- data.frame(
  Time = abcoxSPS$time,
  SPS = 1 - rowMeans(abcoxSPS$surv),
  APS = 1 - rowMeans(abcoxAPS$surv),
  Full = 1 - rowMeans(abcoxFull$surv)
)
```

```{r baselines, echo=FALSE, eval=eval_cs3}
cox_ci <- ggplot(baseCoxComparisonData, aes(Time)) +
  geom_line(aes(y = APS, colour = "APS"), lwd = 1) +
  geom_line(aes(y = Full, col = "Full"), lwd = 1) +
  geom_line(aes(y = SPS, colour = "SPS"), lwd = 1) +
  geom_step(data = abKMcompare, mapping = aes(x = Time, y = KM, color = "K-M"), lwd = 1) +
  labs(y = "Probability of Death", x = "Survival time (Years)", color = "Models") +
  scale_color_manual(values = q4, breaks = c("APS", "Full", "SPS", "K-M")) + 
  paper_gg_theme
```

```{r brierpt1, echo=FALSE, eval=eval_cs3, fig.cap="\\label{fig:brier1} Comparison of three Cox regression models. (A) Cumulative Incidence curves. (B) Time-dependent Brier scores."}
# Extract time points
times <- sort(unique(test$d.time))
# Compute Brier scores
brierAPS <- Score(list("PredictionModel" = coxAPS), data = test, 
                  formula = Hist(d.time, death != 0) ~ 1, summary = "ipa", 
                  se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)
brierSPS <- Score(list("PredictionModel" = coxSPS), data = test, 
                  formula = Hist(d.time, death != 0) ~ 1, summary = "ipa", 
                  se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)

# First fix NA coefficients, then compute Brier score
coxFull$coefficients[which(is.na(coxFull$coefficients))] <- 0
brierFull <- Score(list("PredictionModel" = coxFull), data = test, 
                   formula = Surv(d.time, death != 0) ~ 1, summary = "ipa", 
                   se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)

# Combine all results for plotting
brierPlotter <- rbind(
  data.frame(brierScore = brierAPS$Brier$score$Brier[brierAPS$Brier$score$model == "PredictionModel"], 
             times = times, model = "APS"),
  data.frame(brierScore = brierFull$Brier$score$Brier[brierFull$Brier$score$model == "PredictionModel"], 
             times = times, model = "Full"),
  data.frame(brierScore = brierSPS$Brier$score$Brier[brierSPS$Brier$score$model == "PredictionModel"], 
             times = times, model = "SPS")
)

cox_brier <- ggplot(data = brierPlotter, mapping = aes(x = times, y = brierScore, col = model)) +
  geom_line() +
  xlab("Survival time (Years)") +
  ylab("Brier score") +
  labs(color = "Models") +
  # ggtitle("Brier scores over time") +
  scale_color_manual(values = q4[1:3]) + 
  paper_gg_theme

# Combine both plots using cowplot
plot_row <- cowplot::plot_grid(cox_ci + theme(legend.position = 'none'),
                               cox_brier + theme(legend.position = 'none'),
                               labels = c("A", "B"))
legend <- cowplot::get_legend(
  # create some space to the left of the legend
  cox_ci + theme(legend.box.margin = margin(0, 0, 0, 12))
)

cowplot::plot_grid(plot_row, legend, rel_heights = c(3, .4), nrow = 2)
```

<!--To get the absolute risk following regularized cox regression, we had to work around limitations in existing packages. To make use of the \pkg{survival} package, we first estimate the hazard coefficients using thecoxnet implementation for lasso penalization in \pkg{glmnet} [@regpathcox]. Next, we fit a cox model using the survival package with the selected variables only, from our regularized fit. Finally, we replaced the coefficients in the survival object with those from our glmnet fit, allowing us to make use of the absolute risk tools available in the \pkg{survival} package.-->

For our penalized logistic regression model, we opt for the natural log of time; recall that this corresponds to a Weibull distribution. For fitting the penalized hazard, we use \code{fitSmoothHazard.fit}, which is a matrix interface to the function \code{fitSmoothHazard}. In order to use this interface, we first create a matrix \code{y} containing the time and event variables, and a matrix \code{x} containing all remaining covariates we would like to include in the model space. We used lasso penalization by setting \code{alpha = 1}. Moreover, we use the argument \code{penalty.factor} to force the time variable into the final model. 
 
```{r supportCB_fit, eval=eval_cs3, echo=TRUE, cache=FALSE}
x <- model.matrix(death ~ . - d.time - aps - sps, 
                  data = train)[, -c(1)] # Remove intercept
y <- data.matrix(subset(train, select = c(d.time, death)))

# Regularized logistic regression to estimate hazard
cbFull <- casebase::fitSmoothHazard.fit(x, y,
  family = "glmnet",
  time = "d.time", event = "death",
  formula_time = ~ log(d.time), alpha = 1,
  ratio = 10, standardize = TRUE,
  penalty.factor = c(0, rep(1, ncol(x)))
)
```

Next, we can use the object \code{cbFull} to estimate the absolute risk curve on the test data. We first create a matrix \code{newx} containing the relevant variables. Next, using \code{times}, we specify the time points at which the cumulative incidence will be computed. Finally, using the parameter \code{s = "lambda.1se"}, we specify at which value of the tuning parameter we want to retrieve the coefficient estimates.

```{r supportCB_abs, eval=eval_cs3, echo=TRUE, cache=TRUE}
# Estimating the absolute risk curve using the newdata parameter
newx <- model.matrix(death ~ . - d.time - aps - sps, 
                  data = test)[, -c(1)]
times <- sort(unique(test$d.time))

abCbFull <- casebase::absoluteRisk(cbFull,
  time = times,
  newdata = newx,
  s = "lambda.1se",
  method = "numerical"
)
```

```{r coxHazAbsolute, echo = FALSE, eval = eval_cs3}
# Create survival object for fitting Coxnet
u <- with(train, survival::Surv(time = d.time, event = death))
coxNet <- glmnet::cv.glmnet(x = x, y = u, family = "cox", alpha = 1, standardize = TRUE)

# Taking the coefficient estimates for later use
nonzero_covariate_cox <- predict(coxNet, type = "nonzero", s = "lambda.1se")
nonzero_coef_cox <- coef(coxNet, s = "lambda.1se")
# Creating a new dataset that only contains the covariates chosen through glmnet
cleanCoxData <- as.data.frame(cbind(y, x[, nonzero_covariate_cox$X1]))

# Fitting a cox model using regular estimation, however we will not keep it.
# this is used more as an object place holder.
coxNet <- survival::coxph(Surv(time = d.time, event = death) ~ ., 
                          data = cleanCoxData, x = TRUE)

# The coefficients of this object will be replaced with the estimates from the
# original coxNet. Doing so makes it so that everything is invalid aside from
# the coefficients. In this case, all we need to estimate the absolute risk is
# the coefficients. Std. error would be incorrect here, if we were to draw error
# bars.
coxNet_coefnames <- names(coxNet$coefficients)
coxNet$coefficients <- nonzero_coef_cox@x
names(coxNet$coefficients) <- coxNet_coefnames

# Fitting absolute risk curve for cox+glmnet
abcoxNet <- survival::survfit(coxNet, type = "breslow", 
                              newdata = as.data.frame(newx[, nonzero_covariate_cox$X1]))

# Combine CI estimates from both penalized methods
abCN <- data.frame(
  Time = abcoxNet$time,
  absRisk = 1 - rowMeans(abcoxNet$surv)
)
abCN$Model <- "Penalized Cox"

abCB <- data.frame(Time = abCbFull[, 1],
                   absRisk = rowMeans(abCbFull[, -c(1)]))
abCB$Model <- "Penalized logistic"

ab <- rbind(abCB, abCN)
```

Our first comparison plot examines the covariates that were selected between all three hazard functions. Note that Cox regression without penalization selected more covariates than the other two, but only the intersection of coefficients with values greater than 0 are displayed in \ref{fig:cs3lolliPlot}. A visible tendency is for Cox regression to have larger estimated coefficients. This is expected, as there are no restrictions on the size of coefficients. The smallest coefficient estimates are found in penalized logistic model, aside from cancer state (cano). 

```{r coefplots, echo=FALSE, eval=eval_cs3, fig.cap="\\label{fig:cs3lolliPlot} Lollipop plot showing the coefficient estimates between three models. If a coefficient estimate is 0 in any model, it is removed from the plot for simplicity."}
library(dotwhisker)
library(broom)
library(dplyr)

# extract coefficients for lollipop plot
estimatescoxNet <- setDT(as.data.frame(coef(coxNet)), 
                         keep.rownames = TRUE)

estimatescoxNet$Model <- "Pen. Cox"
colnames(estimatescoxNet) <- c("term", "estimate", "model")

estimatescb <- setDT(as.data.frame(coef(cbFull)[-c(1, 2), 1]), 
                     keep.rownames = TRUE)
# estimatescb$std.error<-0
estimatescb$Model <- "Pen. logistic"
colnames(estimatescb) <- c("term", "estimate", "model")

estimatescoxFull <- setDT(as.data.frame(coef(coxFull)), 
                          keep.rownames = TRUE)
# estimatescoxFull$std.error<-0
estimatescoxFull$Model <- "Cox"
colnames(estimatescoxFull) <- c("term", "estimate", "model")

# Clean up the data and the variable names
lolliplotDots <- rbind(estimatescoxFull, estimatescoxNet, estimatescb)
lolliplotDots$conf.low <- lolliplotDots$estimate
lolliplotDots$conf.high <- lolliplotDots$estimate
lolliplotDots$conf.high[lolliplotDots$estimate < 0] <- 0
lolliplotDots$conf.low[lolliplotDots$estimate > 0] <- 0
lolliplotDots$term <- gsub("\`", "", as.character(lolliplotDots$term))

lolliplotDots[which(lolliplotDots$estimate == 0), c(2, 4, 5)] <- NA
unChosen <- unique(lolliplotDots[which(is.na(lolliplotDots$estimate)), 1])

lolliplotDots[which(lolliplotDots$term %in% unChosen$term), c(2, 4, 5)] <- NA

lolliplotDots <- lolliplotDots[-which(is.na(lolliplotDots$estimate)), ]
dwplot(lolliplotDots) +
  theme(
    axis.text.y = element_text(size = 11, angle = 0, hjust = 1, vjust = 0),
    strip.text.x = element_blank(),
    strip.background = element_rect(colour = "white", fill = "white"),
    legend.position = c(.855, 0.15), legend.text = element_text(size = 11)
  ) +
  paper_gg_theme +  
  # ggtitle("Coefficient plot") +
  labs(color = "Models") +
  scale_color_manual(values = q4[1:3])
```

These values do not inform us of model performance. To do so, we take our hazard functions and use them to predict the absolute risk of everyone in our test set, over time. The probabilities over time for each individual are averaged, resulting in the absolute risk curves in Figure \ref{fig:cs3FinalBrier} (A). There is separation between the curves, but it is minimal. To better demonstrate the differences in performance between the models, we compare Brier scores over time. In Figure \ref{fig:cs3FinalBrier} (B), the Brier score is lower in our penalized logistic regression model compared to both Cox regression models. However, this difference is minimal. On the other hand, all three models outperform the Kaplan-Meier model. 

```{r cs3Final, echo=FALSE, eval=eval_cs3}
reg_ci <- ggplot(ab, aes(Time)) +
  geom_line(data = baseCoxComparisonData, mapping = aes(y = Full, colour = "Cox"), lwd = 1) +
  geom_line(aes(y = absRisk, colour = Model), lwd = 1) +
  geom_step(data = abKMcompare, mapping = aes(x = Time, y = KM, color = "K-M"), lwd = 1) +
  labs(y = "Probability of Death", x = "Survival time (Years)", color = "Models") +
  # ggtitle("Absolute risk curves") +
  scale_color_manual(values = q4, breaks = c("Cox", "Penalized Cox", "Penalized logistic", "K-M")) + 
  paper_gg_theme
```
  
```{r eval = eval_cs3, echo = FALSE, warning = FALSE}
# We need this chunk until this method reaches the CRAN version of riskRegression
predictRisk.singleEventCB <- function(object, newdata, times, cause, ...) {
  if (!is.null(object$matrix.fit)) {
    #get all covariates excluding intercept and time
    coVars=colnames(object$originalData$x)
    #coVars is used in lines 44 and 50
    newdata=data.matrix(drop(subset(newdata, select=coVars)))
  }
  
  # if (missing(cause)) stop("Argument cause should be the event type for which we predict the absolute risk.")
  # the output of absoluteRisk is an array with dimension dependening on the length of the requested times:
  # case 1: the number of time points is 1
  #         dim(array) =  (length(time), NROW(newdata), number of causes in the data)
  if (length(times) == 1) {
    a <- casebase::absoluteRisk(object, newdata = newdata, time = times)
    p <- matrix(a, ncol = 1)
  } else {
    # case 2 a) zero is included in the number of time points
    if (0 %in% times) {
      # dim(array) =  (length(time)+1, NROW(newdata)+1, number of causes in the data)
      a <- casebase::absoluteRisk(object, newdata = newdata, time = times)
      p <- t(a)
    } else {
      # case 2 b) zero is not included in the number of time points (but the absoluteRisk function adds it)
      a <- casebase::absoluteRisk(object, newdata = newdata, time = times)
      ### we need to invert the plot because, by default, we get cumulative incidence
      #a[, -c(1)] <- 1 - a[, -c(1)]
      ### we remove time 0 for everyone, and remove the time column
      a <- a[-c(1), -c(1)] ### a[-c(1), ] to keep times column, but remove time 0 probabilities
      # now we transpose the matrix because in riskRegression we work with number of
      # observations in rows and time points in columns
      p <- t(a)
    }
  }
  if (NROW(p) != NROW(newdata) || NCOL(p) != length(times)) {
    stop(paste("\nPrediction matrix has wrong dimensions:\nRequested newdata x times: ", 
               NROW(newdata), " x ", length(times), "\nProvided prediction matrix: ", 
               NROW(p), " x ", NCOL(p), "\n\n", sep = ""))
  }
  p
}
```

```{r riskregressionBrier, echo = FALSE, eval = eval_cs3, fig.cap="\\label{fig:cs3FinalBrier} Four curves comparing the Cumulative Incidence (A) and the Brier scores (B) as they change over time between Cox regression, penalized Cox regression, penalized logistic regression, and Kaplan-Meier.", warning = FALSE}
# Compute Brier score for penalized models
brierFinalResults <- Score(list("Penalized Cox" = coxNet, "Penalized Logistic" = cbFull),
                           data = cbind(subset(test, select = c(d.time, death)),
                                        as.data.frame(newx)), 
                           formula = Hist(d.time, death != 0) ~ 1, summary = NULL, 
                           se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)

# Combine Brier scores between unpenalized and penalized models before plotting
data_brier <- bind_rows(
  brierFinalResults$Brier$score,
  filter(brierFull$Brier$score, model == "PredictionModel")
  ) %>% 
  mutate(model = case_when(
    model == "Null model" ~ "K-M",
    model == "PredictionModel" ~ "Cox",
    TRUE ~ model
  ),
  model = factor(model, levels = c("Cox", "Penalized Cox", "Penalized Logistic", "K-M"),
                 labels = c("Cox", "Pen. Cox", "Pen. Logistic", "K-M")))

reg_brier <- ggplot(data = data_brier, mapping = aes(x = times, y = Brier, col = model)) +
  geom_line() +
  xlab("Survival time (Years)") +
  ylab("Brier score") +
  # ggtitle("Brier score over time") +
  labs(color = "Models") +
  paper_gg_theme +  
  scale_color_manual(values = q4)

# Combine both plots using cowplot
plot_row_reg <- cowplot::plot_grid(reg_ci + theme(legend.position = 'none'),
                                   reg_brier + theme(legend.position = 'none'),
                                   labels = c("A", "B"))
legend_reg <- cowplot::get_legend(
  # create some space to the left of the legend
  reg_ci + theme(legend.box.margin = margin(0, 0, 0, 12))
)

cowplot::plot_grid(plot_row_reg, legend_reg, rel_heights = c(3, .4), nrow = 2)
```

# Case study 4---Stanford Heart Transplant Data

In the previous case studies, we only considered covariates that were fixed at baseline. In this next case study, we will use the Stanford Heart Transplant data [@clark1971cardiac,@crowley1977covariance] to show how case-base sampling can also be used in the context of time-varying covariates. As an example that already appeared in the literature, case-base sampling was used to study vaccination safety, where the exposure period was defined as the week following vaccination [@saarela2015case]. Hence, the main covariate of interest, i.e. exposure to the vaccine, was changing over time. In this context, case-base sampling offers an efficient alternative to nested case-control designs or self-matching.

Recall the setting of Stanford Heart Transplant study: patients were admitted to the Stanford program after meeting with their physician and determining that they were unlikely to respond to other forms of treatment. After enrollment, the program searched for a suitable donor for the patient, which could take anywhere between a few days to almost a year. We are interested in the effect of a heart transplant on survival; therefore, the patient is considered exposed only after the transplant has occurred. 

As before, we can look at the population-time plot for a graphical summary of the event incidence. As we can see, most events occur early during the follow-up period, and therefore we do not expect the hazard to be constant.

```{r stanford-poptime, eval = eval_cs4}
stanford_popTime <- popTime(jasa,
  time = "futime",
  event = "fustat"
)
plot(stanford_popTime,
     ribbon.params = list(fill = "grey50")) + 
  paper_gg_theme
```

Since the exposure is time-dependent, we need to manually define the exposure variable *after* case-base sampling and *before* fitting the hazard function. For this reason, we will use the `sampleCaseBase` function directly.

```{r, eval = eval_cs4}
cb_data <- sampleCaseBase(jasa,
  time = "futime",
  event = "fustat", ratio = 10
)
```

Next, we will compute the number of days from acceptance into the program to transplant, and we use this variable to determine whether each population-moment is exposed or not.

```{r, eval = eval_cs4}
# Define exposure variable
cb_data <- mutate(cb_data,
  txtime = time_length(accept.dt %--% tx.date,
                       unit = "days"
  ),
  exposure = case_when(
    is.na(txtime) ~ 0L,
    txtime > futime ~ 0L,
    txtime <= futime ~ 1L
  )
)
```

Finally, we can fit the hazard using various linear predictors.

```{r, eval = eval_cs4, warning = FALSE}
library(splines)
# Fit several models
fit1 <- fitSmoothHazard(fustat ~ exposure,
  data = cb_data, time = "futime"
)
fit2 <- fitSmoothHazard(fustat ~ exposure + futime,
  data = cb_data, time = "futime"
)
fit3 <- fitSmoothHazard(fustat ~ exposure + bs(futime),
  data = cb_data, time = "futime"
)
fit4 <- fitSmoothHazard(fustat ~ exposure * bs(futime),
  data = cb_data, time = "futime"
)
```

Note that the fourth model includes an interaction term between exposure and follow-up time. In other words, this model no longer exhibit proportional hazards. The evidence of non-proportionality of hazards in the Stanford Heart Transplant data has been widely discussed [@arjas1988graphical].

We can then compare the goodness of fit of these four models using the Akaike Information Criterion (AIC).

```{r, eval = eval_cs4}
# Compute AIC
c("Model1" = AIC(fit1),
  "Model2" = AIC(fit2),
  "Model3" = AIC(fit3),
  "Model4" = AIC(fit4))
```

As we can, the best fit is the fourth model. By visualizing the hazard functions for both exposed and unexposed individuals, we can more clearly see how the hazards are no longer proportional.

```{r stanford-hazard, eval = eval_cs4, echo=TRUE}
# Compute hazards---
# First, create a list of time points for both exposure status
hazard_data <- expand.grid(
  exposure = c(0, 1),
  futime = seq(0, 1000,
    length.out = 100
  )
)

# Set the offset to zero
hazard_data$offset <- 0
# Use predict to get the fitted values, and exponentiate to
# transform to the right scale
hazard_data$hazard <- exp(predict(fit4,
  newdata = hazard_data,
  type = "link"
))
# Add labels for plots
hazard_data$Status <- factor(hazard_data$exposure,
  labels = c("NoTrans", "Trans")
)

ggplot(hazard_data, aes(futime, hazard, colour = Status)) +
  geom_line() +
  paper_gg_theme +  
  ylab("Hazard") +
  xlab("Follow-up time")
```

The non-proportionality seems to be more pronounced at the beginning of follow-up than the end. Finally, we can turn these estimates of the hazard function into estimates of the cumulative incidence functions.

```{r stanford-risk, eval = eval_cs4}
# Compute absolute risk curves
newdata <- data.frame(exposure = c(0, 1))
absrisk <- absoluteRisk(fit4,
  newdata = newdata,
  time = seq(0, 1000, length.out = 100)
)

class(absrisk)

plot(absrisk,
     id.names = c("NoTrans","Trans")) + 
  paper_gg_theme

# colnames(absrisk) <- c("Time", "NoTrans", "Trans")
# 
# # Rearrange the data
# absrisk <- gather(
#   as.data.frame(absrisk),
#   "Status", "Risk", -Time
# )
# 
# ggplot(absrisk, aes(Time, Risk, colour = Status)) +
#   geom_line() +
#   theme_minimal() +
#   theme(legend.position = "top") +
#   expand_limits(y = 0.5) +
#   xlab("Follow-up time") +
#   ylab("Cum. Incidence")
```

Note that we can easily adapt the code above to the situation where a patient receives a heart transplant at a point in time of interest, for example after 30 days.

```{r stanford-1y-haz, eval = eval_cs4}
# Compute hazards---
# First, create a list of time points for both exposure status
one_yr_haz <- data.frame(futime = seq(0, 365,
  length.out = 100
))
one_yr_haz <- mutate(one_yr_haz,
  offset = 0,
  exposure = if_else(futime < 30, 0, 1)
)
one_yr_haz$hazard <- exp(predict(fit4,
  newdata = one_yr_haz,
  type = "link"
))
ggplot(one_yr_haz, aes(futime, hazard)) +
  geom_line() +
  paper_gg_theme +  
  ylab("Hazard") +
  xlab("Follow-up time") +
  geom_vline(xintercept = 30, linetype = "dashed")
```

We can then compare the 1-year mortality risk without transplant and with transplant at 30 days.

```{r stanford-1y-risk, eval = eval_cs4}
absoluteRisk(fit4,
  newdata = data.frame(exposure = 0),
  time = 365
)

# Use the trapezoidal rule to estimate the cumulative hazard
cumhaz_est <- pracma::trapz(one_yr_haz$futime, one_yr_haz$hazard)
1 - exp(-cumhaz_est)
```

As we can see, the risk estimate at 1-year is about 30% lower if the patient receives a heart transplant at 30 days.

We can also plot the hazard ratio:
```{r eval = eval_cs4}
newtime <- quantile(fit4[["originalData"]][[fit4[["timeVar"]]]],
                    probs = seq(0.01, 0.99, 0.01))

# reference category
newdata <- data.frame(exposure = 0,
                      futime = newtime)

plot(fit4,
     type = "hr",
     newdata = newdata,
     var = "exposure",
     increment = 1,
     xvar = "futime",
     ci = FALSE,
     rug = TRUE)
```

# Discussion

In this article, we presented the \proglang{R} package \pkg{casebase}, which provides functions for fitting smooth parametric hazards and estimating CIFs using case-base sampling. We outlined the theoretical underpinnings of the approach, we provided details about our implementation, and we illustrated the merits of the approach and the package through four case studies. 

As a methodological framework, case-base sampling is very flexible. This flexibility has been explored before in the literature: for example, Saarela and Hanley [-@saarela2015case] used case-base sampling to model a time-dependent exposure variable in a vaccine safety study. As another example, Saarela and Arjas [-@saarela2015non] combined case-base sampling and a Bayesian non-parametric framework to compute individualized risk assessments for chronic diseases. In the case studies above, we explored this flexibility along two fronts. On the one hand, we showed how splines could be used as part of the linear predictor to model the effect of time on the hazard. This strategy yielded estimates of the survival function that were qualitatively similar to semiparametric estimates derived from Cox regression; however, case-base sampling led to estimates of the survival function that *vary smoothly in time*. On the other hand, we also displayed the flexibility of case-base sampling by showing how it could be combined with penalized logistic regression to perform variable selection. Even though we did not illustrate it in this article, case-base sampling can also be combined with the framework of *generalized additive models*. This functionality has been implemented in our package \pkg{casebase}. Similarly, case-base sampling can also be combined with quasi-likelihood estimation to fit survival models that can account for the presence of over-dispersion. All of these examples illustrate how the case-base sampling framework in general, and the package \pkg{casebase} in particular, allows the user to fit a broad and flexible family of survival functions. 

As presented in Hanley & Miettinen [-@hanley2009fitting], case-base sampling is comprised of three steps: 1) sampling a case series and a base series from the study; 2) fit the log-hazard as a linear function of predictors (including time); and 3) use the fitted hazard to estimate the CIF. Accordingly, our package provides functions for each step. Moreover, the simple interface of the \code{fittingSmoothHazard} function resembles the \code{glm} interface. This interface should look familiar to new users. Our modular approach also provides a convenient way to extend our package for new sampling or fitting strategies.

In the case studies above, we compared the performance of case-base sampling with that of Cox regression and Fine-Gray models. In terms of function interface, \pkg{casebase} uses a formula interface that is closer to that of \code{glm}, in that the event variable is the only variable appearing on the left-hand side of the formula. By contrast, both \code{survival::coxph} and \code{timereg::comp.risk} use arrays that capture both the event type and time. Both approaches to modeling yield user-friendly code. However, in terms of output, both approaches differ significantly. Case-base sampling produces smooth hazards and smooth cumulative incidence curves, whereas Cox regression and Fine-Gray models produce step-wise cumulative incidence functions and never explicitly model the hazard function. Qualitatively, we showed that by using splines in the linear predictor, all three models yielded similar curves. However, the smooth nature of the output of \pkg{casebase} provides a more intuitive interpretation for consumers of these predictions. In Table \ref{tab:compCBvsCox}, we provide a side-by-side comparison between the Cox model and case-base sampling.

\begin{table}
\caption{\label{tab:compCBvsCox}Comparison between the Cox model and case-base sampling}
\centering
\begin{tabular}[t]{llp{5cm}}
\toprule
Feature & Cox model & Case-base sampling\\
\midrule
Model type & Semi-parametric & Fully parametric\\
Time & Left hand side of the formula & Right hand side (allows flexible modeling of time)\\
Cumulative incidence & Step function & Smooth-in-time curve\\
Non-proportional hazards & Interaction of covariates with time & Interaction of covariates with time\\
Model testing &  & Use GLM framework \newline (e.g.\ LRT, AIC, BIC)\\
\addlinespace
Competing risks & Difficult & Cause-specific CIFs\\
% Prediction & Kaplan-Meier-based & ROC, AUC, risk reclassification probabilities\\
\bottomrule
\end{tabular}
\end{table}

Our choice of modeling the log-hazard as a linear function of covariates allows us to develop a simple computational scheme for estimation. However, as a downside, it does not allow us to model location and scale parameters separately like the package \pkg{flexsurv}. For example, if we look at the Weibull distribution as parametrised in \code{stats::pweibull}, the log-hazard function is given by
$$ \log \lambda(t; \alpha, \beta) = \left[\log(\alpha/\beta) - (\alpha - 1)\log(\beta)\right] + (\alpha - 1)\log t,$$
where $\alpha,\beta$ are shape and scale parameters, respectively. Unlike \pkg{casebase}, the approach taken by \pkg{flexsurv} also allows the user to model the scale parameter as a function of covariates. Of course, this added flexibility comes at the cost of interpretability: by modeling the log-hazard directly, the parameter estimates from \pkg{casebase} can be interpreted as estimates of log-hazard ratios. To improve the flexibility of \pkg{casebase} at capturing the scale of a parametric family, we could replace the logistic regression with its quasi-likelihood counterpart and therefore model over- and under-dispersion with respect to the logistic likelihood. We defer the study of the properties and performance of such a model to a future article.

Future work will look at some of the methodological extensions of case-base sampling. First, to assess the quality of the model fit, we want to study the properties of the residuals (e.g. Cox-Snell, martingale). More work needs to be done to understand these residuals in the context of the partial likelihood underlying case-base sampling. The resulting diagnostic tools would then be integrated in this package. Also, we are interested in extending case-base sampling to account for interval censoring. This type of censoring is very common in longitudinal studies, and many packages (e.g. \pkg{SmoothHazard}, \pkg{survival} and \pkg{Rstpm2}) provide functions to account for it. Again, we hope to include any resulting methodology as part of this package.

In future versions of the package, we also want to increase the complement of diagnostic and inferential tools that are currently available. For example, we would like to include the ability to compute confidence intervals for the cumulative incidence curve. The delta method or parametric bootstrap are two different strategies we can use to construct approximate confidence intervals. Furthermore, we would like to include more functions to compute calibration and discrimination statistics (e.g. AUC) for our models. Saarela and Arjas [-@saarela2015non] also describe how to obtain a posterior distribution for the AUC from their model. Their approach could potentially be included in \pkg{casebase}. Finally, we want to provide more flexibility in how the case-base sampling is performed. This could be achieved by adding a \code{hazard} argument to the function \code{sampleCaseBase}. In this way, users could specify their own sampling mechanism. For example, they could provide a hazard that gives sampling probabilities that are proportional to the cardiovascular disease event rate given by the Framingham score [@saarela2015non]. 

In conclusion, we presented the \proglang{R} package \pkg{casebase} which implements case-base sampling for fitting parametric survival models and for estimating smooth cumulative incidence functions using the framework of generalized linear models. We strongly believe that its flexibility and its foundation on the familiar logistic regression model will make it appealing to new and established practioners. 

```{r, echo=FALSE, eval = FALSE}
knitr::kable(data.frame(
  `feature` = c("model type", "time", "cumulative incidence", "non-proportional hazards", "model testing", "competing risks", "prediction"),
  `Cox` = c("semi-parametric", "left hand side of the equation", "step function", "interaction of covariates with time", " ", "difficult", "Kaplan-Meier-based"),
  `Case Base Sampling` = c(
    "fully parametric (logistic/multinomial regression)", "right hand side - allows flexible modeling of time",
    "smooth-in-time curve", "interaction of covariates with time", "make use of GLM framework (LRT, AIC, BIC)",
    "cause-specific cumulative incidence functions (CIFs) directly obtained via multinomial regression", "ROC, AUC, risk reclassification probabilities"
  )
),
col.names = c("Feature", "Cox model", "Case-base sampling"),
format = "latex", booktabs = TRUE,
caption = "Comparison between the Cox model and case-base sampling",
escape = FALSE
)
```

# Environment Details

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, eval = eval_colophon, cache = FALSE}
# which R packages and versions?
# devtools::session_info()
sessionInfo()
```

The current Git commit details are:
```{r, eval = eval_colophon}
# what commit is this file at?
git2r::repository(here::here())
```
