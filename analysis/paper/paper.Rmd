---
documentclass: jss
author:
  - name: Sahir Bhatnagar *
    affiliation: McGill Univeristy
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{sahir.bhatnagar@mail.mcgill.ca}
    url: http://sahirbhatnagar.com/
  - name: Maxime Turgeon *
    affiliation: University of Manitoba
    address: >
      318 Machray Hall
      Winnipeg, MB, Canada R3T 2N2
    email: \email{max.turgeon@umanitoba.ca}
    url: http://maxturgeon.ca/
  - name: Jesse Islam 
    affiliation: McGill University
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{jesse.islam@mail.mcgill.ca}
  - name: James Hanley
    affiliation: McGill Univeristy
    address: >
      1020 Pine Avenue West
      Montreal, QC, Canada H3A 1A2
    email: \email{james.hanley@mcgill.ca}
    url: http://www.medicine.mcgill.ca/epidemiology/hanley/
  - name: Olli Saarela
    affiliation: University of Toronto
    address: >
      Dalla Lana School of Public Health, 
      155 College Street, 6th floor, 
      Toronto, Ontario 
      M5T 3M7, Canada
    email: \email{olli.saarela@utoronto.ca}
    url: http://individual.utoronto.ca/osaarela/
title:
  formatted: "\\pkg{casebase}: An Alternative Framework For Survival Analysis"
  # If you use tex in the formatted title, also supply version without
  plain:     "casebase: An Alternative Framework For Survival Analysis"
  # For running headers, if needed
  short:     "\\pkg{casebase}: An Alternative Framework For Survival Analysis"
abstract: >
  The abstract of the article. * joint co-authors
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
  \usepackage{longtable}
  \usepackage{graphicx}
output: 
  rticles::jss_article:
    number_sections: TRUE     #added argument option 
    citation_package: "natbib"  #All my citations use biblatex, not natbib. 
biblio-style: jss      #Listed to use in JSS Instructions for Authors, but not in template by default. 
bibliography: references.bib  #Also not included in template by default. 
fig_width: 7
fig_height: 6
fig_caption: true
---

# Code formatting

Don't use markdown, instead use the more precise latex commands:

* \proglang{Java}
* \pkg{plyr}
* \code{print("abc")}
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  comment = "#>",
  fig.path = "../figures/"
)

library(tidyverse)
library(magrittr)
library(survival)
library(casebase)
library(splines)
#library(readr)
#library(tibble)
library(glmnet)
options(digits = 2, scipen = 999)
# set the seed for reproducible output
set.seed(2)

eval_introduction <- TRUE
eval_theory <- FALSE
eval_implementation <- TRUE
eval_cs1 <- FALSE
eval_cs2 <- FALSE
eval_cs3 <- FALSE
```

# Introduction

The purpose of the `casebase` package is to provide practitioners with an easy-to-use software tool to predict the risk (or cumulative incidence (CI)) of an event, for a particular patient. The following points should be noted:

hazard ratios
2. If, however, the absolute risks are of interest, they have to be recovered using the semi-parametric Breslow estimator
3. Alternative approaches for fitting flexible hazard models for estimating absolute risks, not requiring this two-step approach? Yes!
[@hanley2009fitting]



[@hanley2009fitting] propose a fully parametric hazard model that can be fit via logistic regression. From the fitted hazard function, cumulative incidence and, thus, risk functions of time, treatment and profile can be easily derived.




\begin{table}
  \begin{tabular}{p{1.8in}lll}
\hline
    &  Parameters &  Density \proglang{R} function & \code{dist}\\
    & {\footnotesize{(location in \code{\color{red}{red}})}} & & \\
\hline
    Exponential & \code{\color{red}{rate}}             & \code{dexp}   & \code{"exp"} \\
    Weibull (accelerated failure time)     & \code{shape, {\color{red}{scale}}}     & \code{dweibull} & \code{"weibull"} \\
    Weibull (proportional hazards)     & \code{shape, {\color{red}{scale}}}     & \code{dweibullPH} & \code{"weibullPH"} \\
    Gamma       & \code{shape, \color{red}{rate}}      & \code{dgamma} & \code{"gamma"}\\
    Log-normal  & \code{{\color{red}{meanlog}}, sdlog}   & \code{dlnorm} & \code{"lnorm"}\\
    Gompertz    & \code{shape, {\color{red}{rate}}}      & \code{dgompertz} & \code{"gompertz"} \\
    Log-logistic & \code{shape, {\color{red}{scale}}}   & \code{dllogis} & \code{"llogis"}\\
    Generalized gamma (Prentice 1975)   & \code{{\color{red}{mu}}, sigma, Q} & \code{dgengamma} & \code{"gengamma"} \\
    Generalized gamma (Stacy 1962)& \code{shape, {\color{red}{scale}}, k} & \code{dgengamma.orig} & \code{"gengamma.orig"} \\
    Generalized F     (stable)    & \code{{\color{red}{mu}}, sigma, Q, P} & \code{dgenf} & \code{"genf"} \\
    Generalized F     (original)  & \code{{\color{red}{mu}}, sigma, s1, s2} & \code{dgenf.orig} & \code{"genf.orig"} \\
\hline
  \end{tabular}
  \caption{Built-in parametric survival distributions in \pkg{flexsurv}.}
  \label{tab:dists}
\end{table}

# Literature review
To conduct our review on current methods available in survival analysis, a scraper was coded to examine all packages in the survival task view on CRAN [-@survTaskView]. There was a total of 259 packages, many of which are not relevant to the problems casebase addresses. To filter these packages, a scraper was used to search for competing risks, cumulative incidence functions, non- proportional hazards and penalization. This reduced our set of 259 packages down to 60. From here, we manually examined each package to determine which ones are comparable to casebase, with either a different approach to the same problems or future features we would like to add to casebase. 

These packages can belong to three main categories: non-proportional hazards, regularization and competing risks. Besides these three main categories, there are extra features of interest as well. Table XXX summarizes the findings. 

Many packages exist that implement semi-parametric cox models. Of those, glmnet [-@regpathcox]???, glmpath [-@park_hastie], penalized [-@l1penal], RiskRegression [-@gerds_blanche] introduce penalization in their model fitting. CoxRidge [-@perperoglou] , rstpm2 [-@clements_liu] and survival [-@survival-package] include penalization, but they implement the extended cox model permitting intuitive non proportional hazard modeling while the previous four do not. In terms of penalization, the benefit of casebase comes from its flexibility. Linear models have a wide tool set, considerably larger than those made for cox regression (*reference for number of packages between their task views, regression is spread out across Multivariate, SocialSciences, Robust among others. The focus of this task view is on Optimization Infrastructure Packages , General Purpose Continuous Solvers , Mathematical Programming Solvers , and Specific Applications in Optimization , or Multi Objective Optimization*). Casebase permits many regularization tools developed for linear regression to be repurposed within a survival context. 

Parametric models permit non-proportional hazards and are implemented in a handful of packages. Of interest are CFC [-@sharabiani_mahani_2019], flexsurv [-@flexsurv] , SmoothHazard [-@smoothHazard], Rsptm2 [-@clements_liu] and Survival [-@survival-package]. Smooth hazard  is limited to weibull distributions[-@smoothHazard], where as flexsurv and survival contain implementations for users to supply any distribution they would like [-@survival-package], [-@flexsurv]. Flexsurv, smooth hazard, mets and rstpm2 can use splines, permitting a flexible fit over time [-@flexsurv], [-@smoothHazard], [-@scheike2014estimating], [-@clements_liu] . Though casebase is limited to 3 distributions at the moment, it also uses splines allowing for distributions that may not be consistent over survival time. 

Of the methods mentioned so far, only CFC,flexsurv, mets and survival contain implementations for competing risks [-@sharabiani_mahani_2019], [-@flexsurv],[-@scheike2014estimating] ,[-@survival-package] (*how is it done for parametric models?*). The differentiating factor between these packages and casebase is that casebase handles competing risks through multiple logistic regression. 
Smooth hazard, survival and Rstpm2 can handle interval censoring, where as casebase currently can not. Interval censoring is simple feature to implement and is a potential improvement to add to casebase. (There's also multistate models, which I haven't mentioned in this paper)

Of all the packages listed in the table, CFC , Flexsurv, mets, RiskRegression, rstpm2 and survival have implementations for cumulative incidence [-@sharabiani_mahani_2019], [-@flexsurv], [-@scheike2014estimating], [-@gerds_blanche], [-@clements_liu], [-@survival-package]. Since casebase is parametric, the baseline hazard is known and requires integration to retrieve the cumulative incidence. This is similar to the approaches of CFC and flexsurv [-@sharabiani_mahani_2019], [-@flexsurv], while the other packages use semi-parametric approaches to retrieve the baseline after fitting [-@scheike2014estimating], [-@gerds_blanche]. rstpm2 and survival use both for their respective parametric and semi-parametric model implementations [-@clements_liu], [-@survival-package].

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
\hline
\textbf{Package}        & \textbf{Competing Risks} & \textbf{Non-proportional} & \textbf{Penalization} & \textbf{Splines} & \textbf{Parametric} & \textbf{Semi-parametric} & \textbf{Interval/left Censoring} & \textbf{Absolute Risk}           \\
\hline
\textbf{casebase} \cite{hanley2009fitting}       & x                        & x                         & x                     & x                & x                   &                          &                                  & x                                \\
\textbf{CFC} \cite{sharabiani_mahani_2019}            & x                        & x                         &                       &                  & x                   &                          &                                  & x                                \\
\textbf{coxRidge} \cite{perperoglou}       &                          & x                         & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{crrp} \cite{fu}           & x                        &                           & x                     &                  &                     &                          &                                  &                                  \\
\textbf{fastcox} \cite{yi_zou}        &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{Flexrsurv} \cite{flexrsurv}      &                          & x                         &                       & x                & x                   &                          &                                  & Cumulative hazard?               \\
\textbf{Flexsurv} \cite{flexsurv}       & x                        & x                         &                       & x                & x                   &                          &                                  & Cumulative hazard?               \\
\textbf{glmnet} \cite{regpathcox}         &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{glmpath} \cite{park_hastie}       &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{mets} \cite{scheike2014estimating}           & x                        &                           &                       & x                &                     & x                        &                                  & x                                \\
\textbf{penalized} \cite{goeman_meijer2019}      &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{RiskRegression} \cite{gerds_blanche} &                          &                           & x                     &                  &                     & x                        &                                  & x                                \\
\textbf{Rstpm2} \cite{clements_liu}         &                          & x                         & x                     & x                & x                   & x                        & x                                & Cumm haz                         \\
\textbf{SmoothHazard} \cite{smoothHazard}   &                          & x                         &                       & x                & x                   &                          & x                            &                                      \\
\textbf{Survival} \cite{survival-package}       & x                        & x                         &                       &                  & x                   & x                        & x                                & x                               
\end{tabular}%
}
\caption{Different features of interest in various survival packages.}
\label{tab:my-table}
\end{table}

# Theoretical details

As discussed in Hanley & Miettinen [-@hanley2009fitting], the key idea behind case-base sampling is to discretize the study base into an infinite amount of *person moments*. These person moments are indexed by both an individual in the study and a time point, and therefore each person moment has a covariate profile, an exposure status and an outcome status attached to it. We note that there is only a finite number of person moments associated with the event of interest (what Hanley & Miettinen call the *case series*). The case-base sampling refers to the sampling from the base of a representative finite sample called the *base series*. 

Popular survival analysis methods, like Kaplan-Meier and Cox regression, rely on the notion of risk-set sampling. Case-base sampling can be seen as an alternative.

As shown by Saarela & Arjas [-@saarela2015non] (and further expanded in Saarela [-@saarela2016case]), writing the likelihood arising from this data-generating mechanism using the framework of non-homogenous Poisson processes, we eventually reach an expression where each person-moment's contribution is of the form
$$\frac{\lambda(t)^{dN(t)}}{\rho(t) + \lambda(t)},$$
where $N(t)$ is the counting process associated with the event of interest, $\lambda(t)$ is the corresponding hazard function, and $\rho(t)$ is the hazard function for the Poisson process associated with case-base sampling. This parametric form suggests that we can readily estimate log-hazards of the form $\log(\lambda(t)) = g(t; X)$ using logistic regression, where each observation corresponds to a person moment, the function $g(t; X)$ is linear in a finite number of parameters, and where we treat $\log(\rho(t))$ as an offset. 

In Hanley & Miettinen [-@hanley2009fitting], the authors suggest performing case-base samping *uniformly*, i.e. to sample the base series uniformly from the study base. In terms of Poisson processes, this sampling strategy corresponds to a time-homogeneous Poisson process with intensity equal to $b/B$, where $b$ is the number of sampled observations in the base series, and $B$ is the total population-time for the study base. More complex examples are also available; see for example Saarela & Arjas [-@saarela2015non], where the probabilities of the sampling mechanism  are proportional to the cardiovascular disease event rate given by the Framingham score.

The `casebase` package fits the family of hazard functions of the form

$$ h(x,t) = exp[g(x,t)] $$
where $t$ denotes the numerical value (number of units) of a point in prognostic/prospective time and $x$ is the realization of the vector $X$ of variates based on the patient's profile and intervention (if any). Different functions of $t$ lead to different parametric hazard models. 

The simplest of these models is the one-parameter exponential distribution which is obtained by taking the hazard function to be constant over the range of $t$. 

$$ h(x,t) = exp(\beta_0 + \beta_1 x) $$

The instantaneous failure rate is independent of $t$, so that the conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been on study a.k.a the memoryless property (Kalbfleisch and Prentice, 2002).


The Gompertz hazard model is given by including a linear term for time:

$$ h(x,t)  = exp(\beta_0 + \beta_1 t + \beta_2 x) $$

Use of $log(t)$ yields the Weibull hazard which allows for a power dependence of the hazard on time (Kalbfleisch and Prentice, 2002):

$$ h(x, t)  = exp(\beta_0 + \beta_1 \log(t) + \beta_2 x) $$


# Implementation details

The functions in the casebase package can be divided into two categories: 1) data visualization, in the form of population-time plots; and 2) data analysis.

We explicitely aimed at being compatible with both \code{data.frame}s and \code{data.table}s. This is evident in some of the coding choices we made, and it is also reflected in our testing units.

## Population-time plots

## Data analysis

The data analysis step was separated into three parts: 1) case-base sampling; 2) estimation of the smooth hazard function; and 3) calculation of the risk function. By separating the sampling and estimation functions, we allowed the possibility of users implementing more complex sampling scheme, as described in Saarela [-@saarela2016case].

The sampling scheme selected for \code{sampleCaseBase} was described in Hanley and Miettinen [-@hanley2009fitting]: we first sample along the ``person'' axis, proportional to each individual's total follow-up time, and then we sample a moment uniformly over their follow-up time. This sampling scheme is equivalent to the following picture: imagine representing the total follow-up time of all individuals in the study along a single dimension, where the follow-up time of the next individual would start exactly when the follow-up time of the previous individual ends. Then the base series could be sampled uniformly from this one-dimensional representation of the overall follow-up time. In any case, the output is a dataset of the same class as the input, where each row corresponds to a person-moment. The covariate profile for each such person-moment is retained, and an offset term is added to the dataset. This output could then be used to fit a smooth hazard function, or for visualization of the base series.

The fitting function \code{fitSmoothHazard} starts by looking at the class of the dataset: if it was generated from \code{sampleCaseBase}, it automatically inherited the class \code{cbData}. If the dataset supplied to \code{fitSmoothHazard} does not inherit from \code{cbData}, then the fitting function starts by calling \code{sampleCaseBase} to generate the base series. In other words, the occasional user can bypass \code{sampleCaseBase} altogether and only worry about the fitting function \code{fitSmoothHazard}. 

The fitting function retains the familiar formula interface of \code{glm}. The left-hand side of the formula should be the name of the column corresponding to the event type. The right-hand side can be any combination of the covariates, along with an explicit functional form for the time variable. Note that non-proportional hazard models can be achieved at this stage by adding an interaction term involving time. The offset term does not need to be specified by the user, as it is automatically added to the formula.

Finally, the hazard function is fitted to the data using the function \code{glm}, unless there is more than one type event (i.e. in a competing-risk analysis), in which case it uses the multinomical regression capabilities of the \pkg{VGAM} package. This package was selected for its ability to fit multinomial regression models with an offset.

Once a model-fit object has been returned by \code{fitSmoothHazard}, all the familiar summary and diagnostic functions are available: \code{print}, \code{summary}, \code{predict}, \code{plot}, etc. Our package provides one more functionality: it computes risk functions from the model fit. For the case of a single event, it uses the familiar identity
$$S(t) = \exp\left(-\int_0^t h(u;X) du\right).$$
The integral is computed using either the \code{stats::integrate} function or Monte-Carlo integration. For the case of a competing-event analysis, the event-specific risk is computed using a nested double integral; in this setting, Monte-Carlo integration is faster and more flexible than \code{stats::integrate}. This is due to the fact that the computation involves a double loop over the selected time points; Monte-Carlo integration performs this step by using the vectorized \code{rowSums} and \code{colSums} functions.

To decide between a single-event and a competing-event analysis, we created \code{absoluteRisk} as an \code{S3} generic, with methods for both \code{glm} and \code{CompRisk} objects (the latter inherits from \code{vglm} as well). The method dispatch system of \proglang{R} then takes care of matching the correct input to the correct methodology, without the user's intervention.

# Case study 1--European Randomized Study of Prostate Cancer Screening

To introduce the different features available, we make use of the European Randomized Study of Prostate Cancer Screening data; this dataset is available through the \pkg{casebase} package:

```{r erspc-data, eval = eval_cs1}
data(ERSPC)
ERSPC$ScrArm <- factor(ERSPC$ScrArm, 
                       levels = c(0,1), 
                       labels = c("Control group", "Screening group"))
```

The results of this study were published by [schroder2009screening]. This data was obtained using the approach described in [liu2014recovering].

Population time plots can be extremely informative graphical displays of survival data. They should be the first step in an exploratory data analysis. We facilitate this task in the \pkg{casebase} package using the \code{popTime} function. We first create the necessary dataset for producing the population time plots, and we can produce the plot by using the corresponding \code{plot} method:

```{r plot-erspc-data, eval = eval_cs1}
pt_object <- casebase::popTime(ERSPC, event = "DeadOfPrCa")
plot(pt_object)
```

We can also create exposure stratified plots by specifying the \code{exposure} argument in the \code{popTime} function:

```{r plot-stratified-erspc-data, eval = eval_cs1}
pt_object_strat <- casebase::popTime(ERSPC, 
                                     event = "DeadOfPrCa", 
                                     exposure = "ScrArm")
plot(pt_object_strat)
```


We can also plot them side-by-side using the \code{ncol} argument:

```{r plot-stratified-erspc-data-side, eval = eval_cs1}
plot(pt_object_strat, ncol = 2)
```

First, we fit a Cox model to the data, examine the hazard ratio for the screening group (relative to the control group), and plot the cumulative incidence function (CIF). 

```{r erspc-cox, eval = eval_cs1}
cox_model <- survival::coxph(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm, 
                             data = ERSPC)
summary(cox_model)
```

We can plot the CIF for each group:

```{r erspc-cox-cif, eval = eval_cs1}
new_data <- data.frame(ScrArm = c("Control group", "Screening group"),
                       ignore = 99)

plot(survfit(cox_model, newdata = new_data),
     xlab = "Years since Randomization", 
     ylab = "Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    exp(coef(cox_model)), 
                    exp(confint(cox_model))[1], 
                    exp(confint(cox_model))[2]))
legend("topleft", 
       legend = c("Control group", "Screening group"), 
       col = c("red","blue"),
       lty = c(1, 1), 
       bg = "gray90")
```

Next we fit several models using case-base sampling. The models we fit differ in how we choose to model time. 

The \code{fitSmoothHazard} function provides an estimate of the hazard function $h(x, t)$ is the hazard function, where $t$ denotes the numerical value of a point in prognostic/prospective time and $x$ is the realization of the vector $X$ of variates based on the patient's profile and intervention (if any).

```{r erspc-casebase-exponential, eval = eval_cs1}
casebase_exponential <- casebase::fitSmoothHazard(DeadOfPrCa ~ ScrArm, 
                                                  data = ERSPC, 
                                                  ratio = 100)

summary(casebase_exponential)
exp(coef(casebase_exponential)[2])
exp(confint(casebase_exponential)[2,])
```

The \code{absoluteRisk} function provides an estimate of the cumulative incidence curves for a specific risk profile using the following equation:

$$ CI(x, t) = 1 - exp\left( - \int_0^t h(x, u) \textrm{d}u \right) $$

In the plot below, we overlay the estimated CIF from the casebase exponential model on the Cox model CIF:

```{r erspc-casebase-cif, eval = eval_cs1}
smooth_risk_exp <- casebase::absoluteRisk(object = casebase_exponential, 
                                          time = seq(0,15,0.1), 
                                          newdata = new_data)

plot(survfit(cox_model, newdata = new_data),
     xlab = "Years since Randomization", 
     ylab = "Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    exp(coef(cox_model)), 
                    exp(confint(cox_model))[1], 
                    exp(confint(cox_model))[2]))
lines(smooth_risk_exp[,1], smooth_risk_exp[,2], col = "red", lty = 2)
lines(smooth_risk_exp[,1], smooth_risk_exp[,3], col = "blue", lty = 2)


legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")
```

As we can see, the exponential model is not a good fit. Based on what we observed in the population time plot, where more events are observed later on in time, this poor fit is expected. A constant hazard model would overestimate the cumulative incidence earlier on in time, and underestimate it later on; this is what we see on the cumulative incidence plot. This example demonstrates the benefits of population time plots as an exploratory analysis tool. 

Next we enter time linearly into the model:

```{r erspc-casebase-weibull, eval = eval_cs1}
casebase_time <- fitSmoothHazard(DeadOfPrCa ~ Follow.Up.Time + ScrArm, 
                                 data = ERSPC, 
                                 ratio = 100)

summary(casebase_time)
exp(coef(casebase_time))
exp(confint(casebase_time))
```


```{r erspc-casebase-weibull-cif, eval = eval_cs1}
smooth_risk_time <- casebase::absoluteRisk(object = casebase_time, 
                                          time = seq(0,15,0.1), 
                                          newdata = new_data)

plot(survfit(cox_model, newdata = new_data),
     xlab = "Years since Randomization", 
     ylab = "Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    exp(coef(cox_model)), 
                    exp(confint(cox_model))[1], 
                    exp(confint(cox_model))[2]))
lines(smooth_risk_time[,1], smooth_risk_time[,2], col = "red", lty = 2)
lines(smooth_risk_time[,1], smooth_risk_time[,3], col = "blue", lty = 2)

legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")
```

We see that the Weibull model leads to a better fit.

Next we try to enter a smooth function of time into the model using the \code{splines} package:

```{r erspc-casebase-splines, eval = eval_cs1}
casebase_splines <- fitSmoothHazard(DeadOfPrCa ~ splines::bs(Follow.Up.Time) + ScrArm, 
                                    data = ERSPC, 
                                    ratio = 100)

summary(casebase_splines)
exp(coef(casebase_splines))
exp(confint(casebase_splines))
```


```{r erspc-casebase-splines-cif, eval = eval_cs1}
smooth_risk_splines <- absoluteRisk(object = casebase_splines, 
                                    time = seq(0,15,0.1), 
                                    newdata = new_data)

plot(survfit(cox_model, newdata = new_data),
     xlab = "Years since Randomization", 
     ylab = "Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    exp(coef(cox_model)), 
                    exp(confint(cox_model))[1], 
                    exp(confint(cox_model))[2]))
lines(smooth_risk_splines[,1], smooth_risk_splines[,2], col = "red", lty = 2)
lines(smooth_risk_splines[,1], smooth_risk_splines[,3], col = "blue", lty = 2)

legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")
```

It looks like the best fit. 

Since we are within the GLM framework, we can easily test for which model better fits the data using a Likelihood Ratio Test (LRT). The null hypothesis here is that the linear model is just as good as the larger (in terms of number of parameters) splines model.

```{r erscp-compare, eval = eval_cs1}
anova(casebase_time, casebase_splines, test = "LRT")
```

As espected, we see that splines model provides a better fit.

# Case study 2--Bone-marrow transplant

The next example shows how case-base sampling can also be used in the context of a competing risk analysis. For illustrative purposes, we will use the same data that was used in Scrucca *et al* [-@scrucca2010regression]. The data was downloaded from the main author's website, and it is also available as part of the \pkg{casebase} package.

```{r bmtcrr-data, eval = eval_cs2, message = FALSE, echo=2}
library(casebase)
data(bmtcrr)
```

The data contains information on 177 patients who received a stem-cell transplant for acute leukemia. The event of interest is relapse, but other competing causes (e.g. transplant-related death) were also recorded Several covariates were also captured at baseline: sex, disease type (acute lymphoblastic or myeloblastic leukemia, abbreviated as ALL and AML, respectively), disease phase at transplant (Relapse, CR1, CR2, CR3), source of stem cells (bone marrow and peripheral blood, coded as BM+PB, or only peripheral blood, coded as PB), and age. A summary of these baseline characteristics appear in Table \ref{tab:table1bmtcrr}. We note that the statistical summaries were generated differently for different variable types: for continuous variables, we gave the range, followed by the mean and standard deviation; for categorical variables, we gave the counts for each category.

```{r echo=FALSE, eval = FALSE}
table1 <- tibble::tribble(
    ~Variable, ~Description, ~`Statistical summary`,
    "Sex", "Sex", "M=Male (100)",
    "", "", "F=Female (77)",
    "D", "Disease", "ALL (73)",
    "", "", "AML (104)",
    "Phase", "Phase", "CR1 (47)", 
    "", "", "CR2 (45)", 
    "", "", "CR3 (12)", 
    "", "", "Relapse (73)",
    "Source", "Type of transplant", "BM+PB (21)", 
    "", "", "PB (156)",
    "Age", "Age of patient (years)", "4–62", 
    "", "", "30.47 (13.04)",
    "Ftime", "Failure time (months)", "0.13–131.77",
    "", "", "20.28 (30.78)",
    "Status", "Status indicator", "0=censored (46)", 
    "", "", "1=relapse (56)", 
    "", "", "2=competing event (75)"
)

xtable::xtable(table1) %>%
    print(include.rownames = FALSE)
```

<!-- latex table generated in R 3.4.1 by xtable 1.8-2 package -->
<!-- Sun Sep 17 16:03:22 2017 -->
\begin{table}[ht]
\centering
\begin{tabular}{ccc}
  \hline
Variable & Description & Statistical summary \\ 
  \hline
Sex & Sex & M=Male (100) \\ 
   &  & F=Female (77) \\ 
  D & Disease & ALL (73) \\ 
   &  & AML (104) \\ 
  Phase & Phase & CR1 (47) \\ 
   &  & CR2 (45) \\ 
   &  & CR3 (12) \\ 
   &  & Relapse (73) \\ 
  Source & Type of transplant & BM+PB (21) \\ 
   &  & PB (156) \\ 
  Age & Age of patient (years) & 4–62 \\ 
   &  & 30.47 (13.04) \\ 
  Ftime & Failure time (months) & 0.13–131.77 \\ 
   &  & 20.28 (30.78) \\ 
  Status & Status indicator & 0=censored (46) \\ 
   &  & 1=relapse (56) \\ 
   &  & 2=competing event (75) \\ 
   \hline
\end{tabular}
\caption{Baseline characteristics of patients in the stem-cell transplant study.}
\label{tab:table1bmtcrr}
\end{table}

In order to try and visualize the incidence density of relapse, we can look at the corresponding population-time plot. In Figure \ref{fig:compPop1}, failure times associated with relapse are highlighted on the plot using red points, while Figure \ref{fig:compPop2} provides a similar population-time plot for competing events.

```{r compPop1, fig.cap="\\label{fig:compPop1}Population-time plot for the stem-cell transplant study. The points represent the event of interest (i.e., relapse).", echo=FALSE, eval = eval_cs2}
pt_object <- casebase::popTime(bmtcrr, event = "Status", time = "ftime")
plot(pt_object)
```

```{r compPop2, fig.cap="\\label{fig:compPop2}Population-time plot for the stem-cell transplant study. The points represent the competing events.", echo=FALSE, eval = eval_cs2}
pt_object_comp <- bmtcrr %>% 
    mutate(Status = case_when(Status == 1 ~ 2L, 
                              Status == 2 ~ 1L, 
                              TRUE ~ Status)) %>% 
    casebase::popTime(event = "Status", time = "ftime")
plot(pt_object_comp, point.colour = "blue")
```

Our main objective is to compute the absolute risk of relapse for a given set of covariates. First, we fit a smooth hazard to the data; for the sake of this example, we opted for a linear term for time:

```{r bmtcrr-casebase-weibull, warning = FALSE, eval = eval_cs2}
model_cb <- fitSmoothHazard(
    Status ~ ftime + Sex + D + Phase + Source + Age, 
    data = bmtcrr, 
    ratio = 100, 
    time = "ftime")
```

```{r bmtcrr-cox, echo = FALSE, eval = eval_cs2}
library(survival)
# Treat competing event as censoring
model_cox <- coxph(Surv(ftime, Status == 1) ~ Sex + D + Phase + Source + Age,
                   data = bmtcrr)
```

From the fit object, we can extract both the hazard ratios and their corresponding confidence intervals:

```{r, echo=FALSE, eval = eval_cs2}
# Table of coefficients----
z_value <- qnorm(0.975)
foo <- summary(model_cb)@coef3
table_cb <- foo[!grepl("Intercept", rownames(foo)) & 
                    !grepl("ftime", rownames(foo)) &
                    grepl(":1", rownames(foo)), 
                1:2]
table_cb <- cbind(table_cb[,1], 
                  table_cb[,1] - z_value * table_cb[,2],
                  table_cb[,1] + z_value * table_cb[,2])
table_cb <- round(exp(table_cb), 2)

table_cox <- round(summary(model_cox)$conf.int[,-2], 2)
rownames(table_cb) <- rownames(table_cox)
colnames(table_cb) <- colnames(table_cox)

table_cb %<>% as.data.frame %>% 
    mutate(Covariates = rownames(.)) %>% 
    mutate(`95% CI` = glue::glue_data(., "({`lower .95`}, {`upper .95`})")) %>% 
    rename(HR = `exp(coef)`) %>% 
    select(Covariates, HR, `95% CI`)

# table_cox %<>% as.data.frame %>% 
#     mutate(Covariates = rownames(.)) %>% 
#     mutate(`CI (Cox)` = glue::glue_data(., "({`lower .95`}, {`upper .95`})")) %>% 
#     rename(`HR (Cox)` = `exp(coef)`) %>% 
#     select(Covariates, `HR (Cox)`, `CI (Cox)`)

table_cb %>% 
# inner_join(table_cb, table_cox, by = "Covariates") %>%
    mutate(Covariates = case_when(
        Covariates == "SexM" ~ "Sex",
        Covariates == "DAML" ~ "Disease",
        Covariates == "PhaseCR2" ~ "Phase (CR2 vs. CR1)",
        Covariates == "PhaseCR3" ~ "Phase (CR3 vs. CR1)",
        Covariates == "PhaseRelapse" ~ "Phase (Relapse vs. CR1)",
        Covariates == "SourcePB" ~ "Source",
        TRUE ~ Covariates
    )) %>% 
    knitr::kable(format = "latex")
```

As we can see, the only significant hazard ratio is the one associated with the phase of the disease at transplant. More precisely, being in relapse at transplant is associated with a hazard ratio of 3.92 when compared to CR1.

Given our estimate of the hazard function, we can compute the absolute risk curve for a fixed covariate profile. We performed this computation for a 35 year old woman who received a stem-cell transplant from peripheral blood at relapse. We compared the absolute risk curve for such a woman with acute lymphoblastic leukemia with that for a similar woman with acute myeloblastic leukemia. Figure \ref{fig:compAbsrisk} shows these two curves as a function of time. This figure also shows the Kaplan-Meier estimate fitted to the two disease groups (ignoring the other covariates).

```{r cb_risk, warning = FALSE, cache = FALSE, eval = eval_cs2}
# Pick 100 equidistant points between 0 and 60 months
time_points <- seq(0, 60, length.out = 50)

# Data.frame containing risk profile
newdata <- data.frame("Sex" = factor(c("F", "F"), 
                                     levels = levels(bmtcrr[,"Sex"])),
                      "D" = c("ALL", "AML"),
                      "Phase" = factor(c("Relapse", "Relapse"), 
                                       levels = levels(bmtcrr[,"Phase"])),
                      "Age" = c(35, 35),
                      "Source" = factor(c("PB", "PB"), 
                                        levels = levels(bmtcrr[,"Source"])))

# Estimate absolute risk curve
risk_cb <- absoluteRisk(object = model_cb, time = time_points,
                        method = "numerical", newdata = newdata)
```

```{r fg_risk, eval = FALSE, echo = FALSE}
library(timereg)
model_fg <- comp.risk(Event(ftime, Status) ~ const(Sex) + const(D) + 
                          const(Phase) + const(Source) + const(Age), 
                      data = bmtcrr, cause = 1, model = "fg")

# Estimate absolute risk curve
risk_fg <- predict(model_fg, newdata, times = time_points)
```

```{r echo = FALSE, fig.cap="\\label{fig:compAbsrisk}Absolute risk curve for a fixed covariate profile and the two disease groups. The estimate obtained from case-base sampling is compared to the Kaplan-Meier estimate.", eval = eval_cs2}
risk_cb <- bind_rows(
    data.frame(Time = time_points, 
               Method = "Case-base", 
               Risk = risk_cb[,2], 
               Disease = "ALL",
               stringsAsFactors = FALSE),
    data.frame(Time = time_points, 
               Method = "Case-base", 
               Risk = risk_cb[,3], 
               Disease = "AML",
               stringsAsFactors = FALSE)
    )

# risk_fg <- bind_rows(
#     data.frame(Time = time_points, 
#                Method = "Fine-Gray", 
#                Risk = risk_fg$P1[1,], 
#                Disease = "ALL",
#                stringsAsFactors = FALSE),
#     data.frame(Time = time_points, 
#                Method = "Fine-Gray", 
#                Risk = risk_fg$P1[2,], 
#                Disease = "AML",
#                stringsAsFactors = FALSE)
#     )

risk_km <- purrr::map_df(c("ALL", "AML"), 
                         function(disease) {
    tmp <- bmtcrr %>% 
        filter(D == disease) %>% 
        survival::survfit(survival::Surv(ftime,Status == 1) ~ 1, 
                data = .)
    data.frame(Time = tmp$time, 
               Method = "Kaplan-Meier", 
               Risk = 1 - tmp$surv, 
               Disease = disease,
               stringsAsFactors = FALSE) %>%
        filter(Time <= 60)
})

risk_all <- bind_rows(
    risk_cb,
    risk_km
    ) %>% arrange(Disease, Risk)

ggplot(risk_all, aes(x = Time, y = Risk, colour = Method)) + 
    # geom_line for smooth curve
    geom_line(data = filter(risk_all, Method == "Case-base")) + 
    # geom_step for step function
    geom_step(data = filter(risk_all, Method != "Case-base")) + 
    facet_grid(Disease ~ .) +
    ylim(c(0,1)) +
    theme(legend.position = "top") +
    xlab("Time (in Months)") + ylab("Relapse risk")
```

# Case study 3--Study to Understand Prognoses Preferences Outcomes and Risks of Treatment

We examined the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset (REF from main site). the SUPPORT dataset tracks death for individuals who are considered seriously ill within a hospital.  Imputation was conducted using the default methods in the mice package in R, available on CRAN. Before imputation, there was a total of 1000 individuals and 35 variables. After imputation, we have 474 observations where 67.5% of individuals in the study experience the event of interest. The SUPPORT dataset will be used to demonstrate regularization within casebase (REF), while comparing their absolute risk preditions for a new individual. A description of each variable can be found in table XXX1 and a breakdown of each categorical variable in table XXX2. The data was downloaded from the main author's website, and it is also available as part of the casebase package.

\begin{table}[]
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Name} & \textbf{Labels}                          & \textbf{Levels} & \textbf{Storage} & \textbf{NAs} \\
\hline
age           & Age                                      &                 & double           & 0            \\
death         & Death at any time up to NDI date:31DEC94 &                 & double           & 0            \\
sex           &                                          & 2               & integer          & 0            \\
hospdead      & Death in Hospital                        &                 & double           & 0            \\
slos          & Days from Study Entry to Discharge       &                 & double           & 0            \\
d.time        & Days of Follow-Up                        &                 & double           & 0            \\
dzgroup       &                                          & 8               & integer          & 0            \\
dzclass       &                                          & 4               & integer          & 0            \\
num.co        & number of comorbidities                  &                 & double           & 0            \\
edu           & Years of Education                       &                 & double           & 202          \\
income        &                                          & 4               & integer          & 349          \\
scoma         & SUPPORT Coma Score based on Glasgow D3   &                 & double           & 0            \\
charges       & Hospital Charges                         &                 & double           & 25           \\
totcst        & Total RCC cost                           &                 & double           & 105          \\
totmcst       & Total micro-cost                         &                 & double           & 372          \\
avtisst       & Average TISS, Days 3-25                  &                 & double           & 6            \\
race          &                                          & 5               & integer          & 5            \\
meanbp        & Mean Arterial Blood Pressure Day 3       &                 & double           & 0            \\
wblc          & White Blood Cell Count Day 3             &                 & double           & 24           \\
hrt           & Heart Rate Day 3                         &                 & double           & 0            \\
resp          & Respiration Rate Day 3                   &                 & double           & 0            \\
temp          & Temperature (celcius) Day 3              &                 & double           & 0            \\
pafi          & PaO2/(.01*FiO2) Day 3                    &                 & double           & 253          \\
alb           & Serum Albumin Day 3                      &                 & double           & 378          \\
bili          & Bilirubin Day 3                          &                 & double           & 297          \\
crea          & Serum creatinine Day 3                   &                 & double           & 3            \\
sod           & Serum sodium Day 3                       &                 & double           & 0            \\
ph            & Serum pH (arterial) Day 3                &                 & double           & 250          \\
glucose       & Glucose Day 3                            &                 & double           & 470          \\
bun           & BUN Day 3                                &                 & double           & 455          \\
urine         & Urine Output Day 3                       &                 & double           & 517          \\
adlp          & ADL Patient Day 3                        &                 & double           & 634          \\
adls          & ADL Surrogate Day 3                      &                 & double           & 310          \\
sfdm2         &                                          & 5               & integer          & 159          \\
adlsc         & Imputed ADL Calibrated to Surrogate      &                 & double           & 0           
\end{tabular}
\caption{A description of each variable in the SUPPORT dataset, taken from (REF).}
\label{tab:my-table}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{cc}
\hline
\textbf{Variable} & \textbf{Levels}                      \\
\hline
sex      & female                                        \\
         & male                                          \\
dzgroup  & ARF/MOSF w/Sepsis                             \\
         & COPD                                          \\
         & CHF                                           \\
         & Cirrhosis                                     \\
         & Coma                                          \\
         & Colon Cancer                                  \\
         & Lung Cancer                                   \\
         & MOSF w/Malig                                  \\
dzclass  & ARF/MOSF                                      \\
         & COPD/CHF/Cirrhosis                            \\
         & Coma                                          \\
         & Cancer                                        \\
income   & under \$11k                                   \\
         & $11-$25k                                      \\
         & $25-$50k                                      \\
         & \textgreater{}\$50k                           \\
race     & white                                         \\
         & black                                         \\
         & asian                                         \\
         & other                                         \\
         & hispanic                                      \\
sfdm2    & no(M2 and SIP pres)                           \\
         & adl\textgreater{}=4 (\textgreater{}=5 if sur) \\
         & SIP\textgreater{}=30                          \\
         & Coma or Intub                                 \\
         & \textless{}2 mo. follow-up                   
\end{tabular}
\caption{A description of each level within each categorical variable in the dataset, taken from REF.}
\label{tab:my-table}
\end{table}

Hospital death as a covariate was removed, as this is directly informative of death. The glmnet package(REF) on CRAN was used to introduce elastic net, lasso and ridge regressions to casebase. To visualize the effect of imputation on incidence density, we can refer to the pre-imputation population-time plot in Figure \ref{fig:ptPreI} and post-imputation population time plot in Figure \ref{fig:ptPostI}. The incidence density is consistent between imputed and complete datasets.

```{r supportSetUp, echo = FALSE, eval = eval_cs3}
completeData=casebase::support
#keep one individual out, at random, for which we will develop an 
#absolute risk curve.
sam=sample(1:nrow(completeData), 1)
#Set workingData to the remaining individuals in the data
#workingData=completeData[-c(sam),]
workingCompleteData=as.matrix(sparse.model.matrix(death~ .-d.time,data=completeData))[,-c(1)]
#Create u and xCox, which will be used when fitting Cox with glmnet.
newData=workingCompleteData[sam,]
x=workingCompleteData[-c(sam),]
#x and y will be used to fit the casebase model under glmnet.
#x=as.matrix(sparse.model.matrix(death~ .-d.time+0,data=workingData))
y=data.matrix(completeData[-c(sam),c(34,4)])
```

```{r poptimePreImputation, echo = TRUE, eval = eval_cs3, fig.cap="\\label{fig:ptPreI} Population-time plot before imputation. Red dots demonstrate death, while each gray dot makes up all the person moments spent in the study."}
#demonstrating incidence density before imputation
plot(casebase::popTime(as.data.frame(support),time = "d.time",event = "death"))
```

```{r poptimePostImputation, echo = TRUE, eval = eval_cs3, fig.cap="\\label{fig:ptPostI} Population-time plot before imputation. Red dots demonstrate death, while each gray dot makes up all the person moments spent in the study."}
#demonstrating incidence density after imputation
plot(casebase::popTime(as.data.frame(workingCompleteData),time = "d.time",event = "death"))
```
Our main objective is to compute the absolute risk of death for a given set of covariates, using regularization when fitting our model. First, we fit a smooth hazard to the data; for the sake of this example, we opted for a linear term for time. As casebase makes use of the glmnet package, we interact with the fitsmoothhazard.fit function using a matrix interface, where y contains the time and event variables and x contains all other variables we would like to include in the model. For the SUPPORT dataset, all factor variables were converted into dummy variables. Here, we used lasso penalization by setting alpha to 1.
```{r CBhazard, echo = TRUE, eval = eval_cs3}
#Using casebase's glmnet implementation, using splines and a case to base ratio of 100,
#to determine the hazard function
cb.Model=casebase::fitSmoothHazard.fit(x,y,family="glmnet",time="d.time",event="death",
                             formula_time = ~d.time,alpha=1,ratio=100)
```
Then, we take our model and use the absoluteRisk function to integrate and retrieve our absolute risk function.
```{r CBabsolute, echo = TRUE, eval = eval_cs3}
#Estimating the absolute risk curve using the newData parameter.
casebaseAbsolute=casebase::absoluteRisk(cb.Model,time = seq(1,max(completeData$d.time), 0.5),newdata = t(newData) ,
                              s="lambda.1se",method=c("numerical"))
```
We will compare this curve to a regularized version of cox regression, and a kaplan-meier survival curve. The regularized cox regression absolute risk curve required two extra steps to retrive. Coxnet from the glmnet package has tools to fit a regularized cox model, but requires that a survival object skeleton of the selected variables, so that the survival package tools can be used to retrive the absolute risk. We handle this by fitting a regularized cox model with the glmnet package, and a cox model from the survival package with the selected variables from the regularized fit. This second model serves as a skeleton that permits the use of survival package functions. The coefficients from the regularized model will replace the coeficients in our skeleton. the resulting model will have the correct coefficients when integrating, but will have an incorrect standard error. For the purposes of this case study, we are only interested in the absolute risk curve itself and not the standard error.

```{r coxHazAbsolute, echo = TRUE, eval = eval_cs3}
#Create u and xCox, which will be used when fitting Cox with glmnet.
u=survival::Surv(time = as.numeric(y[,2]), event = as.factor(y[,1]))
xCox=as.matrix(sparse.model.matrix(death~ .-d.time,data=completeData))[-c(sam),]
#hazard for cox using glmnet
coxglmFit=glmnet::cv.glmnet(x=xCox,y=u, family="cox",alpha=1)
#convergence demonstrated in plot
plot(coxglmFit)
#taking the coefficient estimates for later use
nonzero_covariate_cox <- predict(coxglmFit, type = "nonzero", s = "lambda.1se")
nonzero_coef_cox <- coef(coxglmFit, s = "lambda.1se")
#creating a new dataset that only contains the covariates chosen through glmnet.
#cleanCoxData<- as.data.frame(cbind(as.numeric(workingData$d.time),as.factor(workingData$death), xCox[,nonzero_covariate_cox$X1]))
cleanCoxData<-as.data.frame(cbind(as.numeric(y[,2]),as.numeric(y[,1]),xCox[,nonzero_covariate_cox$X1]))
#newDataCox<-xCox[sam,nonzero_covariate_cox$X1]
#fitting a cox model using regular estimation, however we will not keep it.
#this is used more as an object place holder.
coxFit <- survival::coxph(Surv(time=V1,event=V2) ~ ., data = cleanCoxData)
#The coefficients of this object will be replaced with the estimates from coxglmFit.
#Doing so makes it so that everything is invalid aside from the coefficients.
#In this case, all we need to estimate the absolute risk is the coefficients.
#Std. error would be incorrect here, if we were to draw error bars.
coxFit$coefficients<-nonzero_coef_cox@x
#Fitting absolute risk curve for cox+glmnet
newDataCox=newData[nonzero_covariate_cox$X1-1]

abCoxFit<-survival::survfit(coxFit,newdata=as.data.frame(t(newDataCox)),time = seq(0,max(completeData$d.time), 1),type="breslow")
```
We used the survival package to calculate the kaplan-meier absolute risk function. 
```{r fitKM+abRisk, echo = TRUE, eval = eval_cs3}
#creating a surv object to be used to fit an unadjusted absolute risk curve.
# (Kaplan-Meier risk curve)
km <- survival::Surv(time = completeData$d.time, event = completeData$death)
abKm<-survival::survfit(km~1,type='kaplan-meier',conf.type='log')
```

Now that our three absolute risk functions have been calculated, we compare them all in Figure \ref{fig:abSC}. Both coxnet and casebase decrease the absolute risk by the end of the study, in comparison to kaplan-meier.
```{r abSupportComparison, echo = TRUE, eval = eval_cs3, fig.cap="\\label{fig:abSC} Compares regularized casebase in black, regularized cox in red and Kaplan meier in blue."}
# A plot to compare all three Absolute risk (Commulative Incidence)
plot(casebaseAbsolute,type='l',col="black",lwd=2,main="Support- CaseBase vs. Cox+glmnet vs KM Absolute Risk Curves",xlab="Survival-Time",ylab="Cumulative Incidence",ylim=c(0,1),xlim=)
lines(abCoxFit ,col="red",fun="event",lwd=3,conf.int = FALSE)
lines(abKm ,col="Blue",fun="event",lwd=3,conf.int = FALSE)
legend("bottom", 
       legend = c( "Casebase (Lasso+linear)","semi-parametric (Cox)+glmnet","KM curve"), 
       col = c("black","red","Blue"),
       lty = c(1, 1, 1), 
       bg = "gray90")
```
# Discussion

In the following table we provide a comparison between the Cox model and case-base sampling:

```{r, echo=FALSE, eval = FALSE, results='asis'}
knitr::kable(data.frame(`feature` = c("model type", "time", "cumulative incidence", "non-proportional hazards","model testing","competing risks", "prediction"),
           `Cox` = c("semi-parametric","left hand side of the equation", "step function", "interaction of covariates with time"," ", "difficult", "Kaplan-Meier-based"),
           `Case Base Sampling` = c("fully parametric (logistic/multinomial regression)", "right hand side - allows flexible modeling of time",
           "smooth-in-time curve", "interaction of covariates with time", "make use of GLM framework (LRT, AIC, BIC)",
           "cause-specific cumulative incidence functions (CIFs) directly obtained via multinomial regression", "ROC, AUC, risk reclassification probabilities")), escape = FALSE)
```


# Environment Details

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:
```{r}
# what commit is this file at? 
git2r::repository(here::here())
```
