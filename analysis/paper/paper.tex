\documentclass[
]{jss}

\usepackage[utf8]{inputenc}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
Sahir Bhatnagar *\\McGill University \And Maxime Turgeon *\\University of Manitoba \AND Jesse Islam\\McGill University \And James Hanley\\McGill University \And Olli Saarela\\University of Toronto
}
\title{\pkg{casebase}: An Alternative Framework For Survival Analysis}

\Plainauthor{Sahir Bhatnagar *, Maxime Turgeon *, Jesse Islam, James Hanley, Olli Saarela}
\Plaintitle{casebase: An Alternative Framework For Survival Analysis}
\Shorttitle{\pkg{casebase}: An Alternative Framework For Survival Analysis}

\Abstract{
The abstract of the article. * joint co-authors
}

\Keywords{keywords, not capitalized, \proglang{Java}}
\Plainkeywords{keywords, not capitalized, Java}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Sahir Bhatnagar *\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{sahir.bhatnagar@mail.mcgill.ca}\\
  URL: \url{http://sahirbhatnagar.com/}\\~\\
      Maxime Turgeon *\\
  University of Manitoba\\
  186 Dysart Road Winnipeg, MB, Canada R3T 2N2\\
  E-mail: \email{max.turgeon@umanitoba.ca}\\
  URL: \url{https://maxturgeon.ca/}\\~\\
      Jesse Islam\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{jesse.islam@mail.mcgill.ca}\\
  
      James Hanley\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{james.hanley@mcgill.ca}\\
  URL: \url{http://www.medicine.mcgill.ca/epidemiology/hanley/}\\~\\
      Olli Saarela\\
  University of Toronto\\
  Dalla Lana School of Public Health, 155 College Street, 6th floor,
  Toronto, Ontario M5T 3M7, Canada\\
  E-mail: \email{olli.saarela@utoronto.ca}\\
  URL: \url{http://individual.utoronto.ca/osaarela/}\\~\\
  }

% Pandoc header

\usepackage{amsmath} \usepackage{longtable} \usepackage{graphicx}

\begin{document}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The purpose of the \pkg{casebase} package is to provide practitioners
with an easy-to-use software tool to predict the risk (or cumulative
incidence (CI)) of an event, for a particular patient. The following
points should be noted:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  hazard ratios
\item
  If, however, the absolute risks are of interest, they have to be
  recovered using the semi-parametric Breslow estimator
\item
  Alternative approaches for fitting flexible hazard models for
  estimating absolute risks, not requiring this two-step approach? Yes!
  \citep{hanley2009fitting}
\end{enumerate}

\citep{hanley2009fitting} propose a fully parametric hazard model that
can be fit via logistic regression. From the fitted hazard function,
cumulative incidence and, thus, risk functions of time, treatment and
profile can be easily derived.

\begin{table}
  \begin{tabular}{p{1.8in}lll}
\hline
    &  Parameters &  Density \proglang{R} function & \code{dist}\\
    & {\footnotesize{(location in \code{\color{red}{red}})}} & & \\
\hline
    Exponential & \code{\color{red}{rate}}             & \code{dexp}   & \code{"exp"} \\
    Weibull (accelerated failure time)     & \code{shape, {\color{red}{scale}}}     & \code{dweibull} & \code{"weibull"} \\
    Weibull (proportional hazards)     & \code{shape, {\color{red}{scale}}}     & \code{dweibullPH} & \code{"weibullPH"} \\
    Gamma       & \code{shape, \color{red}{rate}}      & \code{dgamma} & \code{"gamma"}\\
    Log-normal  & \code{{\color{red}{meanlog}}, sdlog}   & \code{dlnorm} & \code{"lnorm"}\\
    Gompertz    & \code{shape, {\color{red}{rate}}}      & \code{dgompertz} & \code{"gompertz"} \\
    Log-logistic & \code{shape, {\color{red}{scale}}}   & \code{dllogis} & \code{"llogis"}\\
    Generalized gamma (Prentice 1975)   & \code{{\color{red}{mu}}, sigma, Q} & \code{dgengamma} & \code{"gengamma"} \\
    Generalized gamma (Stacy 1962)& \code{shape, {\color{red}{scale}}, k} & \code{dgengamma.orig} & \code{"gengamma.orig"} \\
    Generalized F     (stable)    & \code{{\color{red}{mu}}, sigma, Q, P} & \code{dgenf} & \code{"genf"} \\
    Generalized F     (original)  & \code{{\color{red}{mu}}, sigma, s1, s2} & \code{dgenf.orig} & \code{"genf.orig"} \\
\hline
  \end{tabular}
  \caption{Built-in parametric survival distributions in \pkg{flexsurv}.}
  \label{tab:dists}
\end{table}

\hypertarget{literature-review}{%
\section{Literature review}\label{literature-review}}

To conduct our review on current methods available in survival analysis,
a scraper was coded to examine all packages in the survival task view on
CRAN \citeyearpar{survTaskView}. There was a total of 259 packages, many
of which are not relevant to the problems casebase addresses. To filter
these packages, a scraper was used to search for competing risks,
cumulative incidence functions, non- proportional hazards and
penalization. This reduced our set of 259 packages down to 60. From
here, we manually examined each package to determine which ones are
comparable to casebase, with either a different approach to the same
problems or future features we would like to add to casebase.

These packages can belong to three main categories: non-proportional
hazards, regularization and competing risks. Besides these three main
categories, there are extra features of interest as well. Table
\ref{tab:surv-pkgs} summarizes the findings.

Many packages exist that implement semi-parametric Cox models. Of those,
\pkg{glmnet} \citeyearpar{regpathcox}???, \pkg{glmpath}
\citeyearpar{park_hastie}, \pkg{penalized} \citeyearpar{l1penal},
\pkg{RiskRegression} \citeyearpar{gerds_blanche} introduce penalization
in their model fitting. \pkg{CoxRidge} \citeyearpar{perperoglou},
\pkg{rstpm2} \citeyearpar{clements_liu} and \pkg{survival}
\citeyearpar{survival-package} include penalization, but they implement
the extended cox model permitting intuitive non proportional hazard
modeling while the previous four do not. In terms of penalization, the
benefits of \pkg{casebase} comes from its flexibility. Linear models
have a wide tool set, considerably larger than those made for Cox
regression. \pkg{casebase} permits many regularization tools developed
for linear regression to be repurposed within a survival context.

Parametric models permit non-proportional hazards and are implemented in
a handful of packages. Of interest are \pkg{CFC}
\citeyearpar{sharabiani_mahani_2019}, \pkg{flexsurv}
\citeyearpar{flexsurv}, \pkg{SmoothHazard} \citeyearpar{smoothHazard},
\pkg{rsptm2} \citeyearpar{clements_liu} and \pkg{survival}
\citeyearpar{survival-package}. \pkg{SmoothHazard} is limited to Weibull
distributions \citeyearpar{smoothHazard}, whereas \pkg{flexsurv} and
\pkg{survival} contain implementations for users to supply any
distribution they would like \citeyearpar{survival-package},
\citeyearpar{flexsurv}. \pkg{flexsurv}, \pkg{smoothhazard}, \pkg{mets}
and \pkg{rstpm2} can use splines, permitting a flexible fit over time
\citeyearpar{flexsurv}, \citeyearpar{smoothHazard},
\citeyearpar{scheike2014estimating}, \citeyearpar{clements_liu}. Though
\pkg{casebase} is limited to three distributions at the moment, it also
uses splines allowing for distributions that may not be consistent over
survival time.

Of the methods mentioned so far, only \pkg{CFC}, \pkg{flexsurv},
\pkg{mets} and \pkg{survival} contain implementations for competing
risks \citeyearpar{sharabiani_mahani_2019}, \citeyearpar{flexsurv},
\citeyearpar{scheike2014estimating}, \citeyearpar{survival-package}. The
differentiating factor between these packages and \pkg{casebase} is that
\pkg{casebase} handles competing risks through multiple logistic
regression. \pkg{SmoothHazard}, \pkg{survival} and \pkg{Rstpm2} can
handle interval censoring, whereas \pkg{casebase} currently cannot.
Interval censoring is a simple feature to implement and is a potential
improvement to add to \pkg{casebase}.

Of all the packages listed in Table \ref{tab:surv-pkgs}, \pkg{CFC},
\pkg{Flexsurv}, \pkg{mets}, \pkg{RiskRegression}, \pkg{rstpm2} and
\pkg{survival} have implementations for cumulative incidence
\citeyearpar{sharabiani_mahani_2019}, \citeyearpar{flexsurv},
\citeyearpar{scheike2014estimating}, \citeyearpar{gerds_blanche},
\citeyearpar{clements_liu}, \citeyearpar{survival-package}. Since
\pkg{casebase} is parametric, the baseline hazard is known and requires
integration to retrieve the cumulative incidence. This is similar to the
approaches of \pkg{CFC} and \pkg{flexsurv}
\citeyearpar{sharabiani_mahani_2019}, \citeyearpar{flexsurv}, while the
other packages use semi-parametric approaches to retrieve the baseline
after fitting \citeyearpar{scheike2014estimating},
\citeyearpar{gerds_blanche}. \pkg{rstpm2} and \pkg{survival} use both
for their respective parametric and semi-parametric model
implementations \citeyearpar{clements_liu},
\citeyearpar{survival-package}.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
\hline
\textbf{Package}        & \textbf{Competing Risks} & \textbf{Non-proportional} & \textbf{Penalization} & \textbf{Splines} & \textbf{Parametric} & \textbf{Semi-parametric} & \textbf{Interval/left Censoring} & \textbf{Absolute Risk}           \\
\hline
\textbf{casebase} \cite{hanley2009fitting}       & x                        & x                         & x                     & x                & x                   &                          &                                  & x                                \\
\textbf{CFC} \cite{sharabiani_mahani_2019}            & x                        & x                         &                       &                  & x                   &                          &                                  & x                                \\
\textbf{coxRidge} \cite{perperoglou}       &                          & x                         & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{crrp} \cite{fu}           & x                        &                           & x                     &                  &                     &                          &                                  &                                  \\
\textbf{fastcox} \cite{yi_zou}        &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{Flexrsurv} \cite{flexrsurv}      &                          & x                         &                       & x                & x                   &                          &                                  & Cumulative hazard?               \\
\textbf{Flexsurv} \cite{flexsurv}       & x                        & x                         &                       & x                & x                   &                          &                                  & Cumulative hazard?               \\
\textbf{glmnet} \cite{regpathcox}         &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{glmpath} \cite{park_hastie}       &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{mets} \cite{scheike2014estimating}           & x                        &                           &                       & x                &                     & x                        &                                  & x                                \\
\textbf{penalized} \cite{goeman_meijer2019}      &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{RiskRegression} \cite{gerds_blanche} &                          &                           & x                     &                  &                     & x                        &                                  & x                                \\
\textbf{Rstpm2} \cite{clements_liu}         &                          & x                         & x                     & x                & x                   & x                        & x                                & Cumm haz                         \\
\textbf{SmoothHazard} \cite{smoothHazard}   &                          & x                         &                       & x                & x                   &                          & x                            &                                      \\
\textbf{Survival} \cite{survival-package}       & x                        & x                         &                       &                  & x                   & x                        & x                                & x                               
\end{tabular}%
}
\caption{Different features of interest in various survival packages.}
\label{tab:surv-pkgs}
\end{table}

\hypertarget{theoretical-details}{%
\section{Theoretical details}\label{theoretical-details}}

As discussed in Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the
key idea behind case-base sampling is to discretize the study base into
an infinite amount of \emph{person moments}. These person moments are
indexed by both an individual in the study and a time point, and
therefore each person moment has a covariate profile, an exposure status
and an outcome status attached to it. We note that there is only a
finite number of person moments associated with the event of interest
(what Hanley \& Miettinen call the \emph{case series}). The case-base
sampling refers to the sampling from the base of a representative finite
sample called the \emph{base series}.

As shown by Saarela \& Arjas \citeyearpar{saarela2015non} (and further
expanded in Saarela \citeyearpar{saarela2016case}), writing the
likelihood arising from this data-generating mechanism using the
framework of non-homogeneous Poisson processes, we eventually reach an
expression where each person-moment's contribution is of the form
\[\frac{h(t)^{dN(t)}}{\rho(t) + h(t)},\] where \(N(t)\) is the counting
process associated with the event of interest, \(h(t)\) is the
corresponding hazard function, and \(\rho(t)\) is the hazard function
for the Poisson process associated with case-base sampling. This
parametric form suggests that we can readily estimate log-hazards of the
form \(\log(h(t)) = g(t; X)\) using logistic regression, where each
observation corresponds to a person moment, the function \(g(t; X)\) is
linear in a finite number of parameters, and where we treat
\(-\log(\rho(t))\) as an offset.

In Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the authors
suggest performing case-base sampling \emph{uniformly}, i.e.~to sample
the base series uniformly from the study base. In terms of Poisson
processes, this sampling strategy corresponds essentially to a
time-homogeneous Poisson process with intensity equal to \(b/B\), where
\(b\) is the number of sampled observations in the base series, and
\(B\) is the total population-time for the study base. More complex
examples are also available; see for example Saarela \& Arjas
\citeyearpar{saarela2015non}, where the probabilities of the sampling
mechanism are proportional to the cardiovascular disease event rate
given by the Framingham score.

The \texttt{casebase} package fits the family of hazard functions of the
form

\[ h(t;X) = \exp[g(t;X)] \] where \(t\) denotes time and \(X\), the
individual's covariate profile. Different functions of \(t\) lead to
different parametric hazard models. The simplest of these models is the
one-parameter exponential distribution which is obtained by taking the
hazard function to be constant over the range of \(t\).

\[ h(t;X) = \exp(\beta_0 + \beta_1 X) \]

The instantaneous failure rate is independent of \(t\), so that the
conditional chance of failure in a time interval of specified length is
the same regardless of how long the individual has been in the study.
This is also known as the \emph{memoryless property} (Kalbfleisch and
Prentice, 2002).

The Gompertz hazard model is given by including a linear term for time:

\[ h(t;X)  = \exp(\beta_0 + \beta_1 t + \beta_2 X) \]

Use of \(\log(t)\) yields the Weibull hazard which allows for a power
dependence of the hazard on time (Kalbfleisch and Prentice, 2002):

\[ h(t;X)  = \exp(\beta_0 + \beta_1 \log(t) + \beta_2 X) \]

For competing-risk analyses with \(J\) possible events, we can show that
each person-moment's contribution of the likelihood is of the form

\[\frac{h_j(t)^{dN_j(t)}}{\rho(t) + \sum_{j=1}^Jh_j(t)},\]

where \(N_j(t)\) is the counting process associated with the event of
type \(j\) and \(h_j(t)\) is the corresponding hazard function. As may
be expected, this functional form is similar to the terms appearing in
the likelihood function for multinomial
regression.\footnote{Specifically, it corresponds to the following parametrization: \begin{align*} \log\left(\frac{P(Y=j \mid X)}{P(Y = J \mid X)}\right) = X^T\beta_j, \qquad j = 1,\ldots, J-1\end{align*}}

\hypertarget{implementation-details}{%
\section{Implementation details}\label{implementation-details}}

The functions in the casebase package can be divided into two
categories: 1) data visualization, in the form of population-time plots;
and 2) parametric modeling. We explicitly aimed at being compatible with
both \code{data.frame}s and \code{data.table}s. This is evident in some
of the coding choices we made, and it is also reflected in our unit
tests.

\hypertarget{population-time-plots}{%
\subsection{Population-time plots}\label{population-time-plots}}

\hypertarget{parametric-modeling}{%
\subsection{Parametric modeling}\label{parametric-modeling}}

The parametric modeling step was separated into three parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  case-base sampling;
\item
  estimation of the smooth hazard function;
\item
  calculation of the risk function.
\end{enumerate}

By separating the sampling and estimation functions, we allowed the
possibility of users implementing more complex sampling scheme, as
described in Saarela \citeyearpar{saarela2016case}.

The sampling scheme selected for \code{sampleCaseBase} was described in
Hanley and Miettinen \citeyearpar{hanley2009fitting}: we first sample
along the ``person'' axis, proportional to each individual's total
follow-up time, and then we sample a moment uniformly over their
follow-up time. This sampling scheme is equivalent to the following
picture: imagine representing the total follow-up time of all
individuals in the study along a single dimension, where the follow-up
time of the next individual would start exactly when the follow-up time
of the previous individual ends. Then the base series could be sampled
uniformly from this one-dimensional representation of the overall
follow-up time. In any case, the output is a dataset of the same class
as the input, where each row corresponds to a person-moment. The
covariate profile for each such person-moment is retained, and an offset
term is added to the dataset. This output could then be used to fit a
smooth hazard function, or for visualization of the base series.

The fitting function \code{fitSmoothHazard} starts by looking at the
class of the dataset: if it was generated from \code{sampleCaseBase}, it
automatically inherited the class \code{cbData}. If the dataset supplied
to \code{fitSmoothHazard} does not inherit from \code{cbData}, then the
fitting function starts by calling \code{sampleCaseBase} to generate the
base series. In other words, the occasional user can bypass
\code{sampleCaseBase} altogether and only worry about the fitting
function \code{fitSmoothHazard}.

The fitting function retains the familiar formula interface of
\code{glm}. The left-hand side of the formula should be the name of the
column corresponding to the event type. The right-hand side can be any
combination of the covariates, along with an explicit functional form
for the time variable. Note that non-proportional hazard models can be
achieved at this stage by adding an interaction term involving time. The
offset term does not need to be specified by the user, as it is
automatically added to the formula.

To fit the hazard function, we provide several approaches that are
available via the \code{family} parameter. These approaches are:

\begin{itemize}
\tightlist
\item
  \code{glm}: This is the familiar logistic regression.
\item
  \code{glmnet}: This option allows for variable selection using Lasso
  or elastic-net. This functionality is provided through the
  \code{glmnet} package \citep{friedman2010jss}.
\item
  \code{gam}: This option provides support for \emph{Generalized
  Additive Models} via the \code{gam} package
  \citep{hastie1987generalized}.
\item
  \code{gbm}: This option provides support for \emph{Gradient Boosted
  Trees} via the \code{gbm} package. This feature is still experimental.
\end{itemize}

In the case of multiple events, the hazard is fitted via multinomial
regression as performed by the \pkg{VGAM} package. This package was
selected for its ability to fit multinomial regression models with an
offset.

Once a model-fit object has been returned by \code{fitSmoothHazard}, all
the familiar summary and diagnostic functions are available:
\code{print}, \code{summary}, \code{predict}, \code{plot}, etc. Our
package provides one more functionality: it computes risk functions from
the model fit. For the case of a single event, it uses the familiar
identity \[S(t) = \exp\left(-\int_0^t h(u;X) du\right).\] The integral
is computed using either the \code{stats::integrate} function or
Monte-Carlo integration. The risk function (or cumulative incidence
function) is then defined as

\[ CI(t) = 1 - S(t).\]

For the case of a competing-event analysis, the event-specific risk is
computed using the following procedure: first, we compute the overall
survival function (i.e.~for all event types):

\[ S(t) = \exp\left(-\int_0^t H(u;X) du\right),\qquad H(t;X) = \sum_{j=1}^J h_j(t;X).\]
From this, we can derive the event-specific subdensities:

\[ f_j(t) = h_j(t)S(t).\]

Finally, by integrating these subdensities, we obtain the event-specific
cumulative incidence functions:

\[ CI_j(t) = \int_0^t f_j(u)du.\]

We created \code{absoluteRisk} as an \code{S3} generic, with methods for
the different types of outputs of \code{fitSmoothHazard}. The method
dispatch system of \proglang{R} then takes care of matching the correct
output to the correct methodology for calculating the cumulative
incidence function, without the user's intervention.

In the following sections, we illustrate these functionalities in the
context of three case studies.

\hypertarget{case-study-1european-randomized-study-of-prostate-cancer-screening}{%
\section{Case study 1--European Randomized Study of Prostate Cancer
Screening}\label{case-study-1european-randomized-study-of-prostate-cancer-screening}}

To introduce the different features available, we make use of the
European Randomized Study of Prostate Cancer Screening data; this
dataset is available through the \pkg{casebase} package:

\begin{CodeChunk}

\begin{CodeInput}
R> data(ERSPC)
R> ERSPC$ScrArm <- factor(ERSPC$ScrArm, 
R>                        levels = c(0,1), 
R>                        labels = c("Control group", "Screening group"))
\end{CodeInput}
\end{CodeChunk}

The results of this study were published by {[}schroder2009screening{]}.
This data was obtained using the approach described in
{[}liu2014recovering{]}.

Population time plots can be extremely informative graphical displays of
survival data. They should be the first step in an exploratory data
analysis. We facilitate this task in the \pkg{casebase} package using
the \code{popTime} function. We first create the necessary dataset for
producing the population time plots, and we can produce the plot by
using the corresponding \code{plot} method:

\begin{CodeChunk}

\begin{CodeInput}
R> pt_object <- casebase::popTime(ERSPC, event = "DeadOfPrCa")
R> plot(pt_object)
\end{CodeInput}
\end{CodeChunk}

We can also create exposure stratified plots by specifying the
\code{exposure} argument in the \code{popTime} function:

\begin{CodeChunk}

\begin{CodeInput}
R> pt_object_strat <- casebase::popTime(ERSPC, 
R>                                      event = "DeadOfPrCa", 
R>                                      exposure = "ScrArm")
R> plot(pt_object_strat)
\end{CodeInput}
\end{CodeChunk}

We can also plot them side-by-side using the \code{ncol} argument:

\begin{CodeChunk}

\begin{CodeInput}
R> plot(pt_object_strat, ncol = 2)
\end{CodeInput}
\end{CodeChunk}

First, we fit a Cox model to the data, examine the hazard ratio for the
screening group (relative to the control group), and plot the cumulative
incidence function (CIF).

\begin{CodeChunk}

\begin{CodeInput}
R> cox_model <- survival::coxph(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm, 
R>                              data = ERSPC)
R> summary(cox_model)
\end{CodeInput}
\end{CodeChunk}

We can plot the CIF for each group:

\begin{CodeChunk}

\begin{CodeInput}
R> new_data <- data.frame(ScrArm = c("Control group", "Screening group"),
R>                        ignore = 99)
R> 
R> plot(survfit(cox_model, newdata = new_data),
R>      xlab = "Years since Randomization", 
R>      ylab = "Cumulative Incidence", 
R>      fun = "event",
R>      xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
R>      main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
R>                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
R>                     exp(coef(cox_model)), 
R>                     exp(confint(cox_model))[1], 
R>                     exp(confint(cox_model))[2]))
R> legend("topleft", 
R>        legend = c("Control group", "Screening group"), 
R>        col = c("red","blue"),
R>        lty = c(1, 1), 
R>        bg = "gray90")
\end{CodeInput}
\end{CodeChunk}

Next we fit several models using case-base sampling. The models we fit
differ in how we choose to model time.

The \code{fitSmoothHazard} function provides an estimate of the hazard
function \(h(x, t)\) is the hazard function, where \(t\) denotes the
numerical value of a point in prognostic/prospective time and \(x\) is
the realization of the vector \(X\) of variates based on the patient's
profile and intervention (if any).

\begin{CodeChunk}

\begin{CodeInput}
R> casebase_exponential <- casebase::fitSmoothHazard(DeadOfPrCa ~ ScrArm, 
R>                                                   data = ERSPC, 
R>                                                   ratio = 100)
R> 
R> summary(casebase_exponential)
R> exp(coef(casebase_exponential)[2])
R> exp(confint(casebase_exponential)[2,])
\end{CodeInput}
\end{CodeChunk}

The \code{absoluteRisk} function provides an estimate of the cumulative
incidence curves for a specific risk profile using the following
equation:

\[ CI(x, t) = 1 - \exp\left( - \int_0^t h(x, u) \textrm{d}u \right) \]

In the plot below, we overlay the estimated CIF from the casebase
exponential model on the Cox model CIF:

\begin{CodeChunk}

\begin{CodeInput}
R> smooth_risk_exp <- casebase::absoluteRisk(object = casebase_exponential, 
R>                                           time = seq(0,15,0.1), 
R>                                           newdata = new_data)
R> 
R> plot(survfit(cox_model, newdata = new_data),
R>      xlab = "Years since Randomization", 
R>      ylab = "Cumulative Incidence", 
R>      fun = "event",
R>      xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
R>      main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
R>                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
R>                     exp(coef(cox_model)), 
R>                     exp(confint(cox_model))[1], 
R>                     exp(confint(cox_model))[2]))
R> lines(smooth_risk_exp[,1], smooth_risk_exp[,2], col = "red", lty = 2)
R> lines(smooth_risk_exp[,1], smooth_risk_exp[,3], col = "blue", lty = 2)
R> 
R> 
R> legend("topleft", 
R>        legend = c("Control group (Cox)","Control group (Casebase)",
R>                   "Screening group (Cox)", "Screening group (Casebase)"), 
R>        col = c("red","red", "blue","blue"),
R>        lty = c(1, 2, 1, 2), 
R>        bg = "gray90")
\end{CodeInput}
\end{CodeChunk}

As we can see, the exponential model is not a good fit. Based on what we
observed in the population time plot, where more events are observed
later on in time, this poor fit is expected. A constant hazard model
would overestimate the cumulative incidence earlier on in time, and
underestimate it later on; this is what we see on the cumulative
incidence plot. This example demonstrates the benefits of population
time plots as an exploratory analysis tool.

Next we enter time linearly into the model:

\begin{CodeChunk}

\begin{CodeInput}
R> casebase_time <- fitSmoothHazard(DeadOfPrCa ~ Follow.Up.Time + ScrArm, 
R>                                  data = ERSPC, 
R>                                  ratio = 100)
R> 
R> summary(casebase_time)
R> exp(coef(casebase_time))
R> exp(confint(casebase_time))
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}

\begin{CodeInput}
R> smooth_risk_time <- casebase::absoluteRisk(object = casebase_time, 
R>                                           time = seq(0,15,0.1), 
R>                                           newdata = new_data)
R> 
R> plot(survfit(cox_model, newdata = new_data),
R>      xlab = "Years since Randomization", 
R>      ylab = "Cumulative Incidence", 
R>      fun = "event",
R>      xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
R>      main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
R>                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
R>                     exp(coef(cox_model)), 
R>                     exp(confint(cox_model))[1], 
R>                     exp(confint(cox_model))[2]))
R> lines(smooth_risk_time[,1], smooth_risk_time[,2], col = "red", lty = 2)
R> lines(smooth_risk_time[,1], smooth_risk_time[,3], col = "blue", lty = 2)
R> 
R> legend("topleft", 
R>        legend = c("Control group (Cox)","Control group (Casebase)",
R>                   "Screening group (Cox)", "Screening group (Casebase)"), 
R>        col = c("red","red", "blue","blue"),
R>        lty = c(1, 2, 1, 2), 
R>        bg = "gray90")
\end{CodeInput}
\end{CodeChunk}

We see that the Weibull model leads to a better fit.

Next we try to enter a smooth function of time into the model using the
\code{splines} package:

\begin{CodeChunk}

\begin{CodeInput}
R> casebase_splines <- fitSmoothHazard(DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm, 
R>                                     data = ERSPC, 
R>                                     ratio = 100)
R> 
R> summary(casebase_splines)
R> exp(coef(casebase_splines))
R> exp(confint(casebase_splines))
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}

\begin{CodeInput}
R> smooth_risk_splines <- absoluteRisk(object = casebase_splines, 
R>                                     time = seq(0,15,0.1), 
R>                                     newdata = new_data)
R> 
R> plot(survfit(cox_model, newdata = new_data),
R>      xlab = "Years since Randomization", 
R>      ylab = "Cumulative Incidence", 
R>      fun = "event",
R>      xlim = c(0,15), conf.int = FALSE, col = c("red","blue"), 
R>      main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
R>                     Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
R>                     exp(coef(cox_model)), 
R>                     exp(confint(cox_model))[1], 
R>                     exp(confint(cox_model))[2]))
R> lines(smooth_risk_splines[,1], smooth_risk_splines[,2], col = "red", lty = 2)
R> lines(smooth_risk_splines[,1], smooth_risk_splines[,3], col = "blue", lty = 2)
R> 
R> legend("topleft", 
R>        legend = c("Control group (Cox)","Control group (Casebase)",
R>                   "Screening group (Cox)", "Screening group (Casebase)"), 
R>        col = c("red","red", "blue","blue"),
R>        lty = c(1, 2, 1, 2), 
R>        bg = "gray90")
\end{CodeInput}
\end{CodeChunk}

It looks like the best fit.

Since we are within the GLM framework, we can easily test for which
model better fits the data using a Likelihood Ratio Test (LRT). The null
hypothesis here is that the linear model is just as good as the larger
(in terms of number of parameters) splines model.

\begin{CodeChunk}

\begin{CodeInput}
R> anova(casebase_time, casebase_splines, test = "LRT")
\end{CodeInput}
\end{CodeChunk}

As expected, we see that splines model provides a better fit.

\hypertarget{case-study-2bone-marrow-transplant}{%
\section{Case study 2--Bone-marrow
transplant}\label{case-study-2bone-marrow-transplant}}

The next example shows how case-base sampling can also be used in the
context of a competing risk analysis. For illustrative purposes, we will
use the same data that was used in Scrucca \emph{et al}
\citeyearpar{scrucca2010regression}. The data was downloaded from the
main author's website, and it is also available as part of the
\pkg{casebase} package.

\begin{CodeChunk}

\begin{CodeInput}
R> data(bmtcrr)
\end{CodeInput}
\end{CodeChunk}

The data contains information on 177 patients who received a stem-cell
transplant for acute leukemia. The event of interest is relapse, but
other competing causes (e.g.~transplant-related death) were also
recorded Several covariates were also captured at baseline: sex, disease
type (acute lymphoblastic or myeloblastic leukemia, abbreviated as ALL
and AML, respectively), disease phase at transplant (Relapse, CR1, CR2,
CR3), source of stem cells (bone marrow and peripheral blood, coded as
BM+PB, or only peripheral blood, coded as PB), and age. A summary of
these baseline characteristics appear in Table \ref{tab:table1bmtcrr}.
We note that the statistical summaries were generated differently for
different variable types: for continuous variables, we gave the range,
followed by the mean and standard deviation; for categorical variables,
we gave the counts for each category.

\begin{table}[ht]
\centering
\begin{tabular}{ccc}
  \hline
Variable & Description & Statistical summary \\ 
  \hline
Sex & Sex & M=Male (100) \\ 
   &  & F=Female (77) \\ 
  D & Disease & ALL (73) \\ 
   &  & AML (104) \\ 
  Phase & Phase & CR1 (47) \\ 
   &  & CR2 (45) \\ 
   &  & CR3 (12) \\ 
   &  & Relapse (73) \\ 
  Source & Type of transplant & BM+PB (21) \\ 
   &  & PB (156) \\ 
  Age & Age of patient (years) & 4–62 \\ 
   &  & 30.47 (13.04) \\ 
  Ftime & Failure time (months) & 0.13–131.77 \\ 
   &  & 20.28 (30.78) \\ 
  Status & Status indicator & 0=censored (46) \\ 
   &  & 1=relapse (56) \\ 
   &  & 2=competing event (75) \\ 
   \hline
\end{tabular}
\caption{Baseline characteristics of patients in the stem-cell transplant study.}
\label{tab:table1bmtcrr}
\end{table}

In order to try and visualize the incidence density of relapse, we can
look at the corresponding population-time plot. In Figure
\ref{fig:compPop1}, failure times associated with relapse are
highlighted on the plot using red points, while Figure
\ref{fig:compPop2} provides a similar population-time plot for competing
events.

Our main objective is to compute the absolute risk of relapse for a
given set of covariates. First, we fit a smooth hazard to the data; for
the sake of this example, we opted for a linear term for time:

\begin{CodeChunk}

\begin{CodeInput}
R> model_cb <- fitSmoothHazard(
R>     Status ~ ftime + Sex + D + Phase + Source + Age, 
R>     data = bmtcrr, 
R>     ratio = 100, 
R>     time = "ftime")
\end{CodeInput}
\end{CodeChunk}

From the fit object, we can extract both the hazard ratios and their
corresponding confidence intervals:

As we can see, the only significant hazard ratio is the one associated
with the phase of the disease at transplant. More precisely, being in
relapse at transplant is associated with a hazard ratio of 3.92 when
compared to CR1.

Given our estimate of the hazard function, we can compute the absolute
risk curve for a fixed covariate profile. We performed this computation
for a 35 year old woman who received a stem-cell transplant from
peripheral blood at relapse. We compared the absolute risk curve for
such a woman with acute lymphoblastic leukemia with that for a similar
woman with acute myeloblastic leukemia. Figure \ref{fig:compAbsrisk}
shows these two curves as a function of time. This figure also shows the
Kaplan-Meier estimate fitted to the two disease groups (ignoring the
other covariates).

\begin{CodeChunk}

\begin{CodeInput}
R> # Pick 100 equidistant points between 0 and 60 months
R> time_points <- seq(0, 60, length.out = 50)
R> 
R> # Data.frame containing risk profile
R> newdata <- data.frame("Sex" = factor(c("F", "F"), 
R>                                      levels = levels(bmtcrr[,"Sex"])),
R>                       "D" = c("ALL", "AML"),
R>                       "Phase" = factor(c("Relapse", "Relapse"), 
R>                                        levels = levels(bmtcrr[,"Phase"])),
R>                       "Age" = c(35, 35),
R>                       "Source" = factor(c("PB", "PB"), 
R>                                         levels = levels(bmtcrr[,"Source"])))
R> 
R> # Estimate absolute risk curve
R> risk_cb <- absoluteRisk(object = model_cb, time = time_points,
R>                         method = "numerical", newdata = newdata)
\end{CodeInput}
\end{CodeChunk}

\hypertarget{case-study-3study-to-understand-prognoses-preferences-outcomes-and-risks-of-treatment}{%
\section{Case study 3--Study to Understand Prognoses Preferences
Outcomes and Risks of
Treatment}\label{case-study-3study-to-understand-prognoses-preferences-outcomes-and-risks-of-treatment}}

We examined the Study to Understand Prognoses Preferences Outcomes and
Risks of Treatment (SUPPORT) dataset (REF from main site). the SUPPORT
dataset tracks death for individuals who are considered seriously ill
within a hospital. Imputation was conducted using the default methods in
the mice package in R, available on CRAN. Before imputation, there was a
total of 1000 individuals and 35 variables. After imputation, we have
474 observations where 67.5\% of individuals in the study experience the
event of interest. The SUPPORT dataset will be used to demonstrate
regularization within casebase (REF), while comparing their absolute
risk preditions for a new individual. A description of each variable can
be found in Table\textasciitilde{}\ref{tab:support1} and a breakdown of
each categorical variable in Table\textasciitilde{}\ref{tab:support2}.
The data was downloaded from the main author's website, and it is also
available as part of the casebase package.

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Name} & \textbf{Labels}                          & \textbf{Levels} & \textbf{Storage} & \textbf{NAs} \\
\hline
age           & Age                                      &                 & double           & 0            \\
death         & Death at any time up to NDI date:31DEC94 &                 & double           & 0            \\
sex           &                                          & 2               & integer          & 0            \\
hospdead      & Death in Hospital                        &                 & double           & 0            \\
slos          & Days from Study Entry to Discharge       &                 & double           & 0            \\
d.time        & Days of Follow-Up                        &                 & double           & 0            \\
dzgroup       &                                          & 8               & integer          & 0            \\
dzclass       &                                          & 4               & integer          & 0            \\
num.co        & number of comorbidities                  &                 & double           & 0            \\
edu           & Years of Education                       &                 & double           & 202          \\
income        &                                          & 4               & integer          & 349          \\
scoma         & SUPPORT Coma Score based on Glasgow D3   &                 & double           & 0            \\
charges       & Hospital Charges                         &                 & double           & 25           \\
totcst        & Total RCC cost                           &                 & double           & 105          \\
totmcst       & Total micro-cost                         &                 & double           & 372          \\
avtisst       & Average TISS, Days 3-25                  &                 & double           & 6            \\
race          &                                          & 5               & integer          & 5            \\
meanbp        & Mean Arterial Blood Pressure Day 3       &                 & double           & 0            \\
wblc          & White Blood Cell Count Day 3             &                 & double           & 24           \\
hrt           & Heart Rate Day 3                         &                 & double           & 0            \\
resp          & Respiration Rate Day 3                   &                 & double           & 0            \\
temp          & Temperature (celcius) Day 3              &                 & double           & 0            \\
pafi          & PaO2/(.01*FiO2) Day 3                    &                 & double           & 253          \\
alb           & Serum Albumin Day 3                      &                 & double           & 378          \\
bili          & Bilirubin Day 3                          &                 & double           & 297          \\
crea          & Serum creatinine Day 3                   &                 & double           & 3            \\
sod           & Serum sodium Day 3                       &                 & double           & 0            \\
ph            & Serum pH (arterial) Day 3                &                 & double           & 250          \\
glucose       & Glucose Day 3                            &                 & double           & 470          \\
bun           & BUN Day 3                                &                 & double           & 455          \\
urine         & Urine Output Day 3                       &                 & double           & 517          \\
adlp          & ADL Patient Day 3                        &                 & double           & 634          \\
adls          & ADL Surrogate Day 3                      &                 & double           & 310          \\
sfdm2         &                                          & 5               & integer          & 159          \\
adlsc         & Imputed ADL Calibrated to Surrogate      &                 & double           & 0           
\end{tabular}
\caption{A description of each variable in the SUPPORT dataset, taken from (REF).}
\label{tab:support1}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{cc}
\hline
\textbf{Variable} & \textbf{Levels}                      \\
\hline
sex      & female                                        \\
         & male                                          \\
dzgroup  & ARF/MOSF w/Sepsis                             \\
         & COPD                                          \\
         & CHF                                           \\
         & Cirrhosis                                     \\
         & Coma                                          \\
         & Colon Cancer                                  \\
         & Lung Cancer                                   \\
         & MOSF w/Malig                                  \\
dzclass  & ARF/MOSF                                      \\
         & COPD/CHF/Cirrhosis                            \\
         & Coma                                          \\
         & Cancer                                        \\
income   & under \$11k                                   \\
         & $11-$25k                                      \\
         & $25-$50k                                      \\
         & \textgreater{}\$50k                           \\
race     & white                                         \\
         & black                                         \\
         & asian                                         \\
         & other                                         \\
         & hispanic                                      \\
sfdm2    & no(M2 and SIP pres)                           \\
         & adl\textgreater{}=4 (\textgreater{}=5 if sur) \\
         & SIP\textgreater{}=30                          \\
         & Coma or Intub                                 \\
         & \textless{}2 mo. follow-up                   
\end{tabular}
\caption{A description of each level within each categorical variable in the dataset, taken from REF.}
\label{tab:support2}
\end{table}

Hospital death as a covariate was removed, as this is directly
informative of death. The glmnet package(REF) on CRAN was used to
introduce elastic net, lasso and ridge regressions to casebase. To
visualize the effect of imputation on incidence density, we can refer to
the pre-imputation population-time plot in Figure \ref{fig:ptPreI} and
post-imputation population time plot in Figure \ref{fig:ptPostI}. The
incidence density is consistent between imputed and complete datasets.

\begin{CodeChunk}

\begin{CodeInput}
R> #demonstrating incidence density before imputation
R> plot(casebase::popTime(as.data.frame(support),time = "d.time",event = "death"))
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}

\begin{CodeInput}
R> #demonstrating incidence density after imputation
R> plot(casebase::popTime(as.data.frame(workingCompleteData),time = "d.time",event = "death"))
\end{CodeInput}
\end{CodeChunk}

Our main objective is to compute the absolute risk of death for a given
set of covariates, using regularization when fitting our model. First,
we fit a smooth hazard to the data; for the sake of this example, we
opted for a linear term for time. As casebase makes use of the glmnet
package, we interact with the fitsmoothhazard.fit function using a
matrix interface, where y contains the time and event variables and x
contains all other variables we would like to include in the model. For
the SUPPORT dataset, all factor variables were converted into dummy
variables. Here, we used lasso penalization by setting alpha to 1.

\begin{CodeChunk}

\begin{CodeInput}
R> #Using casebase's glmnet implementation, using splines and a case to base ratio of 100,
R> #to determine the hazard function
R> cb.Model=casebase::fitSmoothHazard.fit(x,y,family="glmnet",time="d.time",event="death",
R>                              formula_time = ~d.time,alpha=1,ratio=100)
\end{CodeInput}
\end{CodeChunk}

Then, we take our model and use the absoluteRisk function to integrate
and retrieve our absolute risk function.

\begin{CodeChunk}

\begin{CodeInput}
R> #Estimating the absolute risk curve using the newData parameter.
R> casebaseAbsolute=casebase::absoluteRisk(cb.Model,time = seq(1,max(completeData$d.time), 0.5),newdata = t(newData) ,
R>                               s="lambda.1se",method=c("numerical"))
\end{CodeInput}
\end{CodeChunk}

We will compare this curve to a regularized version of cox regression,
and a kaplan-meier survival curve. The regularized cox regression
absolute risk curve required two extra steps to retrive. Coxnet from the
glmnet package has tools to fit a regularized cox model, but requires
that a survival object skeleton of the selected variables, so that the
survival package tools can be used to retrive the absolute risk. We
handle this by fitting a regularized cox model with the glmnet package,
and a cox model from the survival package with the selected variables
from the regularized fit. This second model serves as a skeleton that
permits the use of survival package functions. The coefficients from the
regularized model will replace the coeficients in our skeleton. the
resulting model will have the correct coefficients when integrating, but
will have an incorrect standard error. For the purposes of this case
study, we are only interested in the absolute risk curve itself and not
the standard error.

\begin{CodeChunk}

\begin{CodeInput}
R> #Create u and xCox, which will be used when fitting Cox with glmnet.
R> u=survival::Surv(time = as.numeric(y[,2]), event = as.factor(y[,1]))
R> xCox=as.matrix(sparse.model.matrix(death~ .-d.time,data=completeData))[-c(sam),]
R> #hazard for cox using glmnet
R> coxglmFit=glmnet::cv.glmnet(x=xCox,y=u, family="cox",alpha=1)
R> #convergence demonstrated in plot
R> plot(coxglmFit)
R> #taking the coefficient estimates for later use
R> nonzero_covariate_cox <- predict(coxglmFit, type = "nonzero", s = "lambda.1se")
R> nonzero_coef_cox <- coef(coxglmFit, s = "lambda.1se")
R> #creating a new dataset that only contains the covariates chosen through glmnet.
R> #cleanCoxData<- as.data.frame(cbind(as.numeric(workingData$d.time),as.factor(workingData$death), xCox[,nonzero_covariate_cox$X1]))
R> cleanCoxData<-as.data.frame(cbind(as.numeric(y[,2]),as.numeric(y[,1]),xCox[,nonzero_covariate_cox$X1]))
R> #newDataCox<-xCox[sam,nonzero_covariate_cox$X1]
R> #fitting a cox model using regular estimation, however we will not keep it.
R> #this is used more as an object place holder.
R> coxFit <- survival::coxph(Surv(time=V1,event=V2) ~ ., data = cleanCoxData)
R> #The coefficients of this object will be replaced with the estimates from coxglmFit.
R> #Doing so makes it so that everything is invalid aside from the coefficients.
R> #In this case, all we need to estimate the absolute risk is the coefficients.
R> #Std. error would be incorrect here, if we were to draw error bars.
R> coxFit$coefficients<-nonzero_coef_cox@x
R> #Fitting absolute risk curve for cox+glmnet
R> newDataCox=newData[nonzero_covariate_cox$X1-1]
R> 
R> abCoxFit<-survival::survfit(coxFit,newdata=as.data.frame(t(newDataCox)),time = seq(0,max(completeData$d.time), 1),type="breslow")
\end{CodeInput}
\end{CodeChunk}

We used the survival package to calculate the kaplan-meier absolute risk
function.

\begin{CodeChunk}

\begin{CodeInput}
R> #creating a surv object to be used to fit an unadjusted absolute risk curve.
R> # (Kaplan-Meier risk curve)
R> km <- survival::Surv(time = completeData$d.time, event = completeData$death)
R> abKm<-survival::survfit(km~1,type='kaplan-meier',conf.type='log')
\end{CodeInput}
\end{CodeChunk}

Now that our three absolute risk functions have been calculated, we
compare them all in Figure \ref{fig:abSC}. Both coxnet and casebase
decrease the absolute risk by the end of the study, in comparison to
kaplan-meier.

\begin{CodeChunk}

\begin{CodeInput}
R> # A plot to compare all three Absolute risk (Commulative Incidence)
R> plot(casebaseAbsolute,type='l',col="black",lwd=2,main="Support- CaseBase vs. Cox+glmnet vs KM Absolute Risk Curves",xlab="Survival-Time",ylab="Cumulative Incidence",ylim=c(0,1),xlim=)
R> lines(abCoxFit ,col="red",fun="event",lwd=3,conf.int = FALSE)
R> lines(abKm ,col="Blue",fun="event",lwd=3,conf.int = FALSE)
R> legend("bottom", 
R>        legend = c( "Casebase (Lasso+linear)","semi-parametric (Cox)+glmnet","KM curve"), 
R>        col = c("black","red","Blue"),
R>        lty = c(1, 1, 1), 
R>        bg = "gray90")
\end{CodeInput}
\end{CodeChunk}

\hypertarget{case-study-4stanford-heart-transplant-data}{%
\section{Case study 4--Stanford Heart Transplant
Data}\label{case-study-4stanford-heart-transplant-data}}

Although the previous case studies provide a broad overview of the
capabilities of \pkg{casebase}, they all have two properties in common:

\begin{itemize}
  \item All covariates are fixed, i.e. they do not change over time;
  \item The estimated hazard functions satisfy the "proportional hazard" assumption.
\end{itemize}

Case-base sampling has been used in the literature to study vaccination
safety, where the exposure period was defined as the week following
vaccination \citep{saarela2015case}. Hence, the main covariate of
interest, i.e.~exposure to the vaccine, was changing over time. In this
context, case-base sampling offers an efficient alternative to nested
case-control designs or self-matching.

The purpose of the next case study is to show how \pkg{casebase} can
also be used for survival analysis problems that do not share the
properties above. To this end, we will use the Stanford Heart Transplant
Data \citep[\citet{crowley1977covariance}]{clark1971cardiac}; we will
use the version available in the \pkg{survival} package.

Recall the setting of this study: patients were admitted to the Stanford
program after meeting with their physician and determining that they
were unlikely to respond to other forms of treatment. After enrollment,
the program searches for a suitable donor for the patient, which can
take anywhere between a few days to almost a year. We are interested in
the effect of a heart transplant on survival; therefore, the patient is
considered exposed only after the transplant has occured.

As before, we can look at the population-time plot for a graphical
summary of the event incidence. As we can see, most events occur early
during the follow-up period, and therefore we do not expect the hazard
to be constant.

\begin{CodeChunk}

\begin{CodeInput}
R> library(survival)
R> library(casebase)
R> 
R> stanford_popTime <- popTime(jasa, time = "futime", 
R+                             event = "fustat")
R> plot(stanford_popTime)
\end{CodeInput}


\begin{center}\includegraphics{../figures/stanford-poptime-1} \end{center}

\end{CodeChunk}

Since the exposure is time-dependent, we need to manually define the
exposure variable \emph{after} case-base sampling and \emph{before}
fitting the hazard function. For this reason, we will use the
\texttt{sampleCaseBase} function directly.

\begin{CodeChunk}

\begin{CodeInput}
R> library(tidyverse)
R> library(lubridate)
R> 
R> cb_data <- sampleCaseBase(jasa, time = "futime", 
R+                           event = "fustat", ratio = 10)
\end{CodeInput}
\end{CodeChunk}

Next, we will compute the number of days from acceptance into the
program to transplant, and we use this variable to determine whether
each population-moment is exposed or not.

\begin{CodeChunk}

\begin{CodeInput}
R> # Define exposure variable
R> cb_data <- mutate(cb_data,
R+                   txtime = time_length(accept.dt %--% tx.date, 
R+                                        unit = "days"),
R+                   exposure = case_when(
R+                     is.na(txtime) ~ 0L,
R+                     txtime > futime ~ 0L,
R+                     txtime <= futime ~ 1L
R+                   ))
\end{CodeInput}
\end{CodeChunk}

Finally, we can fit the hazard using various linear predictors.

\begin{CodeChunk}

\begin{CodeInput}
R> library(splines)
R> # Fit several models
R> fit1 <- fitSmoothHazard(fustat ~ exposure,
R+                         data = cb_data, time = "futime")
R> fit2 <- fitSmoothHazard(fustat ~ exposure + futime,
R+                         data = cb_data, time = "futime")
R> fit3 <- fitSmoothHazard(fustat ~ exposure + bs(futime),
R+                         data = cb_data, time = "futime")
R> fit4 <- fitSmoothHazard(fustat ~ exposure*bs(futime),
R+                         data = cb_data, time = "futime")
\end{CodeInput}
\end{CodeChunk}

Note that the fourth model (i.e.~\texttt{fit4}) includes an interaction
term between exposure and follow-up time. In other words, this model no
longer exhibit proportional hazards. The evidence of non-proportionality
of hazards in the Stanford Heart Transplant data has been widely
discussed \citep{arjas1988graphical}.

We can then compare the goodness of fit of these four models using the
Akaike Information Criterion (AIC).

\begin{CodeChunk}

\begin{CodeInput}
R> # Compute AIC
R> c("Model1" = AIC(fit1),
R+   "Model2" = AIC(fit2),
R+   "Model3" = AIC(fit3),
R+   "Model4" = AIC(fit4))
\end{CodeInput}

\begin{CodeOutput}
#> Model1 Model2 Model3 Model4 
#>    798    759    737    708
\end{CodeOutput}
\end{CodeChunk}

As we can, the best fit is the fourth model. By visualizing the hazard
functions for both exposed and unexposed individuals, we can more
clearly see how the hazards are no longer proportional.

\begin{CodeChunk}

\begin{CodeInput}
R> # Compute hazards---
R> # First, create a list of time points for both exposure status
R> hazard_data <- expand.grid(exposure = c(0, 1),
R+                            futime = seq(0, 1000,
R+                                         length.out = 100))
R> # Set the offset to zero
R> hazard_data$offset <- 0 
R> # Use predict to get the fitted values, and exponentiate to 
R> # transform to the right scale
R> hazard_data$hazard = exp(predict(fit4, newdata = hazard_data,
R+                                  type = "link"))
R> # Add labels for plots
R> hazard_data$Status = factor(hazard_data$exposure,
R+                             labels = c("NoTrans", "Trans"))
R> 
R> ggplot(hazard_data, aes(futime, hazard, colour = Status)) +
R+     geom_line() +
R+     theme_minimal() +
R+     theme(legend.position = 'top') +
R+     ylab('Hazard') + xlab('Follow-up time')
\end{CodeInput}


\begin{center}\includegraphics{../figures/stanford-hazard-1} \end{center}

\end{CodeChunk}

The non-proportionality seems to be more pronounced at the beginning of
follow-up than the end. Finally, we can turn these estimate of the
hazard function into estimates of the cumulative incidence functions.

\begin{CodeChunk}

\begin{CodeInput}
R> # Compute absolute risk curves
R> newdata <- data.frame(exposure = c(0, 1))
R> absrisk <- absoluteRisk(fit4, newdata = newdata, 
R+                         time = seq(0, 1000, length.out = 100))
R> 
R> colnames(absrisk) <- c("Time", "NoTrans", "Trans")
R> 
R> # Rearrange the data
R> absrisk <- gather(as.data.frame(absrisk),
R+                   "Status", "Risk", -Time)
R>  
R> ggplot(absrisk, aes(Time, Risk, colour = Status)) +
R+   geom_line() +
R+   theme_minimal() +
R+   theme(legend.position = 'top') +
R+   expand_limits(y = 0.5) +
R+   xlab('Follow-up time') + ylab('Cum. Incidence')
\end{CodeInput}


\begin{center}\includegraphics{../figures/stanford-risk-1} \end{center}

\end{CodeChunk}

Note that we can easily adapt the code above to the situation where a
patient receives a heart transplant at a point in time of interest, for
example after 30 days.

\begin{CodeChunk}

\begin{CodeInput}
R> # Compute hazards---
R> # First, create a list of time points for both exposure status
R> one_yr_haz <- data.frame(futime = seq(0, 365,
R+                                       length.out = 100))
R> one_yr_haz <- mutate(one_yr_haz,
R+                      offset = 0,
R+                      exposure = if_else(futime < 30, 0, 1))
R> one_yr_haz$hazard = exp(predict(fit4, newdata = one_yr_haz,
R+                                 type = "link"))
R> ggplot(one_yr_haz, aes(futime, hazard)) +
R+   geom_line() +
R+   theme_minimal() +
R+   theme(legend.position = 'top') +
R+   ylab('Hazard') + xlab('Follow-up time') +
R+   geom_vline(xintercept = 30, linetype = 'dashed')
\end{CodeInput}


\begin{center}\includegraphics{../figures/stanford-1y-haz-1} \end{center}

\end{CodeChunk}

We can then compare the 1-year mortality risk without transplant and
with transplant at 30 days.

\begin{CodeChunk}

\begin{CodeInput}
R> absoluteRisk(fit4, newdata = data.frame(exposure = 0),
R+              time = 365)
\end{CodeInput}

\begin{CodeOutput}
#>         
#> 365 0.32
\end{CodeOutput}

\begin{CodeInput}
R> 
R> # Use the trapezoidal rule to estimate the cumulative hazard
R> min_hazard <- min(one_yr_haz$hazard)
R> max_hazard <- max(one_yr_haz$hazard)
R> increment <- 365/99
R> cumhaz_est <- 0.5*increment*(sum(2*one_yr_haz$hazard) - min_hazard - max_hazard)
R> 1 - exp(-cumhaz_est)
\end{CodeInput}

\begin{CodeOutput}
#> [1] 0.24
\end{CodeOutput}
\end{CodeChunk}

As we can see, the risk estimate at 1-year is about 30\% lower if the
patient receives a heart transplant at 30 days.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In this article, we presented the \proglang{R} package \pkg{casebase}
that provides functions to fit smooth parametric hazards and estimate
cumulative incidence functions using case-base sampling. We outlined the
theoretical underpinnings of the approach, we provided details about our
implementation, and we illustrated the merits of the approach and the
package through three case studies.

In the following table we provide a comparison between the Cox model and
case-base sampling:

\hypertarget{environment-details}{%
\section{Environment Details}\label{environment-details}}

This report was generated on 2019-12-08 12:31:41 using the following
computational environment and dependencies:

\begin{CodeChunk}

\begin{CodeInput}
R> # which R packages and versions?
R> devtools::session_info()
\end{CodeInput}

\begin{CodeOutput}
#> - Session info ----------------------------------------------------------
#>  setting  value                       
#>  version  R version 3.6.1 (2019-07-05)
#>  os       Ubuntu 18.04.3 LTS          
#>  system   x86_64, linux-gnu           
#>  ui       X11                         
#>  language (EN)                        
#>  collate  en_CA.UTF-8                 
#>  ctype    en_CA.UTF-8                 
#>  tz       America/Winnipeg            
#>  date     2019-12-08                  
#> 
#> - Packages --------------------------------------------------------------
#>  package     * version    date       lib source                          
#>  assertthat    0.2.1      2019-03-21 [1] CRAN (R 3.6.1)                  
#>  backports     1.1.5      2019-10-02 [1] CRAN (R 3.6.1)                  
#>  broom         0.5.2      2019-04-07 [1] CRAN (R 3.6.1)                  
#>  callr         3.3.1      2019-07-18 [1] CRAN (R 3.6.1)                  
#>  casebase    * 0.2.1.9001 2019-07-29 [1] local                           
#>  cellranger    1.1.0      2016-07-27 [1] CRAN (R 3.5.0)                  
#>  cli           1.1.0      2019-03-19 [1] CRAN (R 3.6.1)                  
#>  codetools     0.2-16     2018-12-24 [4] CRAN (R 3.5.2)                  
#>  colorspace    1.4-1      2019-03-18 [1] CRAN (R 3.6.1)                  
#>  crayon        1.3.4      2017-09-16 [1] CRAN (R 3.5.0)                  
#>  data.table    1.12.2     2019-04-07 [1] CRAN (R 3.6.1)                  
#>  desc          1.2.0      2018-05-01 [1] CRAN (R 3.5.0)                  
#>  devtools      2.1.0      2019-07-06 [1] CRAN (R 3.6.1)                  
#>  digest        0.6.22     2019-10-21 [1] CRAN (R 3.6.1)                  
#>  dplyr       * 0.8.3      2019-07-04 [1] CRAN (R 3.6.1)                  
#>  ellipsis      0.3.0      2019-09-20 [1] CRAN (R 3.6.1)                  
#>  evaluate      0.14       2019-05-28 [1] CRAN (R 3.6.1)                  
#>  forcats     * 0.4.0      2019-02-17 [1] CRAN (R 3.6.1)                  
#>  foreach     * 1.4.4      2017-12-12 [1] CRAN (R 3.5.0)                  
#>  fs            1.3.1      2019-05-06 [1] CRAN (R 3.6.1)                  
#>  generics      0.0.2      2018-11-29 [1] CRAN (R 3.6.1)                  
#>  ggplot2     * 3.2.1      2019-08-10 [1] CRAN (R 3.6.1)                  
#>  glmnet      * 2.0-18     2019-05-20 [1] CRAN (R 3.6.1)                  
#>  glue          1.3.1      2019-03-12 [1] CRAN (R 3.6.1)                  
#>  gtable        0.3.0      2019-03-25 [1] CRAN (R 3.6.1)                  
#>  haven         2.1.1      2019-07-04 [1] CRAN (R 3.6.1)                  
#>  hms           0.5.2      2019-10-30 [1] CRAN (R 3.6.1)                  
#>  htmltools     0.3.6      2017-04-28 [1] CRAN (R 3.5.0)                  
#>  httr          1.4.1      2019-08-05 [1] CRAN (R 3.6.1)                  
#>  iterators     1.0.9      2017-12-12 [1] CRAN (R 3.5.0)                  
#>  jsonlite      1.6        2018-12-07 [1] CRAN (R 3.6.1)                  
#>  knitr         1.23       2019-05-18 [1] CRAN (R 3.6.1)                  
#>  labeling      0.3        2014-08-23 [1] CRAN (R 3.5.0)                  
#>  lattice       0.20-38    2018-11-04 [4] CRAN (R 3.5.1)                  
#>  lazyeval      0.2.2      2019-03-15 [1] CRAN (R 3.6.1)                  
#>  lifecycle     0.1.0      2019-08-01 [1] CRAN (R 3.6.1)                  
#>  lubridate   * 1.7.4      2018-04-11 [1] CRAN (R 3.5.0)                  
#>  magrittr    * 1.5        2014-11-22 [1] CRAN (R 3.5.0)                  
#>  Matrix      * 1.2-18     2019-11-27 [4] CRAN (R 3.6.1)                  
#>  memoise       1.1.0      2017-04-21 [1] CRAN (R 3.5.0)                  
#>  mgcv          1.8-24     2018-06-18 [1] CRAN (R 3.5.1)                  
#>  modelr        0.1.5      2019-08-08 [1] CRAN (R 3.6.1)                  
#>  munsell       0.5.0      2018-06-12 [1] CRAN (R 3.6.1)                  
#>  nlme          3.1-142    2019-11-07 [4] CRAN (R 3.6.1)                  
#>  pillar        1.4.2      2019-06-29 [1] CRAN (R 3.6.1)                  
#>  pkgbuild      1.0.3      2019-03-20 [1] CRAN (R 3.6.1)                  
#>  pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 3.6.1)                  
#>  pkgload       1.0.2      2018-10-29 [1] CRAN (R 3.6.1)                  
#>  prettyunits   1.0.2      2015-07-13 [1] CRAN (R 3.5.2)                  
#>  processx      3.4.1      2019-07-18 [1] CRAN (R 3.6.1)                  
#>  ps            1.3.0      2018-12-21 [1] CRAN (R 3.6.1)                  
#>  purrr       * 0.3.3      2019-10-18 [1] CRAN (R 3.6.1)                  
#>  R6            2.4.1      2019-11-12 [1] CRAN (R 3.6.1)                  
#>  Rcpp          1.0.3      2019-11-08 [1] CRAN (R 3.6.1)                  
#>  readr       * 1.3.1      2018-12-21 [1] CRAN (R 3.6.1)                  
#>  readxl        1.3.1      2019-03-13 [1] CRAN (R 3.6.1)                  
#>  remotes       2.1.0      2019-06-24 [1] CRAN (R 3.6.1)                  
#>  rlang         0.4.1      2019-10-24 [1] CRAN (R 3.6.1)                  
#>  rmarkdown     1.16       2019-10-01 [1] CRAN (R 3.6.1)                  
#>  rprojroot     1.3-2      2018-01-03 [1] CRAN (R 3.5.0)                  
#>  rstudioapi    0.10       2019-03-19 [1] CRAN (R 3.6.1)                  
#>  rticles       0.9.1      2019-07-22 [1] Github (rstudio/rticles@8d56fc6)
#>  rvest         0.3.5      2019-11-08 [1] CRAN (R 3.6.1)                  
#>  scales        1.0.0      2018-08-09 [1] CRAN (R 3.6.1)                  
#>  sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 3.6.1)                  
#>  stringi       1.4.3      2019-03-12 [1] CRAN (R 3.6.1)                  
#>  stringr     * 1.4.0      2019-02-10 [1] CRAN (R 3.6.1)                  
#>  survival    * 2.44-1.1   2019-04-01 [4] CRAN (R 3.6.1)                  
#>  testthat      2.2.0      2019-07-22 [1] CRAN (R 3.6.1)                  
#>  tibble      * 2.1.3      2019-06-06 [1] CRAN (R 3.6.1)                  
#>  tidyr       * 1.0.0      2019-09-11 [1] CRAN (R 3.6.1)                  
#>  tidyselect    0.2.5      2018-10-11 [1] CRAN (R 3.5.2)                  
#>  tidyverse   * 1.2.1      2017-11-14 [1] CRAN (R 3.5.0)                  
#>  usethis       1.5.1      2019-07-04 [1] CRAN (R 3.6.1)                  
#>  vctrs         0.2.0      2019-07-05 [1] CRAN (R 3.6.1)                  
#>  VGAM          1.1-1      2019-02-18 [1] CRAN (R 3.6.1)                  
#>  withr         2.1.2      2018-03-15 [1] CRAN (R 3.5.0)                  
#>  xfun          0.8        2019-06-25 [1] CRAN (R 3.6.1)                  
#>  xml2          1.2.2      2019-08-09 [1] CRAN (R 3.6.1)                  
#>  yaml          2.2.0      2018-07-25 [1] CRAN (R 3.6.1)                  
#>  zeallot       0.1.0      2018-01-28 [1] CRAN (R 3.5.1)                  
#> 
#> [1] /home/mturgeon/Rlibs
#> [2] /usr/local/lib/R/site-library
#> [3] /usr/lib/R/site-library
#> [4] /usr/lib/R/library
\end{CodeOutput}
\end{CodeChunk}

The current Git commit details are:

\begin{CodeChunk}

\begin{CodeInput}
R> # what commit is this file at? 
R> git2r::repository(here::here())
\end{CodeInput}

\begin{CodeOutput}
#> Local:    review-max /home/mturgeon/Documents/git_repositories/cbpaper
#> Head:     [69663bb] 2019-12-08: Merge branch 'review-max' of https://github.com/sahirbhatnagar/cbpaper into review-max
\end{CodeOutput}
\end{CodeChunk}

\bibliography{references.bib}


\end{document}

