\documentclass[
]{jss}

\usepackage[utf8]{inputenc}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
Sahir Bhatnagar *\\McGill University \And Maxime Turgeon *\\University of Manitoba \AND Jesse Islam\\McGill University \And James Hanley\\McGill University \And Olli Saarela\\University of Toronto
}
\title{\pkg{casebase}: An Alternative Framework For Survival Analysis}

\Plainauthor{Sahir Bhatnagar *, Maxime Turgeon *, Jesse Islam, James Hanley, Olli Saarela}
\Plaintitle{casebase: An Alternative Framework For Survival Analysis}
\Shorttitle{\pkg{casebase}: An Alternative Framework For Survival Analysis}

\Abstract{
The abstract of the article. * joint co-authors
}

\Keywords{keywords, not capitalized, \proglang{Java}}
\Plainkeywords{keywords, not capitalized, Java}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    Sahir Bhatnagar *\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{sahir.bhatnagar@mail.mcgill.ca}\\
  URL: \url{http://sahirbhatnagar.com/}\\~\\
      Maxime Turgeon *\\
  University of Manitoba\\
  186 Dysart Road Winnipeg, MB, Canada R3T 2N2\\
  E-mail: \email{max.turgeon@umanitoba.ca}\\
  URL: \url{https://maxturgeon.ca/}\\~\\
      Jesse Islam\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{jesse.islam@mail.mcgill.ca}\\
  
      James Hanley\\
  McGill University\\
  1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
  E-mail: \email{james.hanley@mcgill.ca}\\
  URL: \url{http://www.medicine.mcgill.ca/epidemiology/hanley/}\\~\\
      Olli Saarela\\
  University of Toronto\\
  Dalla Lana School of Public Health, 155 College Street, 6th floor,
  Toronto, Ontario M5T 3M7, Canada\\
  E-mail: \email{olli.saarela@utoronto.ca}\\
  URL: \url{http://individual.utoronto.ca/osaarela/}\\~\\
  }


% Pandoc header

\usepackage{amsmath} \usepackage{longtable} \usepackage{graphicx} \usepackage{tabularx} \usepackage{float} \usepackage{booktabs} \usepackage{makecell} \usepackage{tabu} \DeclareUnicodeCharacter{2500}{-}

\begin{document}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Survival analysis has been greatly influenced over the last 50 years by
the partial likelihood approach of the Cox proportional hazard model
\citep{cox1972regression}. This approach provides a flexible way of
assessing the influence of covariates on the hazard function, without
the need to specify a parametric survival model. This flexibility comes
at the cost of decoupling the baseline hazard from the effect of the
covariates. To recover the whole survival curve---or the cumulative
incidence function (CIF)---we then need to separately estimate the
baseline hazard \citep{breslow1972discussion}. This in turn often leads
to stepwise estimates of the survival function.

From the perspective of clinicians and their patients, the most relevant
quantity is often the 5- or 10-year risk of having a certain event given
the patient's particular circumstances, and not the hazard ratio between
a treatment and control group. Therefore, to make sound clinical
decisions, it is important to accurately estimate the \emph{full} hazard
function, which can then be used to estimate the cumulative incidence
function (CIF). From this perspective, the CIF and the survival function
estimates are now smooth functions of time.

With the goal of fitting smooth-in-time hazard functions, Hanley \&
Miettinen \citeyearpar{hanley2009fitting} proposed a general framework
for estimating fully parametric hazard models via logistic regression.
Their approach provides users that are familiar with generalized linear
models with a natural way of fitting parametric survival models.
Moreover, their framework is very flexible: general functions of time
can be estimated (e.g.~using splines or general additive models), and
hence these models retain some of the flexibility of partial likelihood
approaches.

In this article, we present an \proglang{R} package that combines the
ideas of Hanley \& Miettinen into a simple interface. The purpose of the
\pkg{casebase} package is to provide practitioners with an easy-to-use
software tool to predict the risk (or cumulative incidence) of an event,
conditional on a particular patient's covariate profile. Our package
retains the flexibility of case-base sampling and the familiar interface
of the \code{glm} function.

In what follows, we first recall some theoretical details on case-base
sampling and its use for estimating parameteric hazard functions. We
then give a short review of existing \proglang{R} packages that
implement comparable features as \pkg{casebase}. Next, we provide some
details about the implementation of case-base sampling in our package,
and we give a brief survey of its main functions. This is followed by
four case studies that illustrate the flexibility and capabilities of
\pkg{casebase}. We show how the same framework can be used for competing
risk analyses, penalized estimation, and for studies with time-dependent
exposures. Finally, we end the article with a discussion of the results
and of future directions.

\hypertarget{theory}{%
\section{Theoretical details}\label{theory}}

As discussed in Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the
key idea behind case-base sampling is to discretize the study base into
an infinite amount of \emph{person moments}. These person moments are
indexed by both an individual in the study and a time point, and
therefore each person moment has a covariate profile, an exposure status
and an outcome status attached to it. We note that there is only a
finite number of person moments associated with the event of interest
(what Hanley \& Miettinen call the \emph{case series}). The case-base
sampling refers to the sampling from the base of a representative finite
sample called the \emph{base series}.

As shown by Saarela \& Arjas \citeyearpar{saarela2015non} (and further
expanded in Saarela \citeyearpar{saarela2016case}), writing the
likelihood arising from this data-generating mechanism using the
framework of non-homogeneous Poisson processes, we eventually reach an
expression where each person-moment's contribution is of the form
\[\frac{h(t)^{dN(t)}}{\rho(t) + h(t)},\] where \(N(t)\) is the counting
process associated with the event of interest, \(h(t)\) is the
corresponding hazard function, and \(\rho(t)\) is the hazard function
for the Poisson process associated with case-base sampling. This
parametric form suggests that we can readily estimate log-hazards of the
form \(\log(h(t)) = g(t; X)\) using logistic regression, where each
observation corresponds to a person moment, the function \(g(t; X)\) is
linear in a finite number of parameters, and where we treat
\(-\log(\rho(t))\) as an offset.

In Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the authors
suggest sampling the base series \emph{uniformly} from the study base.
In terms of Poisson processes, this sampling strategy corresponds
essentially to a time-homogeneous Poisson process with intensity equal
to \(b/B\), where \(b\) is the number of sampled observations in the
base series, and \(B\) is the total population-time for the study base.
More complex examples are also available; see for example Saarela \&
Arjas \citeyearpar{saarela2015non}, where the probabilities of the
sampling mechanism are proportional to the cardiovascular disease event
rate given by the Framingham score. With this sampling mechanism,
Saarela \& Arjas are able to increase the efficiency of their
estimators, when compared to uniform sampling.

Let \(g(t; X)\) be the linear predictor such that
\(\log(h(t;X)) = g(t; X)\). Different functions of \(t\) lead to
different parametric hazard models. The simplest of these models is the
one-parameter exponential distribution which is obtained by taking the
hazard function to be constant over the range of \(t\):

\[ \log(h(t; X)) = \beta_0 + \beta_1 X.\]

The instantaneous failure rate is independent of
\(t\).\footnote{The conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been in the study. This is also known as the \textit{memoryless property} \citep{kalbfleisch2011statistical}.}

The Gompertz hazard model is given by including a linear term for time:

\[ \log(h(t; X)) = \beta_0 + \beta_1 t + \beta_2 X.\]

Use of \(\log(t)\) yields the Weibull hazard which allows for a power
dependence of the hazard on time \citep{kalbfleisch2011statistical}:

\[ \log(h(t; X)) = \beta_0 + \beta_1 \log(t) + \beta_2 X. \]

For competing-risk analyses with \(J\) possible events, we can show that
each person-moment's contribution of the likelihood is of the form

\[\frac{h_j(t)^{dN_j(t)}}{\rho(t) + \sum_{j=1}^Jh_j(t)},\]

where \(N_j(t)\) is the counting process associated with the event of
type \(j\) and \(h_j(t)\) is the corresponding hazard function. As may
be expected, this functional form is similar to the terms appearing in
the likelihood function for multinomial
regression.\footnote{Specifically, it corresponds to the following parametrization: \begin{align*} \log\left(\frac{P(Y=j \mid X)}{P(Y = J \mid X)}\right) = X^T\beta_j, \qquad j = 1,\ldots, J-1.\end{align*}}

\hypertarget{existing-packages}{%
\section{Existing packages}\label{existing-packages}}

Survival analysis is an important branch of applied statistics and
epidemiology. Accordingly, there is a vast ecosystem of \code{R}
packages implementing different methods. In this section, we describe
how the functionalities of \pkg{casebase} compare to these packages.

At the time of writing, a cursory examination of CRAN's task view on
survival analysis reveals that there are over 250 packages related to
survival analysis \citeyearpar{survTaskView}. For the purposes of this
article, we restricted our description to packages that implement at
least one of the following features: parametric modeling,
non-proportional hazard models, competing risk analysis, penalized
estimation, and cumulative incidence curve estimation. By searching for
appropriate keywords in the \code{DESCRIPTION} file of these packages,
we found 60 relevant packages. These 60 packages were then manually
examined to determine which ones are comparable to \pkg{casebase}. In
particular, we excluded packages that were focused on a different set of
problems, such as frailty and multi-state models. The remaining 14
packages appear in Table \ref{tab:surv-pkgs}, along with a description
of some of the functionalities they offer.

Several packages implement penalized estimation for the Cox model:
\pkg{glmnet} \citeyearpar{regpathcox}, \pkg{glmpath}
\citeyearpar{park_hastie}, \pkg{penalized} \citeyearpar{l1penal},
\pkg{RiskRegression} \citeyearpar{gerds_blanche}. Moreover, some
packages also include penalized estimation in the context of Cox models
with time-varying coefficients: \pkg{CoxRidge}
\citeyearpar{perperoglou}, \pkg{rstpm2} \citeyearpar{clements_liu} and
\pkg{survival} \citeyearpar{survival-package}. On the other hand,
\pkg{casebase} provides penalized estimation in the context of
parametric hazards. To our knowledge, this is the only package to offer
this functionality.
\textcolor{red}{Is this true? Because it seems to be a logical conclusion from your paragraph.}

Parametric survival models are implemented in a handful of packages:
\pkg{CFC} \citeyearpar{sharabiani_mahani_2019}, \pkg{flexsurv}
\citeyearpar{flexsurv}, \pkg{SmoothHazard} \citeyearpar{smoothHazard},
\pkg{rsptm2} \citeyearpar{clements_liu}, \pkg{mets}
\citeyearpar{scheike2014estimating}, and \pkg{survival}. The types of
models they allow vary for each package. For example, \pkg{SmoothHazard}
is limited to Weibull distributions \citeyearpar{smoothHazard}, whereas
both \pkg{flexsurv} and \pkg{survival} allow users to supply any
distribution of their choice. Also, \pkg{flexsurv}, \pkg{smoothhazard},
\pkg{mets} and \pkg{rstpm2} also have the ability to model the effect of
time using splines, which allows flexible modeling of the hazard
function. Moreover, \pkg{flexsurv} has the ability to estimate both
scale and shape parameters for a variety of parametric families (see
Table \ref{tab:dists}). As discussed above, \pkg{casebase} can model any
parametric family whose log-hazard can be expressed as a linear model of
covariates (including time). Therefore, our package allows the user to
model the effect of time using splines, and through interaction terms
involving covariates and time, it also allows user to fit time-varying
coefficient models. However, we do not explicitly model any shape
parameter, unlike \pkg{flexsurv}.

Of the methods mentioned so far, only \pkg{CFC}, \pkg{flexsurv},
\pkg{mets} and \pkg{survival} contain implementations for competing
risks. The differentiating factor between these packages and
\pkg{casebase} is that \pkg{casebase} handles competing risks through
multiple logistic regression.
\textcolor{red}{There's a literature review of packages for competing-risk analysis in the paper for \pkg{CFC}. Could you incorporate some of their discussion, with proper citations? The biggest omission right now is \pkg{cmprsk}, which we actually use below.}

Finally, several packages include functions to estimate the cumulative
incidence function. The corresponding methods generally fall into two
categories: transformation of the estimated hazard function, and
semi-parametric estimation of the baseline hazard. The first category
broadly corresponds to parametric survival models, where the full hazard
is explictly modeled. Using this estimate, the survival function and the
cumulative incidence function can be obtained using their functional
relationships (see Equations \ref{eqn:surv} and \ref{eqn:CI} below).
Packages including this functionality include \pkg{CFC}, \pkg{Flexsurv},
\pkg{mets}, and \pkg{survival}. Our package \pkg{casebase} also follows
this approach for both single-event and competing-event analyses. The
second category outlined above broadly corresponds to Cox models. These
models do not model the full hazard function, and therefore the baseline
hazard needs to be estimated separately in order to estimate the
survival function. This is achieved using semi-parametric estimators
(e.g.~Breslow's estimator). Packages that implement this approach
include \pkg{RiskRegression}, \pkg{rstpm2}, and \pkg{survival}. As
mentioned in the introduction, a key distinguishing factor between these
two approaches is that the first category leads to smooth estimates of
the cumulative incidence function, whereas the second category produces
estimates in the form of step-wise functions. This was one of the main
motivations for introducting case-base sampling in survival analysis.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
\hline
\textbf{Package}        & \vtop{\hbox{\strut \textbf{Competing}}\hbox{\strut \textbf{risks}}}  & \textbf{Non-proportional} & \textbf{Penalization} & \textbf{Splines} & \textbf{Parametric} & \textbf{Semi-parametric} & \vtop{\hbox{\strut \textbf{Interval/left}}\hbox{\strut \textbf{censoring}}} & \vtop{\hbox{\strut \textbf{Absolute}}\hbox{\strut \textbf{risk}}}           \\
\hline
\textbf{casebasee} \cite{hanley2009fitting}       & x                        & x                         & x                     & x                & x                   &                          &                                  & x                                \\
\textbf{CFC} \cite{sharabiani_mahani_2019}            & x                        & x                         &                       &                  & x                   &                          &                                  & x                                \\
\textbf{coxRidge} \cite{perperoglou}       &                          & x                         & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{crrp} \cite{fu}           & x                        &                           & x                     &                  &                     &                          &                                  &                                  \\
\textbf{fastcox} \cite{yi_zou}        &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{Flexrsurv} \cite{flexrsurv}      &                          & x                         &                       & x                & x                   &                          &                                  & x               \\
\textbf{Flexsurv} \cite{flexsurv}       & x                        & x                         &                       & x                & x                   &                          &                                  & x               \\
\textbf{glmnet} \cite{regpathcox}         &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{glmpath} \cite{park_hastie}       &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{mets} \cite{scheike2014estimating}           & x                        &                           &                       & x                &                     & x                        &                                  & x                                \\
\textbf{penalized} \cite{goeman_meijer2019}      &                          &                           & x                     &                  &                     & x                        &                                  &                                  \\
\textbf{RiskRegression} \cite{gerds_blanche} &                          &                           & x                     &                  &                     & x                        &                                  & x                                \\
\textbf{Rstpm2} \cite{clements_liu}         &                          & x                         & x                     & x                & x                   & x                        & x                                & x                         \\
\textbf{SmoothHazard} \cite{smoothHazard}   &                          & x                         &                       & x                & x                   &                          & x                            &                                      \\
\textbf{Survival} \cite{survival-package}       & x                        & x                         &                       &                  & x                   & x                        & x                                & x                               
\end{tabular}%
}
\caption{Different features of interest in various survival packages.}
\label{tab:surv-pkgs}
\end{table}

\hypertarget{implementation-details}{%
\section{Implementation details}\label{implementation-details}}

The functions in the casebase package can be divided into two
categories: 1) data visualization, in the form of population-time plots
and hazard curves; and 2) parametric modeling. We explicitly aimed at
being compatible with both \code{data.frame}s and \code{data.table}s.
This is evident in some of the coding choices we made, and it is also
reflected in our unit tests.

\hypertarget{population-time-plots}{%
\subsection{Population-time plots}\label{population-time-plots}}

The case-base sampling approach described in Section \ref{theory} can be
visualized in the form of a population time plot. They are extremely
informative graphical displays of survival data and should be one of the
first steps in an exploratory data analysis. We facilitate this task in
the \pkg{casebase} package using the \code{popTime} function and
\code{plot} method as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \code{casebase::popTime} function takes as input the original
  dataset along with the column names corresponding to the timescale,
  the event status and an exposure group of interest (optional). This
  will create an object of class \code{popTime} (or
  \code{popTimeExposure} if you specify a value for the exposure
  argument).\\
\item
  The corresponding \code{plot} methods for the object created in Step 1
  can be called to create the population time plot with several options
  for customizing the aesthetics.
\end{enumerate}

To illustrate these functions we will use the European Randomized Study
of Prostate Cancer Screening data which is available through the
\pkg{casebase} package. The results of this study were published by
\citep{schroder2009screening}, and the dataset itself was obtained using
the approach described in \citep{liu2014recovering}.

We first create the necessary dataset for producing the population time
plots using the \code{casebase::popTime} function. In this example we
stratify by the treatment group. Note that the resulting object stores
the exposure variable as an attribute:

\begin{CodeChunk}

\begin{CodeOutput}
#> [1] "ScrArm"
\end{CodeOutput}
\end{CodeChunk}

We then pass this object to the \code{plot} function which leverages the
\pkg{ggplot2} package \citep{ggplot2} to build population time plots. It
builds the plot by sequentially adding layers, starting with a layer for
the area representing the population time followed by points to show the
casebase sampling mechanism. This function gives user the flexibility to
add any combination of the case.series, base.series and competing
events. The case series and competing events are sampled at random
vertically on the plot in order to visualise the incidence density using
the popTime function. That is, imagine we draw a vertical line at a
specific event time. We then plot the point at a randomly sampled
y-coordinate along this vertical line. This is done to avoid having all
points along the upper edge of the plot (because the subjects with the
least amount of observation time are plotted at the top of the y-axis).
By randomly distributing them, we can get a better sense of the
inicidence density. The base series is sampled horizontally on the plot
using the sampleCaseBase function.

One benefit from these plots is that it allows you to see the incidence
density. This can be seen from the distribution of the red dots in the
above plot. We can see that more events are observed later on in time.
Therefore a constant hazard model would not be appropriate in this
instance as it would overestimate the cumulative incidence earlier on in
time, and underestimate it later on.

The unique \emph{step shape} of the population time plot is due to the
randomization of the Finnish cohorts which were carried out on January 1
of each of the 4 years 1996 to 1999. This, coupled with the uniform
December 31 2006 censoring date, lead to large numbers of men with
exactly 11, 10, 9 or 8 years of follow-up.

It is important to note that the red points are random distributed
across the grey area for each time of event. That is, imagine we draw a
vertical line at a specific event time. We then plot the red point at a
randomly sampled y-coordinate along this vertical line. This is done to
avoid having all red points along the upper edge of the plot (because
the subjects with the least amount of observation time are plotted at
the top of the y-axis). By randomly distributing them, we can get a
better sense of the inicidence density.

We can also create exposure stratified plots by specifying the
\code{exposure} argument in the \code{popTime} function:

We can also plot them side-by-side using the \code{ncol} argument:

\begin{CodeChunk}


\begin{center}\includegraphics{../figures/plot-stratified-erspc-data-side-1} \end{center}

\end{CodeChunk}

ADD PARAGRAPH ABOUT WHAT WE CAN CONCLUDE FROM THESE GRAPHS.

For a more detailed description of how to modify the aesthetics of a
population time plot, please refer to the
\href{http://sahirbhatnagar.com/casebase/articles/customizingpopTime.html}{Customizing
Population Time Plots} vignette.

\hypertarget{parametric-modeling}{%
\subsection{Parametric modeling}\label{parametric-modeling}}

The parametric modeling step was separated into three parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  case-base sampling;
\item
  estimation of the smooth hazard function;
\item
  calculation of the risk function.
\end{enumerate}

By separating the sampling and estimation functions, we allowed the
possibility of users implementing more complex sampling scheme, as
described in Saarela \citeyearpar{saarela2016case}.

The sampling scheme selected for \code{sampleCaseBase} was described in
Hanley and Miettinen \citeyearpar{hanley2009fitting}: we first sample
along the ``person'' axis, proportional to each individual's total
follow-up time, and then we sample a moment uniformly over their
follow-up time. This sampling scheme is equivalent to the following
picture: imagine representing the total follow-up time of all
individuals in the study along a single dimension, where the follow-up
time of the next individual would start exactly when the follow-up time
of the previous individual ends. Then the base series could be sampled
uniformly from this one-dimensional representation of the overall
follow-up time. In any case, the output is a dataset of the same class
as the input, where each row corresponds to a person-moment. The
covariate profile for each such person-moment is retained, and an offset
term is added to the dataset. This output could then be used to fit a
smooth hazard function, or for visualization of the base series.

The fitting function \code{fitSmoothHazard} starts by looking at the
class of the dataset: if it was generated from \code{sampleCaseBase}, it
automatically inherited the class \code{cbData}. If the dataset supplied
to \code{fitSmoothHazard} does not inherit from \code{cbData}, then the
fitting function starts by calling \code{sampleCaseBase} to generate the
base series. In other words, the occasional user can bypass
\code{sampleCaseBase} altogether and only worry about the fitting
function \code{fitSmoothHazard}.

The fitting function retains the familiar formula interface of
\code{glm}. The left-hand side of the formula should be the name of the
column corresponding to the event type. The right-hand side can be any
combination of the covariates, along with an explicit functional form
for the time variable. Note that non-proportional hazard models can be
achieved at this stage by adding an interaction term involving time. The
offset term does not need to be specified by the user, as it is
automatically added to the formula.

To fit the hazard function, we provide several approaches that are
available via the \code{family} parameter. These approaches are:

\begin{itemize}
\tightlist
\item
  \code{glm}: This is the familiar logistic regression.
\item
  \code{glmnet}: This option allows for variable selection using Lasso
  or elastic-net. This functionality is provided through the
  \code{glmnet} package \citep{friedman2010jss}.
\item
  \code{gam}: This option provides support for \emph{Generalized
  Additive Models} via the \code{gam} package
  \citep{hastie1987generalized}.
\item
  \code{gbm}: This option provides support for \emph{Gradient Boosted
  Trees} via the \code{gbm} package. This feature is still experimental.
\end{itemize}

In the case of multiple events, the hazard is fitted via multinomial
regression as performed by the \pkg{VGAM} package. This package was
selected for its ability to fit multinomial regression models with an
offset.

Once a model-fit object has been returned by \code{fitSmoothHazard}, all
the familiar summary and diagnostic functions are available:
\code{print}, \code{summary}, \code{predict}, \code{plot}, etc. Our
package provides one more functionality: it computes risk functions from
the model fit. For the case of a single event, it uses the familiar
identity \begin{equation}\label{eqn:surv}
S(t) = \exp\left(-\int_0^t h(u;X) du\right).
\end{equation} The integral is computed using either the
\code{stats::integrate} function or Monte-Carlo integration. The risk
function (or cumulative incidence function) is then defined as
\begin{equation}\label{eqn:CI}
CI(t) = 1 - S(t).
\end{equation}

For the case of a competing-event analysis, the event-specific risk is
computed using the following procedure: first, we compute the overall
survival function (i.e.~for all event types):

\[ S(t) = \exp\left(-\int_0^t H(u;X) du\right),\qquad H(t;X) = \sum_{j=1}^J h_j(t;X).\]
From this, we can derive the event-specific subdensities:

\[ f_j(t) = h_j(t)S(t).\]

Finally, by integrating these subdensities, we obtain the event-specific
cumulative incidence functions:

\[ CI_j(t) = \int_0^t f_j(u)du.\]

We created \code{absoluteRisk} as an \code{S3} generic, with methods for
the different types of outputs of \code{fitSmoothHazard}. The method
dispatch system of \proglang{R} then takes care of matching the correct
output to the correct methodology for calculating the cumulative
incidence function, without the user's intervention.

In the following sections, we illustrate these functionalities in the
context of three case studies.

\hypertarget{case-study-1european-randomized-study-of-prostate-cancer-screening}{%
\section{Case study 1--European Randomized Study of Prostate Cancer
Screening}\label{case-study-1european-randomized-study-of-prostate-cancer-screening}}

Next, we investigate the differences between the control and the
screening arms. A common choice for this type of analysis is to use a
Cox regression model and estimate the hazard ratio for the screening
group (relative to the control group). In \proglang{R}, it can be done
as follows:

\begin{CodeChunk}

\begin{CodeOutput}
#> Call:
#> survival::coxph(formula = Surv(Follow.Up.Time, DeadOfPrCa) ~ 
#>     ScrArm, data = ERSPC)
#> 
#>   n= 159893, number of events= 540 
#> 
#>                         coef exp(coef) se(coef)     z Pr(>|z|)  
#> ScrArmScreening group -0.222     0.801    0.088 -2.52    0.012 *
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#>                       exp(coef) exp(-coef) lower .95 upper .95
#> ScrArmScreening group     0.801       1.25     0.674     0.952
#> 
#> Concordance= 0.519  (se = 0.011 )
#> Likelihood ratio test= 6.45  on 1 df,   p=0.01
#> Wald test            = 6.37  on 1 df,   p=0.01
#> Score (logrank) test = 6.39  on 1 df,   p=0.01
\end{CodeOutput}
\end{CodeChunk}

We can also plot the cumulative incidence function (CIF) for each group.
However, this involves estimating the baseline hazard, which Cox
regression treated as a nuisance parameter.

\begin{CodeChunk}
\begin{figure}

{\centering \includegraphics{../figures/erspc-cox-cif-1} 

}

\caption[Cumulative incidence functions for control and screening groups in the ERSPC data]{Cumulative incidence functions for control and screening groups in the ERSPC data. In each of the panels, we plot the CIF from the Cox model (solid line) and the CIF from the caseabse sampling scheme (dashed line) with different functional forms of time in a logistic regression. (A) The time variable is excluded so that the hazard function is constant over time which is equivalent to the one-parameter exponential distribution. (B) Linear function of time which is equivalent to a Gompertz distribution. (C) The natural logarithm of time which is equivalent to a Weibull distribution. (D) Cubic b-spline expansion of time.}\label{fig:erspc-cox-cif}
\end{figure}
\end{CodeChunk}

\begin{CodeChunk}


\begin{center}\includegraphics{../figures/unnamed-chunk-1-1} \end{center}

\end{CodeChunk}

\hypertarget{equivalence-between-casebase-and-aft}{%
\subsection{Equivalence between casebase and
AFT}\label{equivalence-between-casebase-and-aft}}

Next, we show how case-base sampling can be used to carry out a similar
analysis. We fit several models that differ in how we choose to model
time.

First, we will fit an exponential model. Recall that this corresponds to
excluding time from the linear predictor.

We can then use the \code{absoluteRisk} function to get an estimate of
the cumulative incidence curve for a specific covariate profile. In the
plot below, we overlay the estimated CIF from the exponential model on
the Cox model CIF:

As we can see, the exponential model gives us an estimate of the hazard
ratio that is similar to the one obtained using Cox regression. However,
the CIF estimates are quite different. Based on what we observed in the
population time plot, where more events are observed later on in time,
we do not expect a constant hazard would be a good description for this
data. In other words, this poor fit for the exponential hazard is
expected. A constant hazard model would overestimate the cumulative
incidence earlier on in time, and underestimate it later on; this is
what we see on the cumulative incidence plot. This example demonstrates
the benefits of population time plots as an exploratory analysis tool.

For the next model, we include time as a linear term. Recall that this
corresponds to a Weibull model.

Again, the estimate of the hazard ratio is similar to that obtained from
Cox regression. We then look at the estimate of the CIF:

We see that the Weibull model leads to a better fit of the CIF than the
Exponential model, when compared with the semi-parametric approach.

Finally, we can model the hazard as a smooth function of time using the
\code{splines} package:

Once again, we can see that the estimate of the hazard ratio is similar
to that from the Cox regression. We then look at the estimate of the
CIF:

Qualitatively, the combination of splines and case-base sampling
produces the parametric model that most closely resembles the Cox model.
In the following table, we see that the confidence intervals are also
similar across all four models. In other words, this reinforces the idea
that, under proportional hazards, we do not need to model the full
hazard to obtain reliable estimates of the hazard ratio. Of course,
different parametric models for the hazards give rise to qualitatively
different estimates for the CIF.

\begin{CodeChunk}
\begin{table}

\caption{\label{tab:unnamed-chunk-4}Mean }
\centering
\begin{tabular}[t]{>{\bfseries}lcc}
\toprule
Model & casebase & survreg\\
\midrule
Exponential & 0.81 (0.68, 0.96) & 0.81 (0.68, 0.96)\\
\cmidrule{1-3}
Gompertz & 0.79 (0.66, 0.94) & 0.80 (0.67, 0.95)\\
\cmidrule{1-3}
Weibull & 0.80 (0.67, 0.95) & 0.80 (0.65, 0.96)\\
\cmidrule{1-3}
Splines & 0.80 (0.67, 0.95) & --\\
\bottomrule
\multicolumn{3}{l}{\textit{Note:}}\\
\multicolumn{3}{l}{Median (Inter-quartile range) is given for Model Size.}\\
\end{tabular}
\end{table}

\end{CodeChunk}

As noted above, the usual asymptotic results hold for likelihood ratio
tests built using case-base sampling models. Therefore, we can easily
test the null hypothesis that the exponential model is just as good as
the larger (in terms of number of parameters) splines model.

\bibliography{references.bib}


\end{document}

